{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN63Zpsy7DHxIHqLdDtdETJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# pytorch broadcast\n","# wT x + b  = z; x = colmn vecvtor, W= column vector\n","import torch\n","a = torch.tensor([1,2,3])\n","b = torch.tensor([4,5,6])\n","print(a*b)\n","print(a + 1) # implicit broadcast\n","\n","# opportunities for parallelism in matrix mul\n","# 1. compute dot product in parallel\n","# 2. multiple dot products in parallel\n","\n","# Xw + b = z, X = (n,m), w = (m,1), z = (n,1)\n","\n","X = torch.arange(6).view(2,3)\n","print(X.shape, X)\n","\n","w = torch.tensor([1,2,3]) # shape [3]\n","print(w.shape, w)\n","\n","Y = X.matmul(w)\n","print(Y.shape, Y)\n","\n","w = w.view(-1,1) # '-1' tells PyTorch to automatically calculate the size of this dimension based on the other dimensions and the total number of elements in the tensor. '1' specifies that the second dimension of the reshaped tensor should have a size of 1.\n","print(w.shape, w) # shape [3,1]\n","\n","Y = X.matmul(w)\n","print(Y.shape, Y)\n","\n","# broadcast example: add a (2,3) and (1,3). i.e implicit dim gets added and elments are duplicated. here the (1,3) gets replicated to create a (2,3) matrix\n","X = torch.arange(6).view(2,3)\n","print(X.shape, X)\n","w = torch.tensor([1,2,3])\n","print(w.shape, w)\n","\n","print (X + w)\n","\n","# another example https://jeskin.net/blog/pytorch-broadcasting-mechanics/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qc9DQnLTZdQ6","executionInfo":{"status":"ok","timestamp":1742357784174,"user_tz":420,"elapsed":243,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"58ddd089-87db-4031-f58e-ff62f26e98a8"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 4, 10, 18])\n","tensor([2, 3, 4])\n","torch.Size([2, 3]) tensor([[0, 1, 2],\n","        [3, 4, 5]])\n","torch.Size([3]) tensor([1, 2, 3])\n","torch.Size([2]) tensor([ 8, 26])\n","torch.Size([3, 1]) tensor([[1],\n","        [2],\n","        [3]])\n","torch.Size([2, 1]) tensor([[ 8],\n","        [26]])\n","torch.Size([2, 3]) tensor([[0, 1, 2],\n","        [3, 4, 5]])\n","torch.Size([3]) tensor([1, 2, 3])\n","tensor([[1, 3, 5],\n","        [4, 6, 8]])\n"]}]},{"cell_type":"code","execution_count":7,"metadata":{"id":"bMxe3Z9SNFXf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742357784185,"user_tz":420,"elapsed":10,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"584b40fd-465f-4629-9b85-53f6f87b129e"},"outputs":[{"output_type":"stream","name":"stdout","text":["GPU is not available, using CPU instead\n"]}],"source":["# prompt: check gpu device in pytorch\n","\n","import torch\n","\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")          # Use GPU\n","    print(\"GPU is available\")\n","    print(torch.cuda.get_device_name(0))\n","else:\n","    device = torch.device(\"cpu\")            # Use CPU\n","    print(\"GPU is not available, using CPU instead\")\n"]},{"cell_type":"code","source":["# prompt: download the url https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch02/01_main-chapter-code/the-verdict.txt\n","\n","!wget https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch02/01_main-chapter-code/the-verdict.txt\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"frujqe1xOiOM","executionInfo":{"status":"ok","timestamp":1742357784632,"user_tz":420,"elapsed":447,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"f6f6f5f5-64de-4933-e63c-3262ec4d9c70"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-03-19 04:16:24--  https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch02/01_main-chapter-code/the-verdict.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 20479 (20K) [text/plain]\n","Saving to: ‘the-verdict.txt’\n","\n","the-verdict.txt     100%[===================>]  20.00K  --.-KB/s    in 0.003s  \n","\n","2025-03-19 04:16:24 (6.07 MB/s) - ‘the-verdict.txt’ saved [20479/20479]\n","\n"]}]},{"cell_type":"code","source":["! ls\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tOgtz_uvFsJP","executionInfo":{"status":"ok","timestamp":1742357784675,"user_tz":420,"elapsed":42,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"efe39425-664c-4ee1-b88e-eb718e61b388"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["readme\t     SMSSpamCollection.tsv  test.csv\t     train.csv\n","sample_data  smsspamcollection.zip  the-verdict.txt  val.csv\n"]}]},{"cell_type":"code","source":["with open('the-verdict.txt', 'r', encoding='UTF-8') as f:\n","    raw_text = f.read()\n","\n","print (\"total number of char\", len(raw_text))\n","print (raw_text[:1000])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DqhlzuCgzHno","executionInfo":{"status":"ok","timestamp":1742357784702,"user_tz":420,"elapsed":26,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"0fc018b5-86a2-43ab-ad0b-ccc7c3c5ce02"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["total number of char 20479\n","I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n","\n","\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. \"Of course it's going to send the value of my picture 'way up; but I don't think of that, Mr. Rickham--the loss to Arrt is all I think of.\" The word, on Mrs. Thwing's lips, multiplied its _rs_ as though they were reflected in an endless vista of mirrors. And it was not only the Mrs. Thwings who mourned. Had not the exquisite Hermia Croft, at the last Grafton Gallery show, stopped me before Gisburn's \"Moon-dancers\" to say, with tears in her eyes: \"We shall not look upon its like again\"?\n","\n","Well!--even through th\n"]}]},{"cell_type":"code","source":["import re\n","# example usage of re to split and remove whitespaces. # removing white spaces depends on teh application. keeping whitespaces, can be useful, that are sensitive to the exact structure e.g code.\n","# we remove them here for simplicity and reduced memory.\n","text = \"Hello, world. this, is a test line.\"\n","result = re.split(r'([,.]|\\s)', text)\n","result = [item for item in result if item.strip()]\n","print(result)\n","\n","text2 = \"hello, world. is this-- a test?\"\n","regex = r'([,.:;?_!\"()\\']|--|\\s)'\n","result = re.split(regex, text2)\n","result = [item for item in result if item.strip()]\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XniPp5kczO_3","executionInfo":{"status":"ok","timestamp":1742357784767,"user_tz":420,"elapsed":58,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"3e58f3d1-e185-4440-fb51-364b7d0db5a7"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello', ',', 'world', '.', 'this', ',', 'is', 'a', 'test', 'line', '.']\n","['hello', ',', 'world', '.', 'is', 'this', '--', 'a', 'test', '?']\n"]}]},{"cell_type":"code","source":["# extract tokens from the raw text of the book\n","preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n","preprocessed = [item for item in preprocessed if item.strip()]\n","print (\"total number of tokens\", len(preprocessed))\n","print (preprocessed[:100])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZjwFYYF-0529","executionInfo":{"status":"ok","timestamp":1742357784806,"user_tz":420,"elapsed":39,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"3ddd0884-8775-4efa-91bf-8715321e9b66"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["total number of tokens 4690\n","['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself', 'in', 'a', 'villa', 'on', 'the', 'Riviera', '.', '(', 'Though', 'I', 'rather', 'thought', 'it', 'would', 'have', 'been', 'Rome', 'or', 'Florence', '.', ')', '\"', 'The', 'height', 'of', 'his', 'glory', '\"', '--', 'that', 'was', 'what', 'the', 'women', 'called', 'it', '.', 'I', 'can', 'hear', 'Mrs', '.', 'Gideon', 'Thwing', '--', 'his', 'last', 'Chicago', 'sitter', '--']\n"]}]},{"cell_type":"code","source":["# convert to token IDs. build a vocab sorted. each token is mapped to a token ID from the sorted list.\n","all_words = sorted(set(preprocessed))\n","vocab_size = len(all_words)\n","print (\"vocab size\", vocab_size)\n","\n","vocab = {token:i for i, token in enumerate(all_words)}\n","\n","for i, item in enumerate(vocab.items()):\n","    print (i, item)\n","    if i > 10:\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QPZcuPBT29G4","executionInfo":{"status":"ok","timestamp":1742357784844,"user_tz":420,"elapsed":20,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"25ac858b-0815-4d16-ad28-406a8f1352f5"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["vocab size 1130\n","0 ('!', 0)\n","1 ('\"', 1)\n","2 (\"'\", 2)\n","3 ('(', 3)\n","4 (')', 4)\n","5 (',', 5)\n","6 ('--', 6)\n","7 ('.', 7)\n","8 (':', 8)\n","9 (';', 9)\n","10 ('?', 10)\n","11 ('A', 11)\n"]}]},{"cell_type":"code","source":["# prompt: create a python class with encode and decode method\n","\n","import torch\n","import re\n","\n","class SimpleTokenizerV1:\n","    def __init__(self, vocab):\n","      self.str_to_int = vocab\n","      self.int_to_str = {i:s for s,i in vocab.items()}\n","\n","    def encode(self, raw_text):\n","      preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n","      preprocessed = [item for item in preprocessed if item.strip()]\n","      return [self.str_to_int[item] for item in preprocessed]\n","\n","    def decode(self, ids):\n","      text = \" \".join([self.int_to_str[item] for item in ids])\n","      # `r'\\s+([,.:;?_!\"()\\'])'`**: This is the regular expression pattern being searched for.\n","      # `\\s+`**: Matches one or more whitespace characters (spaces, tabs, newlines).  The `+` means \"one or more\" of the preceding character or group.\n","      # `([,.:;?_!\"()\\'])`**: This is a capturing group.  It matches any single character within the square brackets (`,`, `.`, `:`, `;`, `?`, `_`, `\"`, `!`, `(`, `)`, `'`). The parentheses create a capturing group, which means the matched punctuation will be stored for later use.\n","      # `r'\\1'`**: This is the replacement string.  `\\1` refers to the first captured group in the regular expression pattern.  So, it replaces the matched whitespace characters and the punctuation mark with just the punctuation mark itself.  Any preceding whitespace is effectively removed.\n","      text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n","      return text"],"metadata":{"id":"JHNu_bE334XK","executionInfo":{"status":"ok","timestamp":1742357784878,"user_tz":420,"elapsed":2,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["tokenizer = SimpleTokenizerV1(vocab)\n","text = \"\"\"--even through the prism of Hermia's tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his own trade hardly a murmur. Professional jealousy?\"\"\"\n","ids = tokenizer.encode(text)\n","print(ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mnsiWle18WSi","executionInfo":{"status":"ok","timestamp":1742357784930,"user_tz":420,"elapsed":52,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"52cf48d9-23d3-47ca-a994-2abf66c46d0e"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["[6, 399, 1007, 988, 795, 722, 50, 2, 850, 976, 53, 436, 117, 1016, 418, 988, 420, 1108, 395, 7, 80, 57, 38, 0, 93, 1112, 514, 654, 546, 6, 585, 1077, 444, 987, 994, 879, 687, 546, 7, 13, 549, 742, 872, 438, 829, 1088, 536, 5, 157, 568, 549, 742, 1026, 528, 115, 694, 7, 81, 589, 10]\n"]}]},{"cell_type":"code","source":["print(tokenizer.decode(ids))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_TXjJTC8kBwt","executionInfo":{"status":"ok","timestamp":1742357784931,"user_tz":420,"elapsed":20,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"7eabc51f-0c98-403c-96cc-02af8e416909"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["-- even through the prism of Hermia' s tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him -- it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his own trade hardly a murmur. Professional jealousy?\n"]}]},{"cell_type":"code","source":["# special context token to handle unknown tokens. Modify the vocab to add special tokens '<|unk|>' '<|endoftext|>'. Say we need to deal with multiple docs, we add <|endoftext|> in the beg of each doc. It signals markers. improves LLM understanding\n","# some additionla tokens BOS -  beg of sequence, EOS - end of sequence (similar to endoftext), PAD - shorter text are padded to extended to the longest sequence size of the batch\n","# GPT uses subword tokenizer so doesnt need to use <|unk|>\n","\n","all_tokens = sorted(list(set(preprocessed)))\n","all_tokens.extend(['<|endoftext|>','<|unk|>'])\n","vocab = {token:id for id, token in enumerate(all_tokens)}\n","print (\"vocab size\", len(vocab.items()))\n","\n","for i, item in enumerate(list(vocab.items())[-5:]):\n","    print (item)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1gNyYO5MpQdz","executionInfo":{"status":"ok","timestamp":1742357785598,"user_tz":420,"elapsed":668,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"ec179993-0f81-4619-b98e-448a48d0eeed"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["vocab size 1132\n","('younger', 1127)\n","('your', 1128)\n","('yourself', 1129)\n","('<|endoftext|>', 1130)\n","('<|unk|>', 1131)\n"]}]},{"cell_type":"code","source":["import torch\n","import re\n","\n","class SimpleTokenizerV2:\n","    def __init__(self, vocab):\n","      self.str_to_int = vocab\n","      self.int_to_str = {i:s for s,i in vocab.items()}\n","\n","    def encode(self, raw_text):\n","      preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n","      preprocessed = [item for item in preprocessed if item.strip()]\n","      preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n","      return [self.str_to_int[item] for item in preprocessed]\n","\n","    def decode(self, ids):\n","      text = \" \".join([self.int_to_str[item] for item in ids])\n","      text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n","      return text"],"metadata":{"id":"9N3NDPwXrvP-","executionInfo":{"status":"ok","timestamp":1742357785619,"user_tz":420,"elapsed":20,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["text1 = \"hello, do you like tea?\"\n","text2 = \"In the sunlit terraces of the palace\"\n","text = \"<|endoftext|> \".join((text1, text2))\n","print (text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MIxBGKGgsV_F","executionInfo":{"status":"ok","timestamp":1742357785636,"user_tz":420,"elapsed":17,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"48d05079-b6aa-4e18-cb22-f87189f63050"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["hello, do you like tea?<|endoftext|> In the sunlit terraces of the palace\n"]}]},{"cell_type":"code","source":["tokenizer = SimpleTokenizerV2(vocab)\n","ids = tokenizer.encode(text)\n","print(ids)\n","print(tokenizer.decode(ids))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iUkHB1dlslkj","executionInfo":{"status":"ok","timestamp":1742357785671,"user_tz":420,"elapsed":29,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"b89fdd63-7182-499c-be22-9233433654b9"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131]\n","<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>\n"]}]},{"cell_type":"code","source":["# chapter: BPE (source: https://www.youtube.com/watch?v=fKd8s29e-l4&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=9)\n","# BPE tokenization\n","# why we need this?\n","# word based, sub-word based, char based tokenizers\n","# word based problems: out of vocab words, different meaning of similar words (e.g boy, boys).\n","# char based problems: very small vocab, meaning associated with the words lost. tokenized sequence much longer than the original text\n","# sub word: does capture root words, (e.g treat boy as a common root word).\n","# sub word rules:  a) do not split frequently used words into subwords b) split rare words into smaller meanigful subwords. This helps model learn that different words with same root wrod are similar meaning. This menaing is lost in word and char based tokenization.\n","# it also helps model learn that words are made up of root words and suffixes like 'ization'.\n","\n","# BPE is a subword tokenization algo. introduced in 1994. data compression algo. we scan the data, most common pair of consecutive bytes of data is replaced with a byte that does not exist in the data.\n","# aaabdaaabac\n","# byte pair 'aa' occurs the most 4 times. replace this with a byte like 'Z'\n","# compressed data: ZabdZabac\n","# next common byte pair is 'ab'. replace this by 'Y', compressed data is ZYdZYac. we dont need to replace 'ac' since it occurs only once.\n","# replace 'ZY' with W. WdWac\n","\n","# in LLM we use BPE to  represent a token for the most common words. rare words are broken down into 2 or more sub words\n","\n","# we have old:7, older:3, finest:9, lowest:4 -> dataset of words. we need to add '</w>' as end of word token (preprocessing step)\n","# old</w>:7, older</w>:3, ...\n","\n","# 1. split all words into char and make a freq table. e.g </w>: 23 times\n","# 2. lookup the most frequent occruing pair and merge. e.g es appears 13 times.  a new token es is added to the table. and the freq of e and s subtract the frequency. subtract 13 from e and s. e:16-13=3, s:13-13=0\n","# 3. now es becomes a token, and es t more frequntly occuring pair. add est to the freq table, and subtract the amount from es and t. we are slowly identifying common root words. Similarly we iterate to find est</w>. This is important to note that we can now differentiate between 'estimate' and 'highest', where ending est if more frequently occuring.\n","# 4. ... stopping criteria can be token count or number of iterations.\n","\n","# BPE it also solves out of vocab problem. but also now solves the common root problem encountered in word level encoding.\n","\n","\n"],"metadata":{"id":"uQ4DLkNGtkzP","executionInfo":{"status":"ok","timestamp":1742357785672,"user_tz":420,"elapsed":1,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# OSS BPE implementation\n","! pip install tiktoken"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"61jsuUFY1O7e","executionInfo":{"status":"ok","timestamp":1742357792729,"user_tz":420,"elapsed":7047,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"9bd3e478-f23c-451e-cebd-c3d651cf2935"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n"]}]},{"cell_type":"code","source":["import importlib\n","import tiktoken\n","print (importlib.metadata.version(\"tiktoken\"))\n","\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","text = (\n","    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n","    \"of someunknownPlace\"\n",")\n","ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n","print (ids)\n","print(tokenizer.decode(ids))\n","\n","# 50256 is the <|endoftext|>\n","# how does BPE handle unknown words without the <|unk|>? BPE can represent unseen woeds as sub word tokens or chars\n","ids = tokenizer.encode(\"Akwirw ier\")\n","print(ids)\n","print(tokenizer.decode(ids))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V5baBQoyxydm","executionInfo":{"status":"ok","timestamp":1742357794402,"user_tz":420,"elapsed":1664,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"ec2e6a42-6c15-4bba-8af8-e8fba3862344"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9.0\n","[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271]\n","Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace\n","[33901, 86, 343, 86, 220, 959]\n","Akwirw ier\n"]}]},{"cell_type":"code","source":["# Chapter: creating input batches for LLM (source: https://www.youtube.com/watch?v=iQZFH8dr2yI&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=9)\n","# create input, target pairs from the input text is the goal here.\n","# text sample:\n","# llms learn to predict one word at a time\n","# the pairs are as follows:\n","# llms -> learn\n","# llms learn -> to\n","# llms learn to predict -> one\n","# what comes after the prediction is masked in auto-regressive models. unsupervised learning. used for pre-trainig the llm.\n","# given a text sample:\n","# 1. extracting input blocks, that serve is input to the llm.\n","# 2. llm task is the predict the next word or token\n","# 3. we will mask out all words beyond the target"],"metadata":{"id":"AsswNDyM5_Fx","executionInfo":{"status":"ok","timestamp":1742357794419,"user_tz":420,"elapsed":10,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["import tiktoken\n","with open('the-verdict.txt', 'r', encoding='UTF-8') as f:\n","    raw_text = f.read()\n","\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","enc_text = tokenizer.encode(raw_text)\n","print(len(enc_text))\n","\n","\n","# e.g. input   x: [1,2,3,4]\n","#      output: y: [2,3,4,5]\n","# if 1,2 is i/p, o/p is 3 and so on.\n","# here the size 4 is the context size. we can give upto 4 tokens as input here\n","\n","context_size = 4\n","enc_sample = enc_text[50:]\n","print (enc_sample)\n","x = enc_sample[:context_size]\n","y = enc_sample[1:context_size+1]\n","print (f\"x: {x}\")\n","print (f\"y: {y}\")\n","\n","\n","# num of predictions = context size\n","for i in range(1, context_size+1):\n","  context = enc_sample[:i]\n","  desired = enc_sample[i]\n","  print (context, \"------>\", desired)\n","\n","print(\"decoded text\")\n","# num of predictions = context size\n","for i in range(1, context_size+1):\n","  context = enc_sample[:i]\n","  desired = enc_sample[i]\n","  print (tokenizer.decode(context), \"------>\", tokenizer.decode([desired]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2dcmyaxgAlIb","executionInfo":{"status":"ok","timestamp":1742357794502,"user_tz":420,"elapsed":77,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"a26b9dcd-4043-453f-b29a-8af7ceadbe7a"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["5145\n","[290, 4920, 2241, 287, 257, 4489, 64, 319, 262, 34686, 41976, 13, 357, 10915, 314, 2138, 1807, 340, 561, 423, 587, 10598, 393, 28537, 2014, 198, 198, 1, 464, 6001, 286, 465, 13476, 1, 438, 5562, 373, 644, 262, 1466, 1444, 340, 13, 314, 460, 3285, 9074, 13, 46606, 536, 5469, 438, 14363, 938, 4842, 1650, 353, 438, 2934, 489, 3255, 465, 48422, 540, 450, 67, 3299, 13, 366, 5189, 1781, 340, 338, 1016, 284, 3758, 262, 1988, 286, 616, 4286, 705, 1014, 510, 26, 475, 314, 836, 470, 892, 286, 326, 11, 1770, 13, 8759, 2763, 438, 1169, 2994, 284, 943, 17034, 318, 477, 314, 892, 286, 526, 383, 1573, 11, 319, 9074, 13, 536, 5469, 338, 11914, 11, 33096, 663, 4808, 3808, 62, 355, 996, 484, 547, 12548, 287, 281, 13079, 410, 12523, 286, 22353, 13, 843, 340, 373, 407, 691, 262, 9074, 13, 536, 48819, 508, 25722, 276, 13, 11161, 407, 262, 40123, 18113, 544, 9325, 701, 11, 379, 262, 938, 402, 1617, 261, 12917, 905, 11, 5025, 502, 878, 402, 271, 10899, 338, 366, 31640, 12, 67, 20811, 1, 284, 910, 11, 351, 10953, 287, 607, 2951, 25, 366, 1135, 2236, 407, 804, 2402, 663, 588, 757, 13984, 198, 198, 5779, 28112, 10197, 832, 262, 46475, 286, 18113, 544, 338, 10953, 314, 2936, 1498, 284, 1986, 262, 1109, 351, 1602, 11227, 414, 13, 23676, 3619, 402, 271, 10899, 0, 383, 1466, 550, 925, 683, 438, 270, 373, 15830, 326, 484, 815, 25722, 683, 13, 9754, 465, 898, 1714, 7380, 30090, 547, 2982, 11, 290, 287, 465, 898, 3292, 8941, 257, 4636, 28582, 13, 18612, 35394, 30, 8673, 13, 1002, 340, 547, 11, 262, 15393, 286, 262, 5977, 373, 29178, 3474, 416, 1310, 40559, 11959, 1636, 11, 508, 11, 287, 477, 922, 4562, 11, 3181, 503, 287, 262, 37090, 257, 845, 22665, 366, 672, 270, 2838, 1, 319, 3619, 438, 505, 286, 883, 905, 88, 6685, 42070, 351, 4738, 6276, 871, 326, 314, 423, 2982, 357, 40, 1839, 470, 910, 416, 4150, 8, 3688, 284, 402, 271, 10899, 338, 12036, 13, 843, 523, 438, 14363, 10568, 852, 5729, 11331, 18893, 540, 438, 1169, 5114, 11835, 3724, 503, 11, 290, 11, 355, 9074, 13, 536, 5469, 550, 11001, 11, 262, 2756, 286, 366, 38, 271, 10899, 82, 1, 1816, 510, 13, 198, 198, 1026, 373, 407, 10597, 1115, 812, 1568, 326, 11, 287, 262, 1781, 286, 257, 1178, 2745, 6, 4686, 1359, 319, 262, 34686, 41976, 11, 340, 6451, 5091, 284, 502, 284, 4240, 1521, 402, 271, 10899, 550, 1813, 510, 465, 12036, 13, 1550, 14580, 11, 340, 1107, 373, 257, 29850, 1917, 13, 1675, 24456, 465, 3656, 561, 423, 587, 1165, 2562, 438, 14363, 3148, 1650, 1010, 550, 587, 6699, 262, 1540, 558, 286, 2282, 326, 9074, 13, 402, 271, 10899, 550, 366, 7109, 14655, 683, 866, 526, 1114, 9074, 13, 402, 271, 10899, 438, 292, 884, 438, 18108, 407, 11196, 10597, 3016, 257, 614, 706, 3619, 338, 10568, 550, 587, 2077, 13, 632, 1244, 307, 326, 339, 550, 6405, 607, 438, 20777, 339, 8288, 465, 10152, 438, 13893, 339, 1422, 470, 765, 284, 467, 319, 12036, 26, 475, 340, 561, 423, 587, 1327, 284, 5879, 326, 339, 550, 1813, 510, 465, 12036, 780, 339, 550, 6405, 607, 13, 198, 198, 5189, 1781, 11, 611, 673, 550, 407, 17901, 683, 866, 11, 673, 550, 8603, 11, 355, 4544, 9325, 701, 42397, 11, 4054, 284, 366, 26282, 683, 510, 1, 438, 7091, 550, 407, 2957, 683, 736, 284, 262, 1396, 417, 13, 1675, 1234, 262, 14093, 656, 465, 1021, 757, 438, 10919, 257, 410, 5040, 329, 257, 3656, 0, 887, 9074, 13, 402, 271, 10899, 4120, 284, 423, 595, 67, 1328, 340, 438, 392, 314, 2936, 340, 1244, 307, 3499, 284, 1064, 503, 1521, 13, 198, 198, 464, 748, 586, 652, 1204, 286, 262, 34686, 41976, 37733, 2346, 284, 884, 14177, 8233, 1020, 5768, 26, 290, 1719, 11, 319, 616, 835, 284, 22489, 40089, 11, 4978, 257, 19350, 286, 3619, 338, 3652, 436, 81, 5286, 8812, 2114, 1022, 262, 279, 1127, 11, 314, 550, 3589, 28068, 294, 1555, 262, 1306, 1110, 13, 198, 198, 40, 1043, 262, 3155, 379, 8887, 11061, 511, 18057, 12, 83, 6037, 26, 290, 9074, 13, 402, 271, 10899, 338, 7062, 373, 523, 2429, 498, 326, 11, 287, 262, 29543, 2745, 11, 314, 4752, 340, 6777, 13, 632, 373, 407, 326, 616, 2583, 408, 373, 366, 47914, 1298, 319, 326, 966, 314, 714, 423, 1813, 4544, 9325, 701, 262, 40830, 12719, 3874, 13, 632, 373, 655, 780, 673, 373, 4808, 1662, 62, 3499, 438, 361, 314, 743, 307, 41746, 12004, 262, 6473, 438, 5562, 314, 1043, 607, 523, 13, 1114, 3619, 11, 477, 465, 1204, 11, 550, 587, 11191, 416, 3499, 1466, 25, 484, 550, 26546, 1068, 465, 1242, 11, 340, 550, 587, 302, 1144, 287, 262, 3024, 12, 4803, 286, 511, 512, 1741, 13, 843, 340, 373, 4361, 5048, 425, 284, 3465, 644, 1245, 262, 366, 25124, 3101, 8137, 286, 16957, 1696, 414, 1, 357, 40, 9577, 4544, 9325, 701, 8, 373, 1719, 319, 683, 13, 198, 198, 40, 423, 4750, 326, 9074, 13, 402, 271, 10899, 373, 5527, 26, 290, 340, 373, 3393, 34953, 856, 326, 607, 5229, 373, 37895, 422, 428, 25179, 257, 19217, 475, 8904, 14676, 13, 632, 318, 11, 355, 257, 3896, 11, 262, 661, 508, 40987, 1637, 508, 651, 749, 503, 286, 340, 26, 290, 3619, 338, 19992, 31564, 286, 465, 3656, 338, 1263, 5236, 9343, 683, 11, 351, 281, 5585, 286, 2818, 922, 12, 49705, 11, 284, 21595, 1133, 340, 656, 5563, 286, 1242, 290, 13064, 13, 1675, 262, 6846, 11, 314, 1276, 751, 11, 339, 6150, 5365, 31655, 26, 475, 339, 373, 7067, 29396, 18443, 12271, 290, 45592, 12, 14792, 5986, 351, 257, 8839, 326, 7284, 35924, 262, 12306, 395, 4133, 13, 198, 198, 1, 26788, 338, 691, 12226, 318, 284, 1234, 8737, 656, 19133, 553, 373, 530, 286, 262, 7877, 72, 3150, 339, 8104, 866, 1973, 262, 37918, 411, 290, 8465, 286, 281, 33954, 271, 3973, 9899, 14678, 40556, 12, 11487, 11, 618, 11, 319, 257, 1568, 1110, 11, 314, 550, 757, 1057, 625, 422, 22489, 40089, 26, 290, 9074, 13, 402, 271, 10899, 11, 307, 3723, 319, 683, 11, 2087, 329, 616, 35957, 25, 366, 14295, 318, 523, 34813, 306, 8564, 284, 790, 1296, 286, 8737, 526, 198, 198, 43920, 3619, 0, 632, 550, 1464, 587, 465, 10030, 284, 423, 1466, 910, 884, 1243, 286, 683, 25, 262, 1109, 815, 307, 900, 866, 287, 1070, 268, 2288, 13, 1867, 7425, 502, 783, 373, 326, 11, 329, 262, 717, 640, 11, 339, 581, 4714, 262, 8216, 13, 314, 550, 1775, 683, 11, 523, 1690, 11, 1615, 3364, 739, 2092, 256, 7657, 438, 9776, 340, 262, 11644, 43778, 3465, 326, 26773, 606, 286, 511, 6799, 454, 30, 1400, 438, 1640, 11, 31414, 1576, 11, 340, 2627, 4156, 326, 339, 373, 16245, 286, 9074, 13, 402, 271, 10899, 438, 69, 623, 1576, 407, 284, 766, 607, 41793, 13, 632, 373, 465, 898, 41793, 339, 3947, 284, 307, 1592, 2259, 739, 438, 14363, 898, 9408, 355, 281, 2134, 329, 5482, 4447, 290, 753, 1072, 13, 198, 198, 1, 3666, 13674, 11, 1201, 314, 1053, 442, 17758, 12036, 661, 836, 470, 910, 326, 3404, 546, 502, 438, 9930, 910, 340, 546, 12622, 41379, 293, 553, 373, 465, 691, 5402, 11, 355, 339, 8278, 422, 262, 3084, 290, 336, 8375, 503, 4291, 262, 4252, 18250, 8812, 558, 13, 198, 198, 40, 27846, 706, 683, 11, 7425, 416, 465, 938, 1573, 13, 12622, 41379, 293, 373, 11, 287, 1109, 11, 5033, 262, 582, 286, 262, 2589, 438, 292, 3619, 2241, 11, 530, 1244, 1234, 340, 11, 550, 587, 262, 582, 286, 262, 1711, 13, 383, 7099, 6802, 373, 531, 284, 423, 7042, 2241, 379, 616, 1545, 338, 3625, 11, 290, 314, 14028, 611, 257, 256, 11912, 286, 35394, 739, 10724, 262, 6846, 338, 11428, 450, 67, 3299, 13, 887, 645, 438, 1640, 340, 373, 407, 10597, 706, 326, 1785, 326, 262, 4808, 13698, 10322, 6532, 62, 8263, 12, 9649, 550, 9258, 284, 3359, 511, 366, 8642, 521, 829, 526, 198, 198, 40, 2900, 284, 9074, 13, 402, 271, 10899, 11, 508, 550, 18459, 1068, 284, 1577, 257, 23844, 286, 7543, 284, 607, 599, 6321, 287, 262, 17423, 12, 3823, 13, 198, 198, 1, 5195, 4808, 10134, 62, 339, 442, 17758, 12036, 1701, 314, 1965, 25891, 13, 198, 198, 3347, 4376, 607, 26928, 351, 257, 9254, 286, 922, 12, 17047, 8167, 5975, 13, 198, 198, 1, 5812, 11, 339, 1595, 470, 4808, 14150, 62, 284, 783, 11, 345, 760, 26, 290, 314, 765, 683, 284, 2883, 2241, 553, 673, 531, 2407, 2391, 13, 198, 198, 40, 3114, 546, 262, 40894, 2330, 12, 6839, 11978, 2119, 11, 351, 663, 4808, 44769, 8270, 12, 332, 660, 62, 410, 1386, 20394, 262, 23755, 286, 262, 14005, 1801, 2093, 41160, 11, 290, 663, 45592, 12, 14792, 1613, 1424, 287, 19217, 24887, 13431, 13, 198, 198, 1, 19242, 339, 442, 17758, 465, 5986, 1165, 30, 314, 4398, 470, 1775, 257, 2060, 530, 287, 262, 2156, 526, 198, 198, 32, 3731, 17979, 286, 32315, 12606, 9074, 13, 402, 271, 10899, 338, 1280, 954, 36368, 13, 366, 1026, 338, 465, 11441, 48740, 11, 345, 760, 13, 679, 1139, 484, 821, 407, 4197, 284, 423, 546, 26, 339, 338, 1908, 606, 477, 1497, 2845, 530, 438, 1820, 18560, 438, 392, 326, 314, 423, 284, 1394, 26148, 526, 198, 198, 6653, 11441, 48740, 438, 14295, 338, 48740, 546, 465, 5986, 30, 2011, 20136, 373, 3957, 588, 262, 26394, 12, 301, 971, 13, 314, 531, 10722, 292, 2280, 284, 616, 2583, 408, 25, 366, 40, 1276, 1107, 766, 534, 18560, 11, 345, 760, 526, 198, 198, 3347, 27846, 503, 2048, 4628, 24882, 379, 262, 8812, 558, 810, 607, 5229, 11, 21081, 782, 278, 287, 257, 14263, 276, 5118, 11, 550, 6578, 257, 24518, 290, 7428, 262, 3394, 20096, 39047, 338, 1182, 1022, 465, 14475, 13, 198, 198, 1, 5779, 11, 1282, 981, 339, 338, 407, 2045, 553, 673, 531, 11, 351, 257, 6487, 326, 3088, 284, 7808, 607, 10927, 1108, 26, 290, 314, 3940, 607, 1022, 262, 30623, 2295, 49406, 286, 262, 6899, 11, 290, 510, 262, 3094, 16046, 351, 1059, 430, 12, 66, 12375, 299, 20896, 82, 24357, 1871, 12734, 379, 1123, 9581, 13, 198, 198, 818, 262, 5391, 76, 395, 5228, 286, 607, 275, 2778, 10840, 11, 10371, 257, 1534, 4241, 286, 19217, 290, 18876, 5563, 11, 9174, 530, 286, 262, 5385, 41186, 39614, 1386, 11, 287, 262, 13203, 5482, 1044, 276, 5739, 13, 383, 5019, 19001, 286, 262, 5739, 1444, 510, 477, 402, 271, 10899, 338, 1613, 0, 198, 198, 27034, 13, 402, 271, 10899, 9859, 736, 262, 4324, 12, 66, 3325, 1299, 11, 3888, 7263, 257, 4808, 73, 446, 259, 13235, 62, 1336, 286, 11398, 35560, 1000, 292, 11, 7121, 281, 3211, 12, 16337, 1497, 11, 290, 531, 25, 366, 1532, 345, 1302, 994, 345, 460, 655, 6687, 284, 766, 340, 13, 314, 550, 340, 625, 262, 24818, 417, 12, 12239, 11, 475, 339, 3636, 470, 1309, 340, 2652, 526, 198, 198, 5297, 438, 40, 714, 655, 6687, 284, 766, 340, 438, 1169, 717, 18560, 286, 3619, 338, 314, 550, 1683, 550, 284, 14022, 616, 2951, 625, 0, 19672, 484, 550, 262, 1295, 286, 15393, 438, 16706, 262, 4318, 6103, 287, 257, 14005, 7872, 393, 4808, 13698, 10322, 6532, 62, 8263, 12, 3823, 11, 393, 257, 36364, 1396, 417, 4624, 523, 326, 340, 1718, 262, 1657, 832, 41160, 286, 1468, 9932, 316, 666, 966, 13, 383, 517, 12949, 1295, 2627, 262, 4286, 1365, 26, 1865, 11, 355, 616, 2951, 6348, 23840, 284, 262, 2063, 12, 2971, 11, 477, 262, 16704, 14482, 1625, 503, 438, 439, 262, 10818, 20597, 32192, 355, 2709, 330, 871, 11, 262, 15910, 286, 16153, 312, 328, 3780, 416, 543, 11, 351, 884, 2784, 9830, 5032, 11, 339, 5257, 284, 36583, 3241, 422, 262, 1103, 1597, 286, 262, 4286, 284, 617, 2495, 11331, 2768, 590, 286, 3703, 13, 9074, 13, 402, 271, 10899, 11, 17728, 257, 8500, 4417, 284, 670, 319, 438, 15464, 11, 355, 340, 547, 11, 523, 16857, 262, 4469, 286, 607, 898, 4286, 438, 18108, 26269, 5223, 287, 281, 8468, 4922, 284, 262, 3359, 286, 428, 3991, 4118, 84, 16579, 13, 383, 4286, 373, 530, 286, 3619, 338, 366, 11576, 395, 553, 355, 465, 21099, 3808, 561, 423, 1234, 340, 438, 270, 7997, 11, 319, 465, 636, 11, 257, 29844, 286, 12749, 11, 257, 22791, 278, 286, 32375, 11, 257, 22486, 11, 965, 2860, 1359, 290, 965, 1397, 11, 326, 14516, 530, 286, 262, 33125, 12, 565, 593, 338, 25304, 4040, 284, 10303, 257, 17972, 13, 632, 1138, 11, 287, 1790, 11, 379, 790, 966, 262, 3512, 286, 14081, 2415, 284, 307, 13055, 366, 11576, 306, 1, 780, 673, 373, 10032, 286, 852, 13055, 366, 34751, 306, 1, 438, 392, 1865, 407, 284, 4425, 281, 22037, 286, 262, 32073, 13, 198, 198, 1, 1026, 338, 262, 938, 339, 13055, 11, 345, 760, 553, 9074, 13, 402, 271, 10899, 531, 351, 27322, 540, 11293, 13, 366, 464, 938, 475, 530, 553, 673, 19267, 5223, 438, 1, 4360, 262, 584, 1595, 470, 954, 11, 780, 339, 6572, 340, 526, 198, 198, 1, 49174, 276, 340, 1701, 314, 373, 546, 284, 1061, 510, 428, 18437, 618, 314, 2982, 257, 2366, 9662, 290, 2497, 3619, 2241, 319, 262, 11387, 13, 198, 198, 1722, 339, 6204, 612, 11, 465, 2832, 287, 262, 16511, 286, 465, 11555, 303, 7821, 13209, 11, 262, 7888, 7586, 9813, 286, 4190, 7121, 736, 422, 465, 2330, 22645, 11, 465, 10904, 4252, 6236, 429, 25839, 9230, 808, 276, 416, 257, 8212, 326, 13663, 262, 9040, 286, 257, 2116, 12, 10414, 738, 285, 23968, 4891, 11, 314, 2936, 284, 644, 257, 4922, 339, 550, 262, 976, 3081, 355, 465, 5986, 438, 1169, 3081, 286, 2045, 1190, 4119, 81, 621, 339, 373, 13, 198, 198, 6653, 3656, 27846, 379, 683, 1207, 8344, 803, 306, 11, 475, 465, 2951, 21650, 1613, 607, 284, 262, 18560, 13, 198, 198, 1, 5246, 13, 8759, 2763, 2227, 284, 766, 340, 553, 673, 2540, 11, 355, 611, 2859, 3500, 5223, 13, 679, 28271, 465, 12450, 11, 991, 16755, 13, 198, 198, 1, 5812, 11, 8759, 2763, 1043, 502, 503, 890, 2084, 553, 339, 531, 15376, 26, 788, 11, 6427, 465, 3211, 832, 6164, 25, 366, 16773, 290, 766, 262, 1334, 286, 262, 2156, 526, 198, 198, 1544, 3751, 340, 284, 502, 351, 257, 1611, 286, 24354, 20154, 11293, 25, 262, 7837, 12, 9649, 11, 262, 5486, 12, 83, 29080, 11, 262, 6576, 12, 565, 418, 1039, 11, 262, 4057, 2655, 12, 8439, 274, 438, 439, 262, 3716, 7106, 6637, 286, 262, 45172, 338, 5928, 3773, 13, 843, 8797, 616, 4240, 3432, 262, 2938, 17547, 339, 531, 11, 9644, 503, 465, 7721, 257, 1310, 25, 366, 5297, 11, 314, 1107, 836, 470, 766, 703, 661, 6687, 284, 2107, 1231, 326, 526, 198, 198, 5779, 438, 270, 373, 655, 262, 886, 530, 1244, 423, 1674, 15898, 329, 683, 13, 5514, 339, 373, 11, 832, 340, 477, 290, 287, 15275, 286, 340, 477, 438, 292, 339, 550, 587, 832, 11, 290, 287, 15275, 286, 11, 465, 5986, 438, 568, 22665, 11, 523, 23332, 11, 523, 595, 18052, 11, 326, 530, 890, 276, 284, 3960, 503, 25, 366, 3856, 44455, 351, 534, 24638, 2474, 355, 1752, 530, 550, 890, 276, 284, 910, 25, 366, 3856, 44455, 351, 534, 670, 2474, 198, 198, 1537, 11, 351, 262, 3960, 319, 616, 11914, 11, 616, 13669, 6989, 281, 10059, 2198, 13, 198, 198, 1, 1212, 318, 616, 898, 49451, 553, 339, 531, 11, 3756, 502, 656, 257, 3223, 8631, 2119, 379, 262, 886, 286, 262, 781, 273, 312, 410, 12523, 13, 632, 373, 6616, 290, 7586, 290, 11620, 88, 25, 645, 366, 34435, 8172, 645, 865, 291, 12, 64, 12, 1671, 330, 11, 4844, 286, 262, 1633, 286, 24380, 329, 20728, 287, 257, 4286, 10273, 438, 29370, 477, 11, 645, 1551, 1051, 286, 1683, 1719, 587, 973, 355, 257, 8034, 13, 198, 198, 464, 1109, 3181, 1363, 284, 502, 262, 4112, 957, 1483, 286, 3619, 338, 2270, 351, 465, 1468, 1204, 13, 198, 198, 1, 3987, 470, 345, 1683, 45553, 903, 351, 7521, 597, 517, 1701, 314, 1965, 11, 991, 2045, 546, 329, 257, 12854, 286, 884, 3842, 13, 198, 198, 1, 12295, 553, 339, 531, 11589, 13, 198, 198, 1, 5574, 1660, 12, 49903, 438, 273, 2123, 10813, 1701, 198, 198, 6653, 6563, 2951, 6348, 5391, 11, 290, 465, 25839, 279, 3021, 257, 1310, 739, 511, 22665, 4252, 10899, 13, 198, 198, 1, 12295, 892, 286, 340, 11, 616, 13674, 5891, 438, 1092, 517, 621, 611, 314, 1549, 1239, 12615, 257, 14093, 526, 198, 198, 1870, 465, 8216, 1297, 502, 287, 257, 7644, 326, 339, 1239, 1807, 286, 1997, 2073, 13, 198, 198, 40, 3888, 1497, 11, 43045, 21100, 416, 616, 10059, 9412, 26, 290, 355, 314, 2900, 11, 616, 4151, 3214, 319, 257, 1402, 4286, 2029, 262, 24818, 417, 12, 12239, 438, 1169, 691, 2134, 7163, 262, 8631, 26210, 3425, 9417, 286, 262, 2119, 13, 198, 198, 1, 5812, 11, 416, 449, 659, 2474, 314, 531, 13, 198, 198, 1026, 373, 257, 17548, 286, 257, 50085, 438, 272, 1468, 10032, 50085, 11, 5055, 287, 262, 6290, 739, 257, 3355, 13, 198, 198, 1, 3886, 449, 659, 438, 64, 520, 5493, 2474, 314, 16896, 13, 198, 198, 1544, 373, 10574, 26, 475, 314, 2936, 683, 1969, 2157, 502, 11, 12704, 257, 1310, 2952, 13, 198, 198, 1, 2061, 257, 4240, 0, 14446, 351, 257, 8667, 3951, 438, 4360, 319, 45697, 19369, 13, 921, 9670, 28022, 11, 810, 750, 345, 651, 340, 1701, 198, 198, 1544, 9373, 6364, 25, 366, 27034, 13, 520, 5493, 2921, 340, 284, 502, 526, 198, 198, 1, 10910, 438, 40, 1422, 470, 760, 345, 772, 2993, 262, 520, 5493, 82, 13, 679, 373, 884, 281, 1167, 2588, 856, 607, 2781, 526, 198, 198, 1, 40, 1422, 470, 438, 83, 359, 706, 13, 764, 764, 764, 1375, 1908, 329, 502, 284, 7521, 683, 618, 339, 373, 2636, 526, 198, 198, 1, 2215, 339, 373, 2636, 30, 921, 1701, 198, 198, 40, 1276, 423, 1309, 257, 1310, 1165, 881, 40642, 972, 6654, 832, 616, 5975, 11, 329, 339, 9373, 351, 257, 1207, 8344, 803, 6487, 25, 366, 5297, 438, 7091, 338, 281, 12659, 2829, 1122, 11, 345, 760, 11, 9074, 13, 520, 5493, 13, 2332, 691, 2126, 373, 284, 423, 683, 1760, 416, 257, 38378, 34537, 438, 993, 11, 3595, 520, 5493, 0, 1375, 1807, 340, 262, 1654, 301, 835, 286, 46431, 465, 27951, 438, 1659, 10833, 340, 319, 257, 1308, 27461, 1171, 13, 843, 379, 262, 2589, 314, 373, 4808, 1169, 62, 38378, 34537, 526, 198, 198, 1, 10910, 11, 3595, 520, 5493, 438, 292, 345, 910, 13, 8920, 4808, 5562, 62, 465, 2106, 1701, 198, 198, 1, 2504, 373, 465, 2106, 13, 1375, 4762, 287, 683, 11, 26996, 798, 287, 683, 438, 273, 1807, 673, 750, 13, 887, 673, 3521, 470, 6842, 407, 284, 423, 477, 262, 8263, 12, 9649, 351, 607, 13, 1375, 3521, 470, 6842, 262, 1109, 326, 11, 319, 1401, 77, 3929, 1528, 11, 530, 714, 1464, 651, 1474, 1576, 284, 766, 465, 5986, 13, 23676, 2415, 0, 1375, 338, 655, 257, 24225, 39136, 278, 329, 584, 21441, 13, 520, 5493, 318, 262, 691, 2187, 314, 1683, 2993, 526, 198, 198, 1, 1639, 1683, 2993, 30, 887, 345, 655, 531, 438, 1, 198, 198, 38, 271, 10899, 550, 257, 11040, 8212, 287, 465, 2951, 13, 198, 198, 1, 5812, 11, 314, 2993, 683, 11, 290, 339, 2993, 502, 438, 8807, 340, 3022, 706, 339, 373, 2636, 526, 198, 198, 40, 5710, 616, 3809, 43045, 13, 366, 2215, 673, 1908, 329, 345, 1701, 198, 198, 1, 5297, 438, 37121, 1035, 27339, 284, 262, 21296, 13, 1375, 2227, 683, 29178, 3474, 438, 392, 416, 502, 2474, 198, 198, 1544, 13818, 757, 11, 290, 9617, 736, 465, 1182, 284, 804, 510, 379, 262, 17548, 286, 262, 50085, 13, 366, 1858, 547, 1528, 618, 314, 3521, 470, 804, 379, 326, 1517, 438, 24089, 77, 470, 1986, 340, 13, 887, 314, 4137, 3589, 284, 1234, 340, 994, 26, 290, 783, 340, 338, 30703, 502, 438, 66, 1522, 502, 13, 1320, 338, 262, 1738, 1521, 314, 836, 470, 45553, 903, 597, 517, 11, 616, 13674, 8759, 2763, 26, 393, 2138, 520, 5493, 2241, 318, 262, 1738, 526, 198, 198, 1890, 262, 717, 640, 616, 21696, 20136, 546, 616, 15185, 2900, 656, 257, 2726, 6227, 284, 1833, 683, 1365, 13, 198, 198, 1, 40, 4601, 345, 1549, 1560, 502, 703, 340, 3022, 553, 314, 531, 13, 198, 198, 1544, 6204, 2045, 510, 379, 262, 17548, 11, 290, 665, 24297, 1022, 465, 9353, 257, 17779, 339, 550, 11564, 284, 1657, 13, 24975, 339, 2900, 3812, 502, 13, 198, 198, 1, 40, 1549, 2138, 588, 284, 1560, 345, 438, 13893, 314, 1053, 1464, 9885, 345, 286, 2376, 26927, 616, 670, 526, 198, 198, 40, 925, 257, 1207, 8344, 803, 18342, 11, 543, 339, 2469, 265, 1572, 351, 257, 922, 12, 17047, 8167, 32545, 13, 198, 198, 1, 5812, 11, 314, 1422, 470, 1337, 257, 14787, 618, 314, 4762, 287, 3589, 438, 392, 783, 340, 338, 281, 2087, 9839, 1022, 514, 2474, 198, 198, 1544, 13818, 4622, 11, 1231, 35987, 11, 290, 7121, 530, 286, 262, 2769, 3211, 12, 49655, 2651, 13, 366, 1858, 25, 787, 3511, 6792, 438, 392, 994, 389, 262, 33204, 345, 588, 526, 198, 198, 1544, 4624, 606, 379, 616, 22662, 290, 3767, 284, 27776, 510, 290, 866, 262, 2119, 11, 12225, 783, 290, 788, 11061, 262, 4286, 13, 198, 198, 1, 2437, 340, 3022, 30, 314, 460, 1560, 345, 287, 1936, 2431, 438, 392, 340, 1422, 470, 1011, 881, 2392, 284, 1645, 13, 764, 764, 764, 314, 460, 3505, 783, 703, 6655, 290, 10607, 314, 373, 618, 314, 1392, 9074, 13, 520, 5493, 338, 3465, 13, 3226, 1781, 11, 2769, 866, 11, 314, 550, 1464, 4808, 31985, 62, 612, 373, 645, 530, 588, 683, 438, 8807, 314, 550, 3750, 351, 262, 4269, 11, 22211, 262, 6678, 40315, 10455, 546, 683, 11, 10597, 314, 2063, 1392, 284, 892, 339, 373, 257, 5287, 11, 530, 286, 262, 1611, 326, 389, 1364, 2157, 13, 2750, 449, 659, 11, 290, 339, 4808, 9776, 62, 1364, 2157, 438, 13893, 339, 550, 1282, 284, 2652, 0, 383, 1334, 286, 514, 550, 284, 1309, 6731, 307, 17676, 1863, 393, 467, 739, 11, 475, 339, 373, 1029, 2029, 262, 1459, 438, 261, 45697, 19369, 11, 355, 345, 910, 13, 198, 198, 1, 5779, 11, 314, 1816, 572, 284, 262, 2156, 287, 616, 749, 34372, 10038, 438, 34330, 3888, 11, 4453, 20927, 502, 11, 379, 262, 3108, 418, 286, 3595, 520, 5493, 338, 3451, 286, 5287, 852, 37492, 416, 262, 13476, 286, 616, 12036, 683, 0, 3226, 1781, 314, 4001, 284, 466, 262, 4286, 329, 2147, 438, 40, 1297, 9074, 13, 520, 5493, 523, 618, 673, 2540, 284, 336, 321, 647, 1223, 546, 607, 8098, 13, 314, 3505, 1972, 572, 257, 40426, 10956, 9546, 546, 262, 15393, 852, 4808, 3810, 62, 438, 1219, 11, 314, 373, 19716, 306, 11, 616, 13674, 8759, 2763, 0, 314, 373, 24380, 284, 3589, 588, 530, 286, 616, 898, 1650, 1010, 13, 198, 198, 1, 6423, 314, 373, 2077, 510, 290, 1364, 3436, 351, 683, 13, 314, 550, 1908, 477, 616, 20348, 287, 5963, 11, 290, 314, 550, 691, 284, 900, 510, 262, 1396, 417, 290, 651, 284, 670, 13, 679, 550, 587, 2636, 691, 8208, 12, 14337, 2250, 11, 290, 339, 3724, 6451, 11, 286, 2612, 4369, 11, 523, 326, 612, 550, 587, 645, 15223, 670, 286, 8166, 438, 14363, 1986, 373, 1598, 290, 36519, 13, 314, 550, 1138, 683, 1752, 393, 5403, 11, 812, 878, 11, 290, 1807, 683, 32081, 290, 44852, 88, 13, 2735, 314, 2497, 326, 339, 373, 21840, 13, 198, 198, 1, 40, 373, 9675, 379, 717, 11, 351, 257, 6974, 19713, 14676, 25, 9675, 284, 423, 616, 1021, 319, 884, 257, 705, 32796, 2637, 3244, 465, 6283, 1204, 12, 46965, 9449, 2540, 284, 2689, 502, 24506, 306, 438, 292, 314, 10226, 262, 1182, 287, 314, 2936, 355, 611, 339, 547, 4964, 502, 466, 340, 13, 383, 18098, 373, 3940, 416, 262, 1807, 25, 611, 339, 4808, 22474, 62, 4964, 502, 11, 644, 561, 339, 910, 284, 616, 835, 286, 1762, 30, 2011, 29483, 2540, 284, 467, 257, 1310, 4295, 438, 40, 2936, 10927, 290, 8627, 13, 198, 198, 1, 7454, 11, 618, 314, 3114, 510, 11, 314, 3947, 284, 766, 257, 8212, 2157, 465, 1969, 12768, 680, 21213, 438, 292, 611, 339, 550, 262, 3200, 11, 290, 547, 28297, 2241, 416, 4769, 340, 736, 422, 502, 13, 1320, 41851, 515, 502, 991, 517, 13, 383, 3200, 30, 4162, 11, 314, 550, 257, 3200, 2861, 8208, 286, 465, 0, 314, 37901, 379, 262, 21978, 44896, 11, 290, 3088, 617, 286, 616, 49025, 5330, 15910, 13, 887, 484, 4054, 502, 11, 484, 1067, 11137, 13, 314, 2497, 326, 339, 2492, 470, 4964, 262, 905, 88, 10340, 438, 40, 3521, 470, 11786, 465, 3241, 26, 339, 655, 4030, 465, 2951, 319, 262, 1327, 22674, 1022, 13, 5845, 547, 262, 3392, 314, 550, 1464, 427, 343, 9091, 11, 393, 5017, 510, 351, 617, 9105, 7521, 13, 843, 703, 339, 2497, 832, 616, 7363, 0, 198, 198, 1, 40, 3114, 510, 757, 11, 290, 4978, 6504, 286, 326, 17548, 286, 262, 50085, 10938, 319, 262, 3355, 1474, 465, 3996, 13, 2399, 3656, 1297, 502, 20875, 340, 373, 262, 938, 1517, 339, 550, 1760, 438, 3137, 257, 3465, 2077, 351, 257, 17275, 1021, 11, 618, 339, 373, 866, 287, 6245, 684, 10695, 20222, 422, 257, 2180, 2612, 1368, 13, 2329, 257, 3465, 0, 887, 340, 4952, 465, 2187, 2106, 13, 1318, 389, 812, 286, 5827, 40987, 913, 30802, 287, 790, 1627, 13, 317, 582, 508, 550, 1509, 388, 351, 262, 1459, 714, 1239, 423, 4499, 326, 18680, 510, 12, 5532, 14000, 13, 764, 764, 764, 198, 198, 1, 40, 2900, 736, 284, 616, 670, 11, 290, 1816, 319, 39136, 278, 290, 285, 4185, 1359, 26, 788, 314, 3114, 379, 262, 50085, 757, 13, 314, 2497, 326, 11, 618, 520, 5493, 8104, 287, 262, 717, 14000, 11, 339, 2993, 655, 644, 262, 886, 561, 307, 13, 679, 550, 17273, 465, 2426, 11, 19233, 340, 11, 11027, 515, 340, 13, 1649, 550, 314, 1760, 326, 351, 597, 286, 616, 1243, 30, 1119, 8020, 470, 587, 4642, 286, 502, 438, 40, 550, 655, 8197, 606, 13, 764, 764, 764, 198, 198, 1, 39, 648, 340, 11, 8759, 2763, 11, 351, 326, 1986, 4964, 502, 314, 3521, 470, 466, 1194, 14000, 13, 383, 8631, 3872, 373, 11, 314, 1422, 470, 760, 810, 284, 1234, 340, 438, 62, 40, 550, 1239, 1900, 44807, 5514, 11, 351, 616, 1650, 1010, 290, 616, 1171, 11, 257, 905, 88, 22870, 286, 9568, 5017, 510, 262, 1109, 438, 40, 655, 9617, 7521, 656, 511, 6698, 13, 764, 764, 764, 3894, 11, 7521, 373, 262, 530, 7090, 883, 2636, 2951, 714, 766, 832, 438, 3826, 3892, 284, 262, 2006, 20212, 19369, 14638, 13, 2094, 470, 345, 760, 703, 11, 287, 3375, 257, 3215, 3303, 11, 772, 6562, 1473, 11, 530, 1139, 2063, 262, 640, 407, 644, 530, 3382, 284, 475, 644, 530, 460, 30, 3894, 438, 5562, 373, 262, 835, 314, 13055, 26, 290, 355, 339, 3830, 612, 290, 7342, 502, 11, 262, 1517, 484, 1444, 616, 705, 23873, 2350, 6, 14707, 588, 257, 2156, 286, 4116, 13, 679, 1422, 470, 10505, 263, 11, 345, 1833, 11, 3595, 520, 5493, 438, 258, 655, 3830, 612, 12703, 4964, 11, 290, 319, 465, 11914, 11, 832, 262, 12768, 21213, 11, 314, 3947, 284, 3285, 262, 1808, 25, 705, 8491, 345, 1654, 345, 760, 810, 345, 821, 2406, 503, 8348, 198, 198, 1, 1532, 314, 714, 423, 13055, 326, 1986, 11, 351, 326, 1808, 319, 340, 11, 314, 815, 423, 1760, 257, 1049, 1517, 13, 383, 1306, 6000, 1517, 373, 284, 766, 326, 314, 3521, 470, 438, 392, 326, 11542, 373, 1813, 502, 13, 887, 11, 11752, 11, 379, 326, 5664, 11, 8759, 2763, 11, 373, 612, 1997, 319, 4534, 314, 3636, 470, 423, 1813, 284, 423, 520, 5493, 6776, 878, 502, 11, 290, 284, 3285, 683, 910, 25, 705, 1026, 338, 407, 1165, 2739, 438, 40, 1183, 905, 345, 703, 30960, 198, 198, 1, 1026, 4808, 9776, 62, 1165, 2739, 438, 270, 561, 423, 587, 11, 772, 611, 339, 1549, 587, 6776, 13, 314, 11856, 510, 616, 20348, 11, 290, 1816, 866, 290, 1297, 9074, 13, 520, 5493, 13, 3226, 1781, 314, 1422, 470, 1560, 607, 4808, 5562, 62, 438, 270, 561, 423, 587, 8312, 284, 607, 13, 314, 2391, 531, 314, 3521, 470, 7521, 683, 11, 326, 314, 373, 1165, 3888, 13, 1375, 2138, 8288, 262, 2126, 438, 7091, 338, 523, 14348, 0, 632, 373, 326, 326, 925, 607, 1577, 502, 262, 50085, 13, 887, 673, 373, 22121, 9247, 379, 407, 1972, 262, 18560, 438, 7091, 750, 523, 765, 683, 705, 28060, 6, 416, 617, 530, 905, 88, 0, 1629, 717, 314, 373, 7787, 673, 3636, 470, 1309, 502, 572, 438, 392, 379, 616, 266, 896, 6, 886, 314, 5220, 41379, 293, 13, 3363, 11, 340, 373, 314, 508, 2067, 41379, 293, 25, 314, 1297, 9074, 13, 520, 5493, 339, 373, 262, 705, 4976, 6, 582, 11, 290, 673, 1297, 8276, 2073, 11, 290, 523, 340, 1392, 284, 307, 2081, 13, 764, 764, 764, 843, 339, 13055, 520, 5493, 1231, 1592, 2259, 26, 290, 673, 9174, 262, 4286, 1871, 607, 5229, 338, 1243, 13, 764, 764, 22135, 198, 198, 1544, 45111, 2241, 866, 287, 262, 3211, 12, 16337, 1474, 6164, 11, 8104, 736, 465, 1182, 11, 290, 47425, 278, 465, 5101, 11061, 340, 11, 3114, 510, 379, 262, 4286, 2029, 262, 18205, 1681, 12, 12239, 13, 198, 198, 1, 40, 588, 284, 14996, 326, 520, 5493, 2241, 561, 423, 1813, 340, 284, 502, 11, 611, 339, 1549, 587, 1498, 284, 910, 644, 339, 1807, 326, 1110, 526, 198, 198, 1870, 11, 287, 3280, 284, 257, 1808, 314, 1234, 2063, 12, 1326, 3147, 1146, 438, 1, 44140, 757, 1701, 339, 30050, 503, 13, 366, 2215, 262, 530, 1517, 326, 6774, 502, 6609, 1474, 683, 318, 326, 314, 2993, 1576, 284, 2666, 572, 1701, 198, 198, 1544, 6204, 510, 290, 8104, 465, 1021, 319, 616, 8163, 351, 257, 6487, 13, 366, 10049, 262, 21296, 286, 340, 318, 326, 314, 4808, 321, 62, 991, 12036, 438, 20777, 41379, 293, 338, 1804, 340, 329, 502, 0, 383, 520, 5493, 82, 1302, 3436, 11, 290, 1645, 1752, 438, 4360, 612, 338, 645, 42393, 803, 674, 1611, 286, 1242, 526]\n","x: [290, 4920, 2241, 287]\n","y: [4920, 2241, 287, 257]\n","[290] ------> 4920\n","[290, 4920] ------> 2241\n","[290, 4920, 2241] ------> 287\n","[290, 4920, 2241, 287] ------> 257\n","decoded text\n"," and ------>  established\n"," and established ------>  himself\n"," and established himself ------>  in\n"," and established himself in ------>  a\n"]}]},{"cell_type":"code","source":["# implement a data loader that fetches the i/p, o/p pair in a sliding windown pattern.\n","# x = tensor( [\n","#     ['In', 'the' , 'heart', 'of'],\n","      # ['the', 'city', 'stood', 'the'],\n","# ])\n","\n","# y = tensor([\n","#     ['the', 'heart', 'of', 'the'],\n","#     ['city', 'stood', 'the', 'old'],\n","# ])\n","\n","# Each row of X represents one input context, correspoinding row of Y represents the output (made by shifting the input by 1 position).\n","# Each ip op pair contains 'context_size' number of prediction samples.\n","\n","from torch.utils.data import Dataset, DataLoader\n","class GPTDatasetV1(Dataset):\n","  def __init__(self, text, tokenizer, max_length, stride):\n","    \"\"\"\n","    max_length: context_length\n","    text: raw text\n","    tokenizer: BPE tokenizer\n","    stride: sliding window stride\n","    \"\"\"\n","    self.input_ids = []\n","    self.target_ids = []\n","    token_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n","    for i in range(0, len(token_ids)-max_length, stride):\n","      input_chunk = token_ids[i:i+max_length]\n","      target_chunk = token_ids[i+1:i+max_length+1]\n","      self.input_ids.append(torch.tensor(input_chunk))\n","      self.target_ids.append(torch.tensor(target_chunk))\n","\n","  def __len__(self):\n","    return len(self.input_ids)\n","\n","  def __getitem__(self, idx):\n","    return self.input_ids[idx], self.target_ids[idx]"],"metadata":{"id":"OmNV3Pf2Cr-K","executionInfo":{"status":"ok","timestamp":1742357794503,"user_tz":420,"elapsed":1,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["def create_dataloader_v1(text, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n","  tokenizer = tiktoken.get_encoding(\"gpt2\")\n","  dataset = GPTDatasetV1(text, tokenizer, max_length, stride)\n","  return DataLoader(dataset,\n","                    batch_size=batch_size,\n","                    shuffle=shuffle,\n","                    drop_last=drop_last,\n","                    num_workers=num_workers)"],"metadata":{"id":"bg-luV8bF_9c","executionInfo":{"status":"ok","timestamp":1742357794534,"user_tz":420,"elapsed":31,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["import torch\n","print(\"torch version\", torch.__version__)\n","\n","with open('the-verdict.txt', 'r', encoding='UTF-8') as f:\n","    raw_text = f.read()\n","\n","print(\"batch size 1, stride 1\")\n","dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n","data_iter = iter(dataloader)\n","x, y = next(data_iter)\n","print(x)\n","print(y)\n","\n","print(\"batch size 8, stride 4\")\n","dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n","data_iter = iter(dataloader)\n","x, y = next(data_iter)\n","print(x)\n","print(y)\n","\n","# some tradoff to think about:\n","# smaller batch size uses less memory, but noisy model updates. its a hyperparam to explore.\n","# low stride could lead to overfitting"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pHW5fjOqIAYg","executionInfo":{"status":"ok","timestamp":1742357795000,"user_tz":420,"elapsed":466,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"3d8b2027-c3fd-4add-fa11-b9a8419add44"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["torch version 2.6.0+cu124\n","batch size 1, stride 1\n","tensor([[  40,  367, 2885, 1464]])\n","tensor([[ 367, 2885, 1464, 1807]])\n","batch size 8, stride 4\n","tensor([[   40,   367,  2885,  1464],\n","        [ 1807,  3619,   402,   271],\n","        [10899,  2138,   257,  7026],\n","        [15632,   438,  2016,   257],\n","        [  922,  5891,  1576,   438],\n","        [  568,   340,   373,   645],\n","        [ 1049,  5975,   284,   502],\n","        [  284,  3285,   326,    11]])\n","tensor([[  367,  2885,  1464,  1807],\n","        [ 3619,   402,   271, 10899],\n","        [ 2138,   257,  7026, 15632],\n","        [  438,  2016,   257,   922],\n","        [ 5891,  1576,   438,   568],\n","        [  340,   373,   645,  1049],\n","        [ 5975,   284,   502,   284],\n","        [ 3285,   326,    11,   287]])\n"]}]},{"cell_type":"code","source":["# Chapter: Token embeddings (source: https://www.youtube.com/watch?v=ghCSGRgVB_o&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=10)\n","# embedding dimension, model\n","# why embeddings are needed?\n","# - computer needs numerical representation of all_words\n","# - why not use 1d embeddings? we cannot use random numbers, some words are relatd to other words. Just assigning number doesnt capture semantic meaning between words\n","# - why cnn work well? they dont use individual pixel only, they encode spatial relation between pixels. inherently present in an image.\n","# one hot encoding -  assing 1 to each token. this also leads to similar problem like random number. this also fails to capture semantic relation.\n","\n","# how many dimension - what if this is determined by features? e.g has a tail, eatbale, has 4 legs, makes sound, is a pet. (for dog, cat, apple, banana). vectors can capture semantic meaning.\n","# can we create a NN which learns how to represent the vector embedding of the tokens."],"metadata":{"id":"2Sfm-rSUIZ83","executionInfo":{"status":"ok","timestamp":1742357795000,"user_tz":420,"elapsed":1,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["# commented out since this is a 1.6G download\n","# import gensim.downloader as api\n","# model = api.load(\"word2vec-google-news-300\")\n","# word_vectors = model\n","# word_vectors.most_similar(positive=['king', 'man'], negative=['woman'], topn=1)\n","# word_vectors.similarity('king', 'man')"],"metadata":{"id":"8BrTaW4sHNu2","executionInfo":{"status":"ok","timestamp":1742357795009,"user_tz":420,"elapsed":10,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["# for llm, embeddings are randomly intialized. the params are learnt during the training via backpropagation.\n","# embedding matrix = token ID X vector dimension. token dimension = vocab size. vector dimension is a hyper param. e.g 768 for gpt2\n","vocab_size = 6 # num tokens\n","output_dim = 3 # vector dimension of embeddings\n","\n","torch.manual_seed(123)\n","# embedding_layer can be thought as a neural network with input dimension of vocab_size, and output neurons of size output_dim. Output X.WT. W is weight matrix of dimension vocab_size * output_dim. token id are covnerted to one hot representation and passed as input to the NN. (torch.nn.linear). both embedding and linear get to the same output. its not efficient due to one hot encoding.\n","embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n","print(embedding_layer.weight)\n","\n","# print the vector for token id 3\n","print(embedding_layer.weight[5])\n","\n","idx = torch.tensor([2,3,1]) # retrive vectors for token ID 2,3,1\n","print(embedding_layer.weight[idx])"],"metadata":{"id":"lO04EYerHeSK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742357795132,"user_tz":420,"elapsed":122,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"04bc84e4-7bbc-4eef-acd5-3566560dc33c"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Parameter containing:\n","tensor([[ 0.3374, -0.1778, -0.1690],\n","        [ 0.9178,  1.5810,  1.3010],\n","        [ 1.2753, -0.2010, -0.1606],\n","        [-0.4015,  0.9666, -1.1481],\n","        [-1.1589,  0.3255, -0.6315],\n","        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n","tensor([-2.8400, -0.7849, -1.4096], grad_fn=<SelectBackward0>)\n","tensor([[ 1.2753, -0.2010, -0.1606],\n","        [-0.4015,  0.9666, -1.1481],\n","        [ 0.9178,  1.5810,  1.3010]], grad_fn=<IndexBackward0>)\n"]}]},{"cell_type":"code","source":["# Chapter: Positional encoding (source: https://www.youtube.com/watch?v=ufrPLpKnapU&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=11)\n","# In embedding, same token Id gets converted to same embedding. it is useful to encode the postio where the token appears.\n","# 2 types - absolute and relative\n","# absolute emb - each position in i/p sequence, a unique embedding is added to token emb. pos vectors has same dimension as token emb.\n","# relative emb - relative pos distance between tokens. \"how far apart\" is learnt by the model. the model can generalize better sequnece of varying length, that the model has not seen in training phase. e.g longer sequence\n","\n","# GPT was trained using absolute. the positional emb was learnt during the traning of the model itself."],"metadata":{"id":"9qkLgZ0pLIud","executionInfo":{"status":"ok","timestamp":1742357795133,"user_tz":420,"elapsed":1,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["vocab_size = 50257\n","output_dim = 256\n","token_emb_layer = torch.nn.Embedding(vocab_size, output_dim)\n","max_length = 4 # context length\n","batch_size = 8\n","dataloader = create_dataloader_v1(raw_text, batch_size=batch_size,max_length=max_length, stride=max_length, shuffle=False)\n","# this makes input 8 X 4 (batch dim, tokens). For each token convert to a 256 dim vector using the token_emb_layer which is a (vocab_size, output_dim) matrix.\n","data_iter = iter(dataloader)\n","inputs, target = next(data_iter)\n","print(\"Token IDs:\\n\", inputs)\n","print(\"Input shape:\\n\", inputs.shape)\n","\n","token_embeddigs = token_emb_layer(inputs)\n","print(\"token emb dim\", token_embeddigs.shape)\n","\n","# now lets create the pos emb\n","context_length = max_length\n","pos_emb_layer = torch.nn.Embedding(context_length, output_dim)\n","pos_embeddings = pos_emb_layer(torch.arange(max_length))\n","print(\"pos emb dim\", pos_embeddings.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TkaO3T4VSbPV","executionInfo":{"status":"ok","timestamp":1742357795727,"user_tz":420,"elapsed":591,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"9b3133a1-f04c-4074-83b9-bdf6f6693ad8"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Token IDs:\n"," tensor([[   40,   367,  2885,  1464],\n","        [ 1807,  3619,   402,   271],\n","        [10899,  2138,   257,  7026],\n","        [15632,   438,  2016,   257],\n","        [  922,  5891,  1576,   438],\n","        [  568,   340,   373,   645],\n","        [ 1049,  5975,   284,   502],\n","        [  284,  3285,   326,    11]])\n","Input shape:\n"," torch.Size([8, 4])\n","token emb dim torch.Size([8, 4, 256])\n","pos emb dim torch.Size([4, 256])\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"tPyacIkASb1Q"}},{"cell_type":"code","source":["# Summarizing the data preprocessing pipeline of LLM\n","# 4 steps\n","# - tokenization (subword), token embeddings, pos embeddings, input embeddings\n","\n","# Stages of LLM\n","# building llm -> data preprocess, attention, llm arch\n","# foundation model -> training loop, model eval, load pre-trained weights\n","# finetuning"],"metadata":{"id":"3bhREHbkWqJK","executionInfo":{"status":"ok","timestamp":1742357795774,"user_tz":420,"elapsed":46,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":["Attention mechanism (source: https://www.youtube.com/watch?v=XN7sevVxyUM&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=13 )\n","\n","lets say \"the cat that was sitting on the mat, which was sitting next to the dog, jumped\"\n","\n","llm needs to pay attention that the cat is 'sitting' 'jumped'. the key action is jumped, cat is the main subject. also need understandig of other dependencies of the cat.\n","\n","study plan:\n","- simplified attention\n","- self attention (introduce the trainable weights)\n","- causal attention (only look at previous tokens to predict the next token for auto regressive models)\n","- multi-head attention (extension of self attention, allows the model to simulatnously attend to info from different representation spaces)\n","\n","\n","problem statement: long sequences.\n","\n","what is problem with arch that came before llms? e.h word by word translation does not work (needs contextual understandig and grammar alignment). to address this models need to have some notion of memory\n","\n","encoder - processes the entire text and captures the semantic menaing in the context vector. this is passed to the decoder. RNN was such encoder decoder architecture. hidden state was an innovation in RNN. hidden state is updated as it ingests more input in the sequence. the final hidden state is the encoder output.\n","\n","decoder -\n","\n","why attention? in RNN , decoder only has access to the final hidden state. for long sequences its inefficient. RNN cannot access the intermediate hidden states during decoding phase. this leads to loss of context (specially for long sequences)\n","\n","Bahdanau attention mechanism in 2014 (precursor to the 'attention is all you need' paper in 2017).\n","modify the RNN, decoder can selectively access the different parts of the input sequence at each decoding step.\n","\n","1980 - RNN\n","1997 - LSTM (solve the vanishing gradient problem of RNN)\n","2014 - Bahdanau Attention (access intermediate hidden state)\n","2017 - Transformers (self attention, multi head attention, emergent behavior - trained for next word prediction, learnt translation)\n","\n","\n","Self attention: allows each position in the seq to attend to all pos in the sequence\n","\n","\n","\n"],"metadata":{"id":"6oFQ65EUXWns"}},{"cell_type":"code","source":["# Chapter: simplified attention (https://www.youtube.com/watch?v=eSRhpYLerw4&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=15)\n","# Your journey starts with one step\n","# preprocess - token ID, vec embedding and pos encoding in higher dimension\n","# input x(1) - token 1 embedding\n","# goal of attention is to create a context vector z(i) for each element x(i) that captures meaning from the input vectors\n","# we need to find a context vector for each input token x(i)\n","# lets focus on x(2). the current element is the query. z(2) would contain info about x(2) and all input elements x(1) .. x(t)\n","# we need to compute w(ij), the attention scores between input x(i) and x(j).\n","\n","# dot product of vector a and b is |a||b| cos0. higher dot product scores are similar vectors\n","# this is what is used for attention scores between query vector and the input vector.\n","\n","import torch\n","inputs = torch.tensor([\n","        [0.43, 0.15, 0.89], # your x(1)\n","        [0.55, 0.87, 0.66],  # journey x(2)\n","        [0.57, 0.85, 0.64], # starts x(3)\n","        [0.22, 0.58, 0.33], # with x(4)\n","        [0.77, 0.25, 0.10],  # one x(5)\n","        [0.05, 0.80, 0.55], # step x(6)\n","    ])\n","\n","print(inputs)\n","\n","\n","query = inputs[1] # second token x(2), second row in inputs\n","attn_scores = torch.empty(inputs.shape[0])\n","for i in range(inputs.shape[0]):\n","  attn_scores[i] = torch.dot(query, inputs[i])\n","\n","print(\"attn_scores:\", attn_scores, \"shape:\", attn_scores.shape)\n","\n","# normalization: need attention weights to sum to 1 to talk in terms of percentage weightage to the input. also this helps maintain stability in training\n","\n","\n","attn_weights_2_tmp = attn_scores / torch.sum(attn_scores) # normalization\n","print(attn_weights_2_tmp.shape)\n","print(\"attn_weights_2_tmp:\", attn_weights_2_tmp)\n","print(torch.sum(attn_weights_2_tmp))\n","\n","# but there are better ways to do normalization. softmax is preferred. use torch softmax which does e^(x1-max) for each element.\n","def softmax_naive(x):\n","  return torch.exp(x) / torch.exp(x).sum(dim=0)\n","\n","attn_weights_2_naive = softmax_naive(attn_scores)\n","print(\"attn_weights_2:\", attn_weights_2_naive)\n","print(torch.sum(attn_weights_2_naive))\n","\n","# now sum the weights * input for the context vector\n","query = inputs[1] # second token x(2), second row in inputs\n","context_vec_2 = torch.zeros(query.shape)\n","for i, x_i in enumerate(inputs):\n","  context_vec_2 += attn_weights_2_naive[i] * x_i\n","\n","print(context_vec_2) # context vector for input x(2)\n","\n","# Summary:\n","# 1. compute attention scores (dot product of each vector with every other input vector)\n","# 2. compute attention weights (softmax w)\n","# 3. compute context vector for the given input vector (w * x)\n","attn_scores = torch.empty(6,6)\n","\n","# step 1\n","# naive for loop\n","for i, x_i in enumerate(inputs):\n","  for j, x_j in enumerate(inputs):\n","    attn_scores[i,j] = torch.dot(x_i, x_j)\n","\n","print(\"attn_scores:\\n\", attn_scores) # each cell (i,j) of attn score between x(i) and x(j)\n","\n","# simpler way to do this same\n","attn_scores = torch.matmul(inputs, inputs.T) # input @ input.T\n","print(\"attn_scores:\\n\", attn_scores)\n","\n","# step 2\n","# softmax of each row\n","attn_weights = torch.softmax(attn_scores, dim=-1) # dim param uses last dimension along which softmax would be executed\n","print(\"attn_weights:\\n\", attn_weights)\n","\n","\n","# verify each row sum to 1\n","print(\"attn_weights row sum\", torch.sum(attn_weights, dim=-1))\n","\n","\n","# step 3\n","# context vector for all input\n","all_context_vec = attn_weights @ inputs # this scales every row of input (which is x(i) by the attention weight)\n","print(\"all_context_vec shape:\\n\", all_context_vec.shape)\n","print(\"all_context_vec:\\n\", all_context_vec) # every row is the corresponding context vector z(i) of input x(i)"],"metadata":{"id":"NhTRwM6eXcoJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742357795908,"user_tz":420,"elapsed":134,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"08362354-66c8-4694-c7ee-1313db340dce"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.4300, 0.1500, 0.8900],\n","        [0.5500, 0.8700, 0.6600],\n","        [0.5700, 0.8500, 0.6400],\n","        [0.2200, 0.5800, 0.3300],\n","        [0.7700, 0.2500, 0.1000],\n","        [0.0500, 0.8000, 0.5500]])\n","attn_scores: tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865]) shape: torch.Size([6])\n","torch.Size([6])\n","attn_weights_2_tmp: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n","tensor(1.0000)\n","attn_weights_2: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n","tensor(1.)\n","tensor([0.4419, 0.6515, 0.5683])\n","attn_scores:\n"," tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n","        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n","        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n","        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n","        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n","        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n","attn_scores:\n"," tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n","        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n","        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n","        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n","        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n","        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n","attn_weights:\n"," tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n","        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n","        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n","        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n","        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n","        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n","attn_weights row sum tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n","all_context_vec shape:\n"," torch.Size([6, 3])\n","all_context_vec:\n"," tensor([[0.4421, 0.5931, 0.5790],\n","        [0.4419, 0.6515, 0.5683],\n","        [0.4431, 0.6496, 0.5671],\n","        [0.4304, 0.6298, 0.5510],\n","        [0.4671, 0.5910, 0.5266],\n","        [0.4177, 0.6503, 0.5645]])\n"]}]},{"cell_type":"markdown","source":["why do we need trainable weights for Q,K,V?\n","\n","\"the cat sat on the mat because it was warm\"\n","if we only use dot product,  of the query warm with each word emb, then we might find warm is only similar to itself. words like cat, sat may get low similarity score. with trainable weights model learns to pay attention to cat, mat."],"metadata":{"id":"ZyEF2v9BWSmS"}},{"cell_type":"code","source":["# Chapter: Coding self attention with query, key and value (https://www.youtube.com/watch?v=UjdRN80c6p8&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=15)\n","# context vector is enriched input vector\n","# trainable params - allows model to produce good context vector. Wq, Wk, Wv - 3 trainbale weight matrices\n","# convert input vectors to Q, K , V vectors\n","# Ip - row i represents the embeddning for token i\n","# say 6 X 3 input, 6 inputs, 3 emb dimension (d_in). Wq,k,v dimension 3X2. 2 is d_out. project the input into a different dimension space.\n","# So Ip * Wq, Ip * Wk, Ip * Wv results in Q, K, V matrices (of size 6 X 2).  Each row of Q,K,V matrics corresponds to an input token.\n","# \"scaled dot product attention\"\n","\n","import torch\n","inputs = torch.tensor([\n","        [0.43, 0.15, 0.89], # your x(1)\n","        [0.55, 0.87, 0.66],  # journey x(2)\n","        [0.57, 0.85, 0.64], # starts x(3)\n","        [0.22, 0.58, 0.33], # with x(4)\n","        [0.77, 0.25, 0.10],  # one x(5)\n","        [0.05, 0.80, 0.55], # step x(6)\n","    ])\n","\n","x_2 = inputs[1] # first token\n","d_in = inputs.shape[1] # emb dimension\n","d_out = 2 # sample value used for this example\n","torch.manual_seed(123)\n","W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n","W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n","W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n","print(W_query)\n","\n","\n","query_2 = x_2 @ W_query\n","print(query_2, query_2.shape)\n","key_2 = x_2 @ W_key\n","print(key_2, key_2.shape)\n","value_2 = x_2 @ W_value\n","print(value_2, value_2.shape)\n","\n","\n","\n","keys = inputs @ W_key\n","print(keys, keys.shape)\n","queries = inputs @ W_query\n","print(queries, queries.shape)\n","values = inputs @ W_value\n","print(values, values.shape)\n","\n","\n","attn_scores = queries @ keys.T\n","print(attn_scores, attn_scores.shape)\n","\n","# scale the attn scores by sqrt(emb dimension of keys matrix), here d_keys = 2. so we scale this example by sqrt(2)\n","# why?\n","# 1. stability of learning - applying softmax without scaling, values can be disproportionately  high. highest value can consume almost all the prob mass. \"peaky softmax\"\n","# in LLM, if dot product between Q and K becomes too large, attn scores can become large. results in sharp softmax distrubution. model can become overly confidet in a key.\n","# why sqrt(), its related to the variance. dot product increases variance proportionately. dividing the sqrt of dimension keeps variance close to 1.\n","d_k = keys.shape[-1]\n","print(\"scale_factor d_k\", d_k)\n","attn_weights = torch.softmax(attn_scores/ d_k ** 0.5, dim=-1)\n","print(attn_weights, attn_weights.shape)\n","\n","\n","# lets compute the context vector\n","# each row of i of attn_weight, captures the weightage of input i w.r.t to other inputs. we scale each row of V (which represents the value vector corresponding to each input token x), and then sum to get the final context vector\n","context_vec = attn_weights @ values\n","print(context_vec, context_vec.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FHyvl81vXRXs","executionInfo":{"status":"ok","timestamp":1742357795953,"user_tz":420,"elapsed":50,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"bf263019-d0ba-448b-fbef-9d007355391c"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Parameter containing:\n","tensor([[0.2961, 0.5166],\n","        [0.2517, 0.6886],\n","        [0.0740, 0.8665]])\n","tensor([0.4306, 1.4551]) torch.Size([2])\n","tensor([0.4433, 1.1419]) torch.Size([2])\n","tensor([0.3951, 1.0037]) torch.Size([2])\n","tensor([[0.3669, 0.7646],\n","        [0.4433, 1.1419],\n","        [0.4361, 1.1156],\n","        [0.2408, 0.6706],\n","        [0.1827, 0.3292],\n","        [0.3275, 0.9642]]) torch.Size([6, 2])\n","tensor([[0.2309, 1.0966],\n","        [0.4306, 1.4551],\n","        [0.4300, 1.4343],\n","        [0.2355, 0.7990],\n","        [0.2983, 0.6565],\n","        [0.2568, 1.0533]]) torch.Size([6, 2])\n","tensor([[0.1855, 0.8812],\n","        [0.3951, 1.0037],\n","        [0.3879, 0.9831],\n","        [0.2393, 0.5493],\n","        [0.1492, 0.3346],\n","        [0.3221, 0.7863]]) torch.Size([6, 2])\n","tensor([[0.9231, 1.3545, 1.3241, 0.7910, 0.4032, 1.1330],\n","        [1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n","        [1.2544, 1.8284, 1.7877, 1.0654, 0.5508, 1.5238],\n","        [0.6973, 1.0167, 0.9941, 0.5925, 0.3061, 0.8475],\n","        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707, 0.7307],\n","        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]]) torch.Size([6, 6])\n","scale_factor d_k 2\n","tensor([[0.1551, 0.2104, 0.2059, 0.1413, 0.1074, 0.1799],\n","        [0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820],\n","        [0.1503, 0.2256, 0.2192, 0.1315, 0.0914, 0.1819],\n","        [0.1591, 0.1994, 0.1962, 0.1477, 0.1206, 0.1769],\n","        [0.1610, 0.1949, 0.1923, 0.1501, 0.1265, 0.1752],\n","        [0.1557, 0.2092, 0.2048, 0.1419, 0.1089, 0.1794]]) torch.Size([6, 6])\n","tensor([[0.2996, 0.8053],\n","        [0.3061, 0.8210],\n","        [0.3058, 0.8203],\n","        [0.2948, 0.7939],\n","        [0.2927, 0.7891],\n","        [0.2990, 0.8040]]) torch.Size([6, 2])\n"]}]},{"cell_type":"code","source":["import torch.nn as nn\n","class SelfAttention_v1(nn.Module):\n","    def __init__(self, d_in, d_out) -> None:\n","      \"\"\"\n","      d_in: embedding dimension\n","      d_out: output dimension of the Q,K,V space\n","      \"\"\"\n","      super().__init__()\n","      self.W_query = nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n","      self.W_key = nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n","      self.W_value = nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n","      # we can use nn.Linear which has an optimized weight init scheme, which results in stable training.\n","\n","    def forward(self, x):\n","      keys = x @ W_key\n","      queries = x @ W_query\n","      values = x @ W_value\n","\n","      attn_scores = queries @ keys.T\n","      attn_weights = torch.softmax(attn_scores/keys.shape[-1] ** 0.5, dim=-1)\n","\n","      context_vec = attn_weights @ values\n","      return context_vec"],"metadata":{"id":"EXnOaBxEMrXn","executionInfo":{"status":"ok","timestamp":1742357796019,"user_tz":420,"elapsed":61,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(123)\n","sa_v1 = SelfAttention_v1(d_in, d_out)\n","print(sa_v1(inputs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FSoCDaH5hq-L","executionInfo":{"status":"ok","timestamp":1742357796066,"user_tz":420,"elapsed":47,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"b6bc3f40-959f-4c11-83db-51c8615656b0"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.2996, 0.8053],\n","        [0.3061, 0.8210],\n","        [0.3058, 0.8203],\n","        [0.2948, 0.7939],\n","        [0.2927, 0.7891],\n","        [0.2990, 0.8040]])\n"]}]},{"cell_type":"code","source":["class SelfAttention_v2(nn.Module):\n","    def __init__(self, d_in, d_out, qkv_bias=False) -> None:\n","      \"\"\"\n","      d_in: embedding dimension\n","      d_out: output dimension of the Q,K,V space\n","      \"\"\"\n","      super().__init__()\n","      # we can use nn.Linear which has an optimized weight init scheme, which results in stable training.\n","      self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n","      self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n","      self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n","\n","    def forward(self, x):\n","      keys = x @ W_key\n","      queries = x @ W_query\n","      values = x @ W_value\n","\n","      attn_scores = queries @ keys.T\n","      attn_weights = torch.softmax(attn_scores/keys.shape[-1] ** 0.5, dim=-1)\n","\n","      context_vec = attn_weights @ values\n","      return context_vec"],"metadata":{"id":"FPOqo88gh8Rf","executionInfo":{"status":"ok","timestamp":1742357796066,"user_tz":420,"elapsed":1,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(789)\n","sa_v2 = SelfAttention_v2(d_in, d_out)\n","print(sa_v2(inputs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L8L5id1Giyav","executionInfo":{"status":"ok","timestamp":1742357796140,"user_tz":420,"elapsed":74,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"30a8e996-a3df-49eb-fdb5-57280ab4887f"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.2996, 0.8053],\n","        [0.3061, 0.8210],\n","        [0.3058, 0.8203],\n","        [0.2948, 0.7939],\n","        [0.2927, 0.7891],\n","        [0.2990, 0.8040]])\n"]}]},{"cell_type":"code","source":["# Chapter: Causal Self attention (https://www.youtube.com/watch?v=h94TQOK7NRA&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=17)\n","# causal attention is also called masked attention. a special form of self attention. only consider previous and current input tokens.\n","# this looks like in the attn weight matrix, the upper triangle (above the diagonal) is masked out.\n","\n","# attn score matrix (unnormalized) -> attn weights (each row sum to 1) -> apply mask -> masked attn weights (again sum to 1 to the subset of the values in a given row)\n","\n","inputs = torch.tensor([\n","        [0.43, 0.15, 0.89], # your x(1)\n","        [0.55, 0.87, 0.66],  # journey x(2)\n","        [0.57, 0.85, 0.64], # starts x(3)\n","        [0.22, 0.58, 0.33], # with x(4)\n","        [0.77, 0.25, 0.10],  # one x(5)\n","        [0.05, 0.80, 0.55], # step x(6)\n","    ])\n","\n","queries = sa_v2.W_query(inputs)\n","keys = sa_v2.W_key(inputs)\n","values = sa_v2.W_value(inputs)\n","attn_scores = queries @ keys.T\n","attn_weights = torch.softmax(attn_scores/keys.shape[-1] ** 0.5, dim=1)\n","print(attn_weights)\n","\n","\n","# use pytorch tril func to create a mask where values would be zero above the diagonal\n","context_length =  attn_scores.shape[1]\n","mask_simple = torch.tril(torch.ones(context_length, context_length))\n","print(mask_simple)\n","\n","# note: attn_weights is already influenced by the future items due to the softmax applied on the original attn_scores.\n","attn_weights_masked = attn_weights * mask_simple\n","print(attn_weights_masked)\n","row_sums = attn_weights_masked.sum(dim=1, keepdim=True)\n","print(row_sums)\n","attn_weights_masked_norm = attn_weights_masked / row_sums\n","print(attn_weights_masked_norm)\n","\n","# a more efficient way is to use the original attn scores, apply upper tril infinity mask -> softmax\n","# attn_scores_masked = attn_scores.masked_fill(mask_simple == 0, float('-inf'))\n","# attn_weights_masked = torch.softmax(attn_scores_masked / keys.shape[-1] ** 0.5, dim=1)\n","# print(attn_weights_masked)\n","\n","mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n","print (\"mask\\n\", mask, \"bool\", mask.bool())\n","attn_scores_masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n","print(\"attn_scores_masked\\n\", attn_scores_masked)\n","attn_weights_masked = torch.softmax(attn_scores_masked / keys.shape[-1] ** 0.5, dim=1)\n","print(attn_weights_masked)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-3tDbZZSiyv0","executionInfo":{"status":"ok","timestamp":1742357796208,"user_tz":420,"elapsed":69,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"0ae4a580-e355-4b6c-f89e-d815b664c499"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n","        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n","        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n","        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n","        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n","        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n","       grad_fn=<SoftmaxBackward0>)\n","tensor([[1., 0., 0., 0., 0., 0.],\n","        [1., 1., 0., 0., 0., 0.],\n","        [1., 1., 1., 0., 0., 0.],\n","        [1., 1., 1., 1., 0., 0.],\n","        [1., 1., 1., 1., 1., 0.],\n","        [1., 1., 1., 1., 1., 1.]])\n","tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n","        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n","        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n","        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n","       grad_fn=<MulBackward0>)\n","tensor([[0.1921],\n","        [0.3700],\n","        [0.5357],\n","        [0.6775],\n","        [0.8415],\n","        [1.0000]], grad_fn=<SumBackward1>)\n","tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n","        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n","        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n","        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n","       grad_fn=<DivBackward0>)\n","mask\n"," tensor([[0., 1., 1., 1., 1., 1.],\n","        [0., 0., 1., 1., 1., 1.],\n","        [0., 0., 0., 1., 1., 1.],\n","        [0., 0., 0., 0., 1., 1.],\n","        [0., 0., 0., 0., 0., 1.],\n","        [0., 0., 0., 0., 0., 0.]]) bool tensor([[False,  True,  True,  True,  True,  True],\n","        [False, False,  True,  True,  True,  True],\n","        [False, False, False,  True,  True,  True],\n","        [False, False, False, False,  True,  True],\n","        [False, False, False, False, False,  True],\n","        [False, False, False, False, False, False]])\n","attn_scores_masked\n"," tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n","        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n","        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n","        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n","        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n","        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n","       grad_fn=<MaskedFillBackward0>)\n","tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n","        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n","        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n","        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n","       grad_fn=<SoftmaxBackward0>)\n"]}]},{"cell_type":"code","source":["#Dropout - randomly selected neurons in the hidden layer is ignored. this prevents overfitting.\n","# some neurons become lazy, do not do any work. co dependency problem. a lazy neuron is forced to do the work, when other neuron is dropped out.\n","# dropout is applied in 2 places:\n","# 1. after calculating the attn scores\n","# 2. after applying weights to the value vector\n","\n","# below code drop out 50% of values.\n","torch.manual_seed(123)\n","dropout = torch.nn.Dropout(0.5)\n","example = torch.ones(6,6)\n","print(dropout(example)) # note: all other values are rescaled by that much amount (i.e divided by 1/1-p) to compensate for reduction of active nodes\n","\n","\n","torch.manual_seed(123)\n","print(\"attn_weights_masked\\n\", attn_weights_masked)\n","attn_weights_dropout = dropout(attn_weights_masked)\n","print(attn_weights_dropout)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4E6jlrAyC2dU","executionInfo":{"status":"ok","timestamp":1742357796319,"user_tz":420,"elapsed":111,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"20b0237c-98ec-43d6-b88c-e937634c5e69"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[2., 2., 2., 2., 2., 2.],\n","        [0., 2., 0., 0., 0., 0.],\n","        [0., 0., 2., 0., 2., 0.],\n","        [2., 2., 0., 0., 0., 2.],\n","        [2., 0., 0., 0., 0., 2.],\n","        [0., 2., 0., 0., 0., 0.]])\n","attn_weights_masked\n"," tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n","        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n","        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n","        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n","       grad_fn=<SoftmaxBackward0>)\n","tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.0000, 0.8966, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.0000, 0.0000, 0.6206, 0.0000, 0.0000, 0.0000],\n","        [0.5517, 0.4921, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.4350, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.0000, 0.3327, 0.0000, 0.0000, 0.0000, 0.0000]],\n","       grad_fn=<MulBackward0>)\n"]}]},{"cell_type":"code","source":["# e2e causal attention + dropout class\n","# we will also introduce batches (>1 input. in the example above \"your journey starts with one step\" is a single input)\n","# each sentence has 6 token, each token as 3 dimensional embedding. 2 such sentence is a batch\n","inputs = torch.tensor([\n","        [0.43, 0.15, 0.89], # your x(1)\n","        [0.55, 0.87, 0.66],  # journey x(2)\n","        [0.57, 0.85, 0.64], # starts x(3)\n","        [0.22, 0.58, 0.33], # with x(4)\n","        [0.77, 0.25, 0.10],  # one x(5)\n","        [0.05, 0.80, 0.55], # step x(6)\n","    ])\n","batch = torch.stack((inputs, inputs), dim=0)\n","print(\"batch\\n\", batch.shape, batch)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C0VbuCFTp0j6","executionInfo":{"status":"ok","timestamp":1742357796337,"user_tz":420,"elapsed":19,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"fe39ef0e-3cc3-4f35-86f9-677f1cdd169a"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["batch\n"," torch.Size([2, 6, 3]) tensor([[[0.4300, 0.1500, 0.8900],\n","         [0.5500, 0.8700, 0.6600],\n","         [0.5700, 0.8500, 0.6400],\n","         [0.2200, 0.5800, 0.3300],\n","         [0.7700, 0.2500, 0.1000],\n","         [0.0500, 0.8000, 0.5500]],\n","\n","        [[0.4300, 0.1500, 0.8900],\n","         [0.5500, 0.8700, 0.6600],\n","         [0.5700, 0.8500, 0.6400],\n","         [0.2200, 0.5800, 0.3300],\n","         [0.7700, 0.2500, 0.1000],\n","         [0.0500, 0.8000, 0.5500]]])\n"]}]},{"cell_type":"code","source":["class CausalAttention(nn.Module):\n","    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False) -> None:\n","      \"\"\"\n","      d_in: embedding dimension\n","      d_out: output dimension of the Q,K,V space\n","      \"\"\"\n","      super().__init__()\n","      self.d_out = d_out\n","      self.dropout = torch.nn.Dropout(dropout) # new diff\n","      # we can use nn.Linear which has an optimized weight init scheme, which results in stable training.\n","      self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n","      self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n","      self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n","      # By registering the mask as a buffer, it becomes a persistent part of the CausalAttention module.\n","      # It's not treated as a learnable parameter, but it's still stored with the module's state.\n","      # This makes it convenient to access and use the mask during the forward pass of the attention mechanism.\n","      # further the buffer will be automatically moved to the appropriate device. we do not need to manually ensure that tensores are on the same device as model params.\n","      self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1)) #\n","\n","    def forward(self, x):\n","      b, num_tokens, d_in = x.shape\n","      keys = self.W_key(x)\n","      queries = self.W_query(x)\n","      values = self.W_value(x)\n","\n","      print(\"keys\\n\",keys)\n","      # In a 3-dimensional tensor, like the one we're assuming for keys, the dimensions are typically:\n","      # Dimension (0,1,2): Batch size, Sequence length, Embedding dimension\n","      # Swapping: transpose(1, 2) swaps the sequence length dimension (1) with the embedding dimension (2).\n","      print(\"keys transpose(1,2)\",keys.transpose(1,2))\n","      attn_scores = queries @ keys.transpose(1,2)\n","      # masked_fill_: This is an in-place operation on the attn_scores tensor.\n","      # It fills elements of the attn_scores tensor with a specified value (-torch.inf in this case) where a corresponding mask is True\n","      attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) # new diff\n","      print(\"attn_scores\\n\",attn_scores)\n","      attn_weights = torch.softmax(attn_scores/keys.shape[-1] ** 0.5, dim=-1)\n","      print(\"attn_weights(before dropout)\\n\",attn_weights)\n","      # apply dropout to the attn weights\n","      attn_weights = self.dropout(attn_weights) # new diff\n","      print(\"attn_weights(after dropout)\\n\",attn_weights)\n","\n","      context_vec = attn_weights @ values\n","      return context_vec"],"metadata":{"id":"fFQjaDAmF6I0","executionInfo":{"status":"ok","timestamp":1742357796390,"user_tz":420,"elapsed":38,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(123)\n","context_length = batch.shape[1] # num tokens for a single input\n","d_in = batch.shape[-1] # emb dimension\n","d_out = 2 # q,k,v space dimension\n","print(\"context_length\", context_length, \"d_in\", d_in, \"d_out\", d_out)\n","ca = CausalAttention(d_in, d_out, context_length, 0.0)\n","context_vec = ca(batch)\n","print(\"context_vec\\n\", context_vec.shape, context_vec)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bWb2WZNoLILZ","executionInfo":{"status":"ok","timestamp":1742357796523,"user_tz":420,"elapsed":123,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"8fe12f7a-e493-48f0-94ed-f92f7f7aa845"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["context_length 6 d_in 3 d_out 2\n","keys\n"," tensor([[[-0.5740,  0.2727],\n","         [-0.8709,  0.1008],\n","         [-0.8628,  0.1060],\n","         [-0.4789,  0.0051],\n","         [-0.4744,  0.1696],\n","         [-0.5888, -0.0388]],\n","\n","        [[-0.5740,  0.2727],\n","         [-0.8709,  0.1008],\n","         [-0.8628,  0.1060],\n","         [-0.4789,  0.0051],\n","         [-0.4744,  0.1696],\n","         [-0.5888, -0.0388]]], grad_fn=<UnsafeViewBackward0>)\n","keys transpose(1,2) tensor([[[-0.5740, -0.8709, -0.8628, -0.4789, -0.4744, -0.5888],\n","         [ 0.2727,  0.1008,  0.1060,  0.0051,  0.1696, -0.0388]],\n","\n","        [[-0.5740, -0.8709, -0.8628, -0.4789, -0.4744, -0.5888],\n","         [ 0.2727,  0.1008,  0.1060,  0.0051,  0.1696, -0.0388]]],\n","       grad_fn=<TransposeBackward0>)\n","attn_scores\n"," tensor([[[0.3111,   -inf,   -inf,   -inf,   -inf,   -inf],\n","         [0.1655, 0.2602,   -inf,   -inf,   -inf,   -inf],\n","         [0.1667, 0.2602, 0.2577,   -inf,   -inf,   -inf],\n","         [0.0510, 0.1080, 0.1064, 0.0643,   -inf,   -inf],\n","         [0.1415, 0.1875, 0.1863, 0.0987, 0.1121,   -inf],\n","         [0.0476, 0.1192, 0.1171, 0.0731, 0.0477, 0.0966]],\n","\n","        [[0.3111,   -inf,   -inf,   -inf,   -inf,   -inf],\n","         [0.1655, 0.2602,   -inf,   -inf,   -inf,   -inf],\n","         [0.1667, 0.2602, 0.2577,   -inf,   -inf,   -inf],\n","         [0.0510, 0.1080, 0.1064, 0.0643,   -inf,   -inf],\n","         [0.1415, 0.1875, 0.1863, 0.0987, 0.1121,   -inf],\n","         [0.0476, 0.1192, 0.1171, 0.0731, 0.0477, 0.0966]]],\n","       grad_fn=<MaskedFillBackward0>)\n","attn_weights(before dropout)\n"," tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n","         [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n","         [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n","         [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n","\n","        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n","         [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n","         [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n","         [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]]],\n","       grad_fn=<SoftmaxBackward0>)\n","attn_weights(after dropout)\n"," tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n","         [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n","         [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n","         [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n","\n","        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n","         [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n","         [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n","         [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]]],\n","       grad_fn=<SoftmaxBackward0>)\n","context_vec\n"," torch.Size([2, 6, 2]) tensor([[[-0.4519,  0.2216],\n","         [-0.5874,  0.0058],\n","         [-0.6300, -0.0632],\n","         [-0.5675, -0.0843],\n","         [-0.5526, -0.0981],\n","         [-0.5299, -0.1081]],\n","\n","        [[-0.4519,  0.2216],\n","         [-0.5874,  0.0058],\n","         [-0.6300, -0.0632],\n","         [-0.5675, -0.0843],\n","         [-0.5526, -0.0981],\n","         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n"]}]},{"cell_type":"code","source":["# Chapter multi-head attention (https://www.youtube.com/watch?v=cPaBCoNdCtE&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=17)\n","# dividing attn mechanism into multiple independent heads. when we get the scores, from one set of Q,K,V this is one attention head.\n","# naive multihead - Creating multiple instances of causalattention class and then combine the output\n","class MultiHeadAttentionWrapper(nn.Module):\n","    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False) -> None:\n","      \"\"\"\n","      note: the result output dimension of the final context vector is d_out * num_heads.\n","      \"\"\"\n","      super().__init__()\n","      self.heads = nn.ModuleList([\n","        CausalAttention(d_in, d_out, context_length, dropout, qkv_bias=qkv_bias) for _ in range(num_heads)\n","      ])\n","\n","    def forward(self, x):\n","      return torch.cat([head(x) for head in self.heads], dim=-1)"],"metadata":{"id":"Ldw34DUgLdUV","executionInfo":{"status":"ok","timestamp":1742357796524,"user_tz":420,"elapsed":0,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["inputs = torch.tensor([\n","        [0.43, 0.15, 0.89], # your x(1)\n","        [0.55, 0.87, 0.66],  # journey x(2)\n","        [0.57, 0.85, 0.64], # starts x(3)\n","        [0.22, 0.58, 0.33], # with x(4)\n","        [0.77, 0.25, 0.10],  # one x(5)\n","        [0.05, 0.80, 0.55], # step x(6)\n","    ])\n","batch = torch.stack((inputs, inputs), dim=0)\n","print(\"batch\\n\", batch.shape, batch)\n","\n","torch.manual_seed(123)\n","context_length = batch.shape[1] # num tokens for a single input\n","d_in = batch.shape[-1] # emb dimension\n","d_out = 2 # q,k,v space dimension\n","mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, dropout=0.0, num_heads=2)\n","context_vec = mha(batch)\n","print(\"context_vec\\n\", context_vec.shape, context_vec)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tTwQ6GaG5JnX","executionInfo":{"status":"ok","timestamp":1742357796645,"user_tz":420,"elapsed":111,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"3827ca9f-757f-4bef-b4a8-40f20ff1e0c2"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["batch\n"," torch.Size([2, 6, 3]) tensor([[[0.4300, 0.1500, 0.8900],\n","         [0.5500, 0.8700, 0.6600],\n","         [0.5700, 0.8500, 0.6400],\n","         [0.2200, 0.5800, 0.3300],\n","         [0.7700, 0.2500, 0.1000],\n","         [0.0500, 0.8000, 0.5500]],\n","\n","        [[0.4300, 0.1500, 0.8900],\n","         [0.5500, 0.8700, 0.6600],\n","         [0.5700, 0.8500, 0.6400],\n","         [0.2200, 0.5800, 0.3300],\n","         [0.7700, 0.2500, 0.1000],\n","         [0.0500, 0.8000, 0.5500]]])\n","keys\n"," tensor([[[-0.5740,  0.2727],\n","         [-0.8709,  0.1008],\n","         [-0.8628,  0.1060],\n","         [-0.4789,  0.0051],\n","         [-0.4744,  0.1696],\n","         [-0.5888, -0.0388]],\n","\n","        [[-0.5740,  0.2727],\n","         [-0.8709,  0.1008],\n","         [-0.8628,  0.1060],\n","         [-0.4789,  0.0051],\n","         [-0.4744,  0.1696],\n","         [-0.5888, -0.0388]]], grad_fn=<UnsafeViewBackward0>)\n","keys transpose(1,2) tensor([[[-0.5740, -0.8709, -0.8628, -0.4789, -0.4744, -0.5888],\n","         [ 0.2727,  0.1008,  0.1060,  0.0051,  0.1696, -0.0388]],\n","\n","        [[-0.5740, -0.8709, -0.8628, -0.4789, -0.4744, -0.5888],\n","         [ 0.2727,  0.1008,  0.1060,  0.0051,  0.1696, -0.0388]]],\n","       grad_fn=<TransposeBackward0>)\n","attn_scores\n"," tensor([[[0.3111,   -inf,   -inf,   -inf,   -inf,   -inf],\n","         [0.1655, 0.2602,   -inf,   -inf,   -inf,   -inf],\n","         [0.1667, 0.2602, 0.2577,   -inf,   -inf,   -inf],\n","         [0.0510, 0.1080, 0.1064, 0.0643,   -inf,   -inf],\n","         [0.1415, 0.1875, 0.1863, 0.0987, 0.1121,   -inf],\n","         [0.0476, 0.1192, 0.1171, 0.0731, 0.0477, 0.0966]],\n","\n","        [[0.3111,   -inf,   -inf,   -inf,   -inf,   -inf],\n","         [0.1655, 0.2602,   -inf,   -inf,   -inf,   -inf],\n","         [0.1667, 0.2602, 0.2577,   -inf,   -inf,   -inf],\n","         [0.0510, 0.1080, 0.1064, 0.0643,   -inf,   -inf],\n","         [0.1415, 0.1875, 0.1863, 0.0987, 0.1121,   -inf],\n","         [0.0476, 0.1192, 0.1171, 0.0731, 0.0477, 0.0966]]],\n","       grad_fn=<MaskedFillBackward0>)\n","attn_weights(before dropout)\n"," tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n","         [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n","         [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n","         [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n","\n","        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n","         [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n","         [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n","         [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]]],\n","       grad_fn=<SoftmaxBackward0>)\n","attn_weights(after dropout)\n"," tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n","         [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n","         [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n","         [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n","\n","        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n","         [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n","         [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n","         [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]]],\n","       grad_fn=<SoftmaxBackward0>)\n","keys\n"," tensor([[[-0.3132, -0.2272],\n","         [-0.1536,  0.2768],\n","         [-0.1574,  0.2865],\n","         [-0.0360,  0.1826],\n","         [-0.1805,  0.3798],\n","         [-0.0080,  0.0967]],\n","\n","        [[-0.3132, -0.2272],\n","         [-0.1536,  0.2768],\n","         [-0.1574,  0.2865],\n","         [-0.0360,  0.1826],\n","         [-0.1805,  0.3798],\n","         [-0.0080,  0.0967]]], grad_fn=<UnsafeViewBackward0>)\n","keys transpose(1,2) tensor([[[-0.3132, -0.1536, -0.1574, -0.0360, -0.1805, -0.0080],\n","         [-0.2272,  0.2768,  0.2865,  0.1826,  0.3798,  0.0967]],\n","\n","        [[-0.3132, -0.1536, -0.1574, -0.0360, -0.1805, -0.0080],\n","         [-0.2272,  0.2768,  0.2865,  0.1826,  0.3798,  0.0967]]],\n","       grad_fn=<TransposeBackward0>)\n","attn_scores\n"," tensor([[[-0.2327,    -inf,    -inf,    -inf,    -inf,    -inf],\n","         [-0.2396,  0.1015,    -inf,    -inf,    -inf,    -inf],\n","         [-0.2323,  0.1004,  0.1045,    -inf,    -inf,    -inf],\n","         [-0.1344,  0.0502,  0.0523,  0.0470,    -inf,    -inf],\n","         [-0.0349,  0.0520,  0.0538,  0.0331,  0.0708,    -inf],\n","         [-0.2142,  0.0650,  0.0679,  0.0668,  0.1004,  0.0395]],\n","\n","        [[-0.2327,    -inf,    -inf,    -inf,    -inf,    -inf],\n","         [-0.2396,  0.1015,    -inf,    -inf,    -inf,    -inf],\n","         [-0.2323,  0.1004,  0.1045,    -inf,    -inf,    -inf],\n","         [-0.1344,  0.0502,  0.0523,  0.0470,    -inf,    -inf],\n","         [-0.0349,  0.0520,  0.0538,  0.0331,  0.0708,    -inf],\n","         [-0.2142,  0.0650,  0.0679,  0.0668,  0.1004,  0.0395]]],\n","       grad_fn=<MaskedFillBackward0>)\n","attn_weights(before dropout)\n"," tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.4400, 0.5600, 0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.2830, 0.3580, 0.3590, 0.0000, 0.0000, 0.0000],\n","         [0.2264, 0.2579, 0.2583, 0.2574, 0.0000, 0.0000],\n","         [0.1903, 0.2024, 0.2026, 0.1997, 0.2051, 0.0000],\n","         [0.1408, 0.1715, 0.1718, 0.1717, 0.1758, 0.1684]],\n","\n","        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.4400, 0.5600, 0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.2830, 0.3580, 0.3590, 0.0000, 0.0000, 0.0000],\n","         [0.2264, 0.2579, 0.2583, 0.2574, 0.0000, 0.0000],\n","         [0.1903, 0.2024, 0.2026, 0.1997, 0.2051, 0.0000],\n","         [0.1408, 0.1715, 0.1718, 0.1717, 0.1758, 0.1684]]],\n","       grad_fn=<SoftmaxBackward0>)\n","attn_weights(after dropout)\n"," tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.4400, 0.5600, 0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.2830, 0.3580, 0.3590, 0.0000, 0.0000, 0.0000],\n","         [0.2264, 0.2579, 0.2583, 0.2574, 0.0000, 0.0000],\n","         [0.1903, 0.2024, 0.2026, 0.1997, 0.2051, 0.0000],\n","         [0.1408, 0.1715, 0.1718, 0.1717, 0.1758, 0.1684]],\n","\n","        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.4400, 0.5600, 0.0000, 0.0000, 0.0000, 0.0000],\n","         [0.2830, 0.3580, 0.3590, 0.0000, 0.0000, 0.0000],\n","         [0.2264, 0.2579, 0.2583, 0.2574, 0.0000, 0.0000],\n","         [0.1903, 0.2024, 0.2026, 0.1997, 0.2051, 0.0000],\n","         [0.1408, 0.1715, 0.1718, 0.1717, 0.1758, 0.1684]]],\n","       grad_fn=<SoftmaxBackward0>)\n","context_vec\n"," torch.Size([2, 6, 4]) tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n","         [-0.5874,  0.0058,  0.5891,  0.3257],\n","         [-0.6300, -0.0632,  0.6202,  0.3860],\n","         [-0.5675, -0.0843,  0.5478,  0.3589],\n","         [-0.5526, -0.0981,  0.5321,  0.3428],\n","         [-0.5299, -0.1081,  0.5077,  0.3493]],\n","\n","        [[-0.4519,  0.2216,  0.4772,  0.1063],\n","         [-0.5874,  0.0058,  0.5891,  0.3257],\n","         [-0.6300, -0.0632,  0.6202,  0.3860],\n","         [-0.5675, -0.0843,  0.5478,  0.3589],\n","         [-0.5526, -0.0981,  0.5321,  0.3428],\n","         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n"]}]},{"cell_type":"code","source":["# Chapter: Parallel Multihead Attention (with weight splits) https://www.youtube.com/watch?v=K5u9eEaoxFg&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=18\n","# Instead of maintaining multiple causal attention instances for each head, have a single MHA class\n","# in the simple case, for each causal attention, we multiply the input x with Query matrix Wq (one weight matrix per head).\n","# Instead, let's use a Wq matrix of larger dimension (which combines all the heads) and then split the Query matrix into num_heads matrices. this reduces the number of matrix multiplication\n","# d_out of the larger weight matrix = head dim * num_heads\n","\n","# step 1: start with input b , num_tokens, d_in (e.g 1,3,6)\n","# step 2: dedice d_out, num_heads (say d_out = 6, same as d_in, n_heads = 2. e.g in GPT2 n_heads=96, head_dim - d_out/n_heads. so here head_dim = 3)\n","# step 3: initialize trainable Wq,k,v weight metrices. d_in is 6, d_out = 6.\n","# step 4: Calculate K,Q,V matrices (x @ W), dim input is (1,3,6), dim W (6,6) -> result matrix is (1,3,6). 1 is batch, 3 is num tokens, 6 is d_out.\n","# step 5: we now need a 4th dimension for the head. unroll the last dimension of K,Q,V to include the number of head and head_dim. remember d_out = num_head * head_dim. view (1,3,6) as (1,3,2,3).\n","# step 6: K,Q,V are grouped by number of token. we group by num of heads (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n","# step 7: perf scaled, causal attention with dropout.\n","# step 8: get back to (b, num_tokens, d_out) for the context vector.\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False) -> None:\n","      super().__init__()\n","      assert(d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n","      self.d_out = d_out\n","      self.num_heads = num_heads\n","      self.head_dim = d_out // num_heads\n","\n","      self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n","      self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n","      self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n","      self.out_proj = nn.Linear(d_out, d_out) # linear layer to combine head outputs\n","      self.dropout = nn.Dropout(dropout)\n","      self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n","\n","    def forward(self, x):\n","      b, num_tokens, d_in = x.shape\n","      keys = self.W_key(x)\n","      queries = self.W_query(x)\n","      values = self.W_value(x)\n","      # we unroll the last dim d_out for the head.\n","      keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n","      queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n","      values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n","\n","\n","      # transpose (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim). this is because we will compute attention scores for each head in parallel, so we group by heads.\n","      keys = keys.transpose(1,2)\n","      queries = queries.transpose(1,2)\n","      values = values.transpose(1,2)\n","\n","      # compute scaled dot-product attention with causal mask. we transpose the num_tokens and head_dim. (think of head_dim as d_out in the simple case we have seen earlier. we need to multiply this with query (b, num_heads, num_tokens, head_dim))\n","      attn_scores = queries @ keys.transpose(2,3) # dot product for each head in parallel. this dimension is (num_token, num_token).\n","      bool_mask = self.mask.bool()[:num_tokens, :num_tokens] # original mask is truncated to num_tokens. if num_tokens < context_length\n","      attn_scores.masked_fill_(bool_mask, -torch.inf)\n","      attn_weights = torch.softmax(attn_scores/keys.shape[-1] ** 0.5, dim=-1) # keys.shape[-1] is head_dim\n","      attn_weights = self.dropout(attn_weights)\n","      context_vec = attn_weights @ values # attn_weight have dim (b, num_heads, num_tokens, num_tokens). values has dim (b, num_heads, num_tokens, head_dim). remember we performed a view operation of the values above to group by num_heads. the output matrix would be (b, num_heads, num_tokens, head_dim)\n","      context_vec = context_vec.transpose(1,2) # (b, num_heads, num_tokens, head_dim) -> (b, num_tokens, num_heads, head_dim)\n","      # The transpose() operation might not always result in a contiguous tensor (a tensor where elements are stored sequentially in memory). contiguous() creates a copy of the tensor to ensure it is contiguous, enabling efficient reshaping operations.\n","      context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out) # (b, num_tokens, num_heads, head_dim) -> (b, num_tokens, d_out)\n","      context_vec = self.out_proj(context_vec) # optional. This line applies a linear transformation (projection) to the combined output of the multi-head attention mechanism. This projection helps to map the concatenated context vectors from different heads back into the original output dimension\n","      return context_vec"],"metadata":{"id":"VbI7keNJ6j-Y","executionInfo":{"status":"ok","timestamp":1742357796691,"user_tz":420,"elapsed":40,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(123)\n","inputs = torch.tensor(\n","    [\n","        [0.43,0.15,0.89,0.55,0.87,0.66],\n","        [0.57,0.85,0.64,0.22,0.58,0.33],\n","        [0.77,0.25,0.10,0.05,0.80,0.55],\n","    ]\n",")\n","\n","batch = torch.stack((inputs, inputs), dim=0)\n","print(\"batch\\n\", batch.shape, batch)\n","batch_size, context_length, d_in = batch.shape\n","d_out = 6\n","mha = MultiHeadAttention(d_in, d_out, context_length, dropout=0.0, num_heads=2)\n","context_vec = mha(batch)\n","print(\"context_vec\\n\", context_vec.shape, context_vec)\n","\n","# smallest GPT-2 has 12 attention heads, and d_in 768. generally d_in = d_out in GPT models."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zDsNklC15ZGv","executionInfo":{"status":"ok","timestamp":1742357796702,"user_tz":420,"elapsed":11,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"d908ca02-bfda-49fd-c37e-63f023620ba2"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["batch\n"," torch.Size([2, 3, 6]) tensor([[[0.4300, 0.1500, 0.8900, 0.5500, 0.8700, 0.6600],\n","         [0.5700, 0.8500, 0.6400, 0.2200, 0.5800, 0.3300],\n","         [0.7700, 0.2500, 0.1000, 0.0500, 0.8000, 0.5500]],\n","\n","        [[0.4300, 0.1500, 0.8900, 0.5500, 0.8700, 0.6600],\n","         [0.5700, 0.8500, 0.6400, 0.2200, 0.5800, 0.3300],\n","         [0.7700, 0.2500, 0.1000, 0.0500, 0.8000, 0.5500]]])\n","context_vec\n"," torch.Size([2, 3, 6]) tensor([[[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n","         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n","         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]],\n","\n","        [[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n","         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n","         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]]],\n","       grad_fn=<ViewBackward0>)\n"]}]},{"cell_type":"code","source":["# Chapter LLM architecture (https://www.youtube.com/watch?v=4i23dYoXp-A&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=19)\n","# transformer block\n","# 1. layer norm\n","# 2. masked Multihead nn.attention\n","# 3. dropout\n","# 4. shortcut conn\n","# 5. dropout\n","# 6. FF\n","# 7. dropout\n","\n","import torch\n","import torch.nn as nn\n","\n","# GPT-2 Config (smallest model. note there are other larger variants of gpt-2)\n","GPT_CONFIG_124M = {\n","  \"vocab_size\": 50257,\n","  \"context_length\": 1024,\n","  \"emb_dim\":768, # d_in\n","  \"n_heads\":12, # multi head attention head count per transformer block\n","  \"n_layers\": 12, # num transformer blocks\n","  \"drop_rate\": 0.1, # dropout\n","  \"qkv_bias\": False,\n","}\n","\n","# GPT placeholder architeture\n","class DummyGPTModel(nn.Module):\n","  def __init__(self, cfg):\n","    super().__init__()\n","    self.token_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n","    self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n","    self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n","    self.trf_blocks = nn.Sequential(*[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n","    self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n","    self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"]) # final projection back from emb dim to vocab dim\n","\n","  def forward(self, in_idx):\n","    batch_size, seq_len = in_idx.shape # in_idx is (batch, token id) . token id is obtained from tokenizer like tiktoken for gpt2\n","    token_embeds = self.token_emb(in_idx) # dimension (batch, token seq, emb)\n","    # print(token_embeds.shape)\n","    pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device)) # dim (token seq, emb)\n","    # print(pos_embeds.shape)\n","    x = token_embeds + pos_embeds\n","    x = self.drop_emb(x)\n","    x = self.trf_blocks(x)\n","    x = self.final_norm(x)\n","    logits = self.out_head(x) # (batch, token seq, vocab size)\n","    return logits\n","\n","class DummyTransformerBlock(nn.Module):\n","  def __init__(self, cfg):\n","    super().__init__()\n","\n","  def forward(self, x):\n","    return x\n","\n","class DummyLayerNorm(nn.Module):\n","  def __init__(self, cfg):\n","    super().__init__()\n","\n","  def forward(self, x):\n","    return x"],"metadata":{"id":"cyXE0HZq81pJ","executionInfo":{"status":"ok","timestamp":1742357796758,"user_tz":420,"elapsed":45,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["import tiktoken\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","batch = []\n","txt1 = \"Every effort moves you\"\n","txt2 = \"Every day holds a\"\n","batch.append(torch.tensor(tokenizer.encode(txt1)))\n","batch.append(torch.tensor(tokenizer.encode(txt2)))\n","print(\"batch\\n\", batch)\n","batch = torch.stack(batch, dim=0)\n","print(\"batch\\n\", batch.shape, batch)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QQv4GWX8tmdK","executionInfo":{"status":"ok","timestamp":1742357796819,"user_tz":420,"elapsed":61,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"415b9073-eb36-4886-82de-e99a0b5d1d04"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["batch\n"," [tensor([6109, 3626, 6100,  345]), tensor([6109, 1110, 6622,  257])]\n","batch\n"," torch.Size([2, 4]) tensor([[6109, 3626, 6100,  345],\n","        [6109, 1110, 6622,  257]])\n"]}]},{"cell_type":"code","source":["torch.manual_seed(123)\n","m = DummyGPTModel(GPT_CONFIG_124M)\n","logits = m(batch) # the output has 2 rows for the two text samples of the batch. each text consists of 4 tokens, for each token, there is a vocab dim vector of logits. encodes the prob of what token comes next\n","print(logits.shape,logits)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qm7i6A_gt6-L","executionInfo":{"status":"ok","timestamp":1742357800949,"user_tz":420,"elapsed":4131,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"60144e43-e143-4f4a-f086-4eefefff70a8"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 4, 50257]) tensor([[[-1.4263,  0.1865, -1.0108,  ..., -1.8458, -0.1784, -0.3773],\n","         [-0.1230,  0.3998, -0.0121,  ..., -0.4483,  1.5452,  1.2868],\n","         [ 0.6295,  1.9302, -0.3573,  ...,  1.6177,  0.2114,  0.6670],\n","         [ 0.8993,  1.6951, -0.8083,  ...,  1.0100, -0.7267, -0.7965]],\n","\n","        [[-1.4560,  0.5308, -0.7231,  ..., -1.3773, -0.2287, -0.4222],\n","         [-0.6794,  1.0382,  0.0130,  ..., -0.2712,  0.7241,  1.5719],\n","         [ 0.6346,  1.0774, -0.1682,  ..., -0.5877, -0.0291,  0.4476],\n","         [ 0.0516, -0.8008,  0.6667,  ...,  2.0342,  0.0381, -0.0332]]],\n","       grad_fn=<ViewBackward0>)\n"]}]},{"cell_type":"code","source":["# layer norm  - https://www.youtube.com/watch?v=G3W-LT79LSI&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=21\n","# problem 1 - address vanishing / exploding gradients problem which can lead to unstable traninig.\n","# if gradient becomes too large, by the time we reach to the last layers gradient can explode.\n","# if gradent is small, it may vanish. learning will stagnate.\n","# if layer outputs are themselves large and small, they affect the gradient in backprop\n","# layer norm keeps gradient stable!\n","\n","# problem 2 - internal covariate shift - during traning process as we pass through the input iteration, the distributio can vary. (). this can delay convergence. layer norm prevents this.\n","\n","# layer norm keeps zero mean/ unit variance  for each layer output. (x - u)/sqrt(v)\n","\n","# where is layer norm used?\n","# 1. before going to MHA (within a tx block)\n","# 2. before feed forward (within a tx block)\n","# 3. bfore the final output layer (after the final o/p tx block)\n","\n","# sample code\n","torch.manual_seed(123)\n","batch_example = torch.randn(2,5) # each input in the batch is 5 dim vector.\n","layer = nn.Sequential(nn.Linear(5,6), nn.ReLU()) # NN of alinear layer  (takes in 5 dim vector as input, and out a 6 dim vector), followed by non linear ReLU\n","out = layer(batch_example)\n","print(out)\n","\n","# each row of out, is corresponding to each input a 6 dimensional vector. we need to find the mean variance for each row (dim=1 or -1 for this example).\n","# keepdim is required to out a 2X1 matrix, where each row is the mean for the given input row. (else it would be just a 2 dim vector)\n","mean = out.mean(dim=1, keepdim=True)\n","var = out.var(dim=1, keepdim=True)\n","print(\"mean\", mean)\n","print(\"var\", var)\n","out_norm = (out - mean) / torch.sqrt(var)\n","print(\"normalized output\", out_norm)\n","\n","# verify zero mean unit variance of normalized output\n","mean = out_norm.mean(dim=1, keepdim=True)\n","var = out_norm.var(dim=1, keepdim=True)\n","torch.set_printoptions(sci_mode=False)\n","print(\"normalized mean\", mean)\n","print(\"normalized var\", var)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CImSqpDa4DnI","executionInfo":{"status":"ok","timestamp":1742357800987,"user_tz":420,"elapsed":39,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"337dc1c6-ee8b-4d06-f482-e3e0eb38cdf5"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n","        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n","       grad_fn=<ReluBackward0>)\n","mean tensor([[0.1324],\n","        [0.2170]], grad_fn=<MeanBackward1>)\n","var tensor([[0.0231],\n","        [0.0398]], grad_fn=<VarBackward0>)\n","normalized output tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n","        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n","       grad_fn=<DivBackward0>)\n","normalized mean tensor([[    0.0000],\n","        [    0.0000]], grad_fn=<MeanBackward1>)\n","normalized var tensor([[1.0000],\n","        [1.0000]], grad_fn=<VarBackward0>)\n"]}]},{"cell_type":"code","source":["class LayerNorm(nn.Module):\n","    def __init__(self, emb_dim, eps=1e-5):\n","        super().__init__()\n","        self.eps = eps\n","        # scale and shift, trainable knobs that best suite the data on which it is training.\n","        self.scale = nn.Parameter(torch.ones(emb_dim))\n","        self.shift = nn.Parameter(torch.zeros(emb_dim))\n","\n","    def forward(self, x):\n","        # for x focus on column, which is the emb dimension. the emb dimension is what is fed into the FF network as input.\n","        mean = x.mean(dim=-1, keepdim=True)\n","        var = x.var(dim=-1, keepdim=True, unbiased=False) # unbiased=False for population variance\n","        x_norm = (x - mean) / torch.sqrt(var + self.eps) # eps prevents divide by zero\n","        return self.scale * x_norm + self.shift\n"],"metadata":{"id":"0uUAafHn_3Ol","executionInfo":{"status":"ok","timestamp":1742357801014,"user_tz":420,"elapsed":24,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["ln = LayerNorm(emb_dim=5)\n","out_ln = ln(batch_example)\n","print(out_ln)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LtDRoXg4HAN0","executionInfo":{"status":"ok","timestamp":1742357801050,"user_tz":420,"elapsed":35,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"802f61c9-387d-4a2f-fdcc-e6f8b07a5606"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0.5528,  1.0693, -0.0223,  0.2656, -1.8654],\n","        [ 0.9087, -1.3767, -0.9564,  1.1304,  0.2940]], grad_fn=<AddBackward0>)\n"]}]},{"cell_type":"code","source":["# Chapter GELU (https://www.youtube.com/watch?v=d_PiwZe8UF4&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=21)\n","# FF n/w\n","# 1. linear layer\n","# 2. GELU\n","# 3. Linear layer\n","\n","# 2 common activation GELU and SwiGLU\n","# let's first look at RElU. f(x)  = x, x > 0 (not differentiable at x=0). often leads to a dead neuron problem. once dead, the neuron cannot be revived.\n","\n","# gelu(x) = x * 0(x), gelu will not be zero for negative values. better to use neumerical appoximation during compute.\n","class GELU(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","    def forward(self, x):\n","        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2/torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))"],"metadata":{"id":"HLc4mCgVPL3j","executionInfo":{"status":"ok","timestamp":1742357801082,"user_tz":420,"elapsed":32,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","gelu, relu = GELU(), nn.ReLU()\n","\n","# sample data\n","x = torch.linspace(-3, 3, 100)\n","y_gelu, y_relu = gelu(x), relu(x)\n","plt.figure(figsize=(8,3))\n","for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]),1):\n","    plt.subplot(1,2,i)\n","    plt.plot(x, y)\n","    plt.title(label)\n","    plt.xlabel(\"x\")\n","    plt.ylabel(\"f(x)\")\n","\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":333},"id":"0miMNiSX9vy8","executionInfo":{"status":"ok","timestamp":1742357802316,"user_tz":420,"elapsed":1234,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"64fcc14e-9414-420d-da54-90db29a800ab"},"execution_count":57,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 800x300 with 2 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["# Chapter Feedforward network https://www.youtube.com/watch?v=d_PiwZe8UF4&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=21\n","# let's focus on the feed forward nw (expansion contraction nn, allow for a richer representation space)\n","# 1. it receives a token of size emb dimension (e.g x = 768)\n","# 2. projected into 4x larger space in the first hidden layer (4x 768)\n","# 3. compress back to original size (x=768)\n","\n","# 1. input tensor shape (batch,seq,emb)\n","# 2. first layer takes input and projects to (batch,seq,4*emb)\n","# 3. gelu works on (batch,seq,4*emb) input and outputs same Dimension\n","# 4. second layer projects back to (batch,seq,emb)\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.layers = nn.Sequential(\n","            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n","            GELU(),\n","            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n","        )\n","    def forward(self, x):\n","        return self.layers(x)"],"metadata":{"id":"GoCYQlVL_-Dn","executionInfo":{"status":"ok","timestamp":1742357802316,"user_tz":420,"elapsed":1,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["ff = FeedForward(GPT_CONFIG_124M)\n","x = torch.rand(2,3,768)\n","op = ff(x)\n","print(op.shape, op)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0yW3waC6D7hi","executionInfo":{"status":"ok","timestamp":1742357802540,"user_tz":420,"elapsed":225,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"d863f369-d0fe-449b-dbac-47dfef275035"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 3, 768]) tensor([[[ 0.1238,  0.0457,  0.0939,  ...,  0.1107,  0.0167, -0.1992],\n","         [ 0.1574, -0.0282,  0.0049,  ...,  0.0026,  0.1120, -0.1075],\n","         [ 0.1184, -0.0052,  0.0839,  ...,  0.1662,  0.0112, -0.1685]],\n","\n","        [[ 0.1302,  0.0630,  0.1050,  ...,  0.1439,  0.0562, -0.1128],\n","         [ 0.1249, -0.0073,  0.1022,  ...,  0.0417,  0.0381, -0.0828],\n","         [ 0.0494,  0.0654,  0.0347,  ...,  0.0701,  0.0793, -0.1810]]],\n","       grad_fn=<ViewBackward0>)\n"]}]},{"cell_type":"code","source":["# Chapter: Shortcut connections (https://www.youtube.com/watch?v=2r0QahNdwMw&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=22)\n","# also known as skip or residual Connection\n","# they were intially proposed in computer vision for addressing vanishing gradients\n","# shortcut provide an alternate path for gradient to flow by skipping one or more layer. achieved by adding output of layer to another layer\n","\n","# mathematical intuition: say layer 1 o/p is YL, output of layer 2 is f(YL)\n","# YL+1 = f(YL) + YL\n","# dL/dYL = dL/dYL+1 (dYL+1/dYL)\n","# and (dYL+1/dYL) = d(fYL)/dYL + 1\n","# dL/dYL = dL/dYL+1 (d(fYL)/dYL + 1) ---> this 1 keeps the gradient flowing through the network, even if d(fYL)/dYL ~ 0\n","class ExampleDeepNeuralNetwork(nn.Module):\n","    def __init__(self, layer_sizes, use_shortcut):\n","        super().__init__()\n","        self.use_shortcut = use_shortcut\n","        self.layers = nn.ModuleList([\n","            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n","            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n","            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n","            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n","            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n","            ])\n","\n","    def forward(self, x):\n","      for layer in self.layers:\n","          layer_output = layer(x)\n","          if self.use_shortcut and x.shape == layer_output.shape:\n","              x = x + layer_output\n","          else:\n","              x = layer_output\n","      return x\n","\n","def print_gradients(model, x):\n","  output = model(x)\n","  target = torch.tensor([[0.0]])\n","  loss = nn.MSELoss()(output, target)\n","\n","  loss.backward()\n","\n","  for name, param in model.named_parameters():\n","      if param.grad is not None and 'weight' in name:\n","          print(f\"Parameter: {name}, Gradient mean: {param.grad.abs().mean().item()}\")"],"metadata":{"id":"QrBxkMW-D_Le","executionInfo":{"status":"ok","timestamp":1742357802586,"user_tz":420,"elapsed":22,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["layer_sizes = [3,3,3,3,3,1]\n","sample_input = torch.tensor([[1.0,0.0,-1.0]])\n","torch.manual_seed(123)\n","model = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=True)\n","output = model(sample_input)\n","print(output)\n","\n","model_without_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=False)\n","output = model_without_shortcut(sample_input)\n","print(output)\n","\n","print(\"model_without_shortcut\")\n","print_gradients(model_without_shortcut, sample_input)\n","print(\"model_with shortcut\")\n","print_gradients(model, sample_input)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DcCmtFRImemt","executionInfo":{"status":"ok","timestamp":1742357802702,"user_tz":420,"elapsed":107,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"d323bff9-1c1c-4b8a-9720-b5060a30302f"},"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.7669]], grad_fn=<MulBackward0>)\n","tensor([[-0.1420]], grad_fn=<MulBackward0>)\n","model_without_shortcut\n","Parameter: layers.0.0.weight, Gradient mean: 0.0003279339289292693\n","Parameter: layers.1.0.weight, Gradient mean: 0.00030065476312302053\n","Parameter: layers.2.0.weight, Gradient mean: 0.000655296491459012\n","Parameter: layers.3.0.weight, Gradient mean: 0.002097164746373892\n","Parameter: layers.4.0.weight, Gradient mean: 0.008189782500267029\n","model_with shortcut\n","Parameter: layers.0.0.weight, Gradient mean: 0.22169791162014008\n","Parameter: layers.1.0.weight, Gradient mean: 0.20694105327129364\n","Parameter: layers.2.0.weight, Gradient mean: 0.32896995544433594\n","Parameter: layers.3.0.weight, Gradient mean: 0.2665732204914093\n","Parameter: layers.4.0.weight, Gradient mean: 1.3258540630340576\n"]}]},{"cell_type":"code","source":["# Chapter Transformer block https://www.youtube.com/watch?v=dvH6lFGhFrs&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=23\n","# we will connect layer norm, dropout, gelu activation, shortcut connections, feed forward network to construct the transformer block\n","GPT_CONFIG_124M = {\n","  \"vocab_size\": 50257,\n","  \"context_length\": 1024,\n","  \"emb_dim\":768, # d_in\n","  \"n_heads\":12, # multi head attention head count per transformer block\n","  \"n_layers\": 12, # num transformer blocks\n","  \"drop_rate\": 0.1, # dropout\n","  \"qkv_bias\": False,\n","}\n","\n","# Consolidating all the classes we have developed in the earlier cells in this one cell for readability\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False) -> None:\n","      super().__init__()\n","      assert(d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n","      self.d_out = d_out\n","      self.num_heads = num_heads\n","      self.head_dim = d_out // num_heads\n","\n","      self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n","      self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n","      self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n","      self.out_proj = nn.Linear(d_out, d_out) # linear layer to combine head outputs\n","      self.dropout = nn.Dropout(dropout)\n","      self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n","\n","    def forward(self, x):\n","      b, num_tokens, d_in = x.shape\n","      keys = self.W_key(x)\n","      queries = self.W_query(x)\n","      values = self.W_value(x)\n","      # we unroll the last dim d_out for the head.\n","      keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n","      queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n","      values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n","\n","\n","      # transpose (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim). this is because we will compute attention scores for each head in parallel, so we group by heads.\n","      keys = keys.transpose(1,2)\n","      queries = queries.transpose(1,2)\n","      values = values.transpose(1,2)\n","\n","      # compute scaled dot-product attention with causal mask. we transpose the num_tokens and head_dim. (think of head_dim as d_out in the simple case we have seen earlier. we need to multiply this with query (b, num_heads, num_tokens, head_dim))\n","      attn_scores = queries @ keys.transpose(2,3) # dot product for each head in parallel. this dimension is (num_token, num_token).\n","      bool_mask = self.mask.bool()[:num_tokens, :num_tokens] # original mask is truncated to num_tokens. if num_tokens < context_length\n","      attn_scores.masked_fill_(bool_mask, -torch.inf)\n","      attn_weights = torch.softmax(attn_scores/keys.shape[-1] ** 0.5, dim=-1) # keys.shape[-1] is head_dim\n","      attn_weights = self.dropout(attn_weights)\n","      context_vec = attn_weights @ values # attn_weight have dim (b, num_heads, num_tokens, num_tokens). values has dim (b, num_heads, num_tokens, head_dim). remember we performed a view operation of the values above to group by num_heads. the output matrix would be (b, num_heads, num_tokens, head_dim)\n","      context_vec = context_vec.transpose(1,2) # (b, num_heads, num_tokens, head_dim) -> (b, num_tokens, num_heads, head_dim)\n","      # The transpose() operation might not always result in a contiguous tensor (a tensor where elements are stored sequentially in memory). contiguous() creates a copy of the tensor to ensure it is contiguous, enabling efficient reshaping operations.\n","      context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out) # (b, num_tokens, num_heads, head_dim) -> (b, num_tokens, d_out)\n","      context_vec = self.out_proj(context_vec) # optional. This line applies a linear transformation (projection) to the combined output of the multi-head attention mechanism. This projection helps to map the concatenated context vectors from different heads back into the original output dimension\n","      return context_vec\n","\n","class LayerNorm(nn.Module):\n","    def __init__(self, emb_dim, eps=1e-5):\n","        super().__init__()\n","        self.eps = eps\n","        # scale and shift, trainable knobs that best suite the data on which it is training.\n","        self.scale = nn.Parameter(torch.ones(emb_dim))\n","        self.shift = nn.Parameter(torch.zeros(emb_dim))\n","\n","    def forward(self, x):\n","        # for x focus on column, which is the emb dimension. the emb dimension is what is fed into the FF network as input.\n","        mean = x.mean(dim=-1, keepdim=True)\n","        var = x.var(dim=-1, keepdim=True, unbiased=False) # unbiased=False for population variance\n","        x_norm = (x - mean) / torch.sqrt(var + self.eps) # eps prevents divide by zero\n","        return self.scale * x_norm + self.shift\n","\n","class GELU(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","    def forward(self, x):\n","        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2/torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.layers = nn.Sequential(\n","            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n","            GELU(),\n","            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n","        )\n","    def forward(self, x):\n","        return self.layers(x)\n","\n","class Transformer(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.attn = MultiHeadAttention(\n","            d_in=cfg[\"emb_dim\"],\n","            d_out=cfg[\"emb_dim\"],\n","            context_length=cfg[\"context_length\"],\n","            dropout=cfg[\"drop_rate\"],\n","            num_heads=cfg[\"n_heads\"],\n","            qkv_bias=cfg[\"qkv_bias\"])\n","        self.ff = FeedForward(cfg)\n","        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n","        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n","        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n","\n","    def forward(self, x):\n","      shortcut = x\n","      x = self.norm1(x) # pre-layer norm.\n","      x = self.attn(x)\n","      x = self.drop_shortcut(x)\n","      x = x + shortcut # add original input back\n","\n","      shortcut = x\n","      x = self.norm2(x) # pre-layer norm\n","      x = self.ff(x)\n","      x = self.drop_shortcut(x)\n","      x = x + shortcut\n","      return x"],"metadata":{"id":"MBKUOPMkm8H3","executionInfo":{"status":"ok","timestamp":1742357802765,"user_tz":420,"elapsed":51,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(123)\n","x = torch.rand(2,4,768)\n","block = Transformer(GPT_CONFIG_124M)\n","op = block(x)\n","print(op.shape, op)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nx7-P9xRRqQY","executionInfo":{"status":"ok","timestamp":1742357803473,"user_tz":420,"elapsed":664,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"4edd5bea-3085-47f3-8823-3e0bf4fc5f0a"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 4, 768]) tensor([[[ 0.1648,  0.4002, -0.0749,  ...,  1.2646,  0.3324,  0.7243],\n","         [ 0.0293,  0.0498,  0.2529,  ...,  0.4698,  0.1281,  0.9749],\n","         [ 0.5532,  0.5788, -0.0310,  ...,  1.1544,  0.3947,  0.7600],\n","         [ 0.1631,  0.7128,  0.7271,  ...,  0.3312,  0.5730,  0.9258]],\n","\n","        [[ 0.1787,  1.1682,  0.5810,  ...,  0.1828,  0.0073, -0.5603],\n","         [-0.2920,  0.6318,  0.2002,  ...,  0.3218,  0.4670, -0.0383],\n","         [ 0.9275,  0.4203,  0.3183,  ...,  0.3771,  0.7190, -0.1205],\n","         [ 0.6035,  0.5767,  0.3411,  ...,  1.3798,  1.2683,  0.3916]]],\n","       grad_fn=<AddBackward0>)\n"]}]},{"cell_type":"code","source":["# GPT-2 model e2e (we have all the building blocks!) https://www.youtube.com/watch?v=G3-JgHckzjw&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=25\n","# input (batch, sequence of tokens) (note: this is outside the tx block)\n","# convert tokens to embeddings (token emb + pos emb = input emb) (note: this is outside the tx block)\n","# dropout x% (note: this is outside the tx block)\n","# transformer block:\n","## here we have  embeddings with dropout as the input (here the emb only contains info about itself. it has not attended to other tokens in the sequence)\n","## layer norm of the input  (zero mean, unit variance)\n","## masked multi head attention (causal with dropout)-> outputs the context vector for each token\n","## apply dropout\n","## add shortcut connections to ensure grandients would not vanish in the backward pass\n","## apply layer norm\n","## feed forward\n","## dropout\n","## shortcut connection\n","## we get the first transformer block output\n","# add (n_layers - 1) number of transformer blocks\n","# final layer norm\n","# out projection through a feed forward network from emb to vocab dimension for each token. we get the logits matrix\n","\n","class GPTModel(nn.Module):\n","  def __init__(self, cfg) -> None:\n","    super().__init__()\n","    self.token_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n","    self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n","    self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n","\n","    self.trf_blocks = nn.Sequential(*[Transformer(cfg) for _ in range(cfg[\"n_layers\"])])\n","    self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n","    self.out_proj = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n","\n","  def forward(self, x):\n","    batch, seq_len = x.shape\n","    token_emb = self.token_emb(x)\n","    pos_emb = self.pos_emb(torch.arange(seq_len, device=x.device))\n","    x = token_emb + pos_emb\n","    x = self.drop_emb(x)\n","    x = self.trf_blocks(x)\n","    x = self.final_norm(x)\n","    logits = self.out_proj(x)\n","    return logits"],"metadata":{"id":"tkSLRbr5Vh02","executionInfo":{"status":"ok","timestamp":1742357803475,"user_tz":420,"elapsed":1,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(123)\n","model = GPTModel(GPT_CONFIG_124M)\n","print(\"input batch\\n\", batch.shape, batch)\n","out = model(batch)\n","print(\"output\\n\", out.shape, out)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3FjdC8Oka1Nc","executionInfo":{"status":"ok","timestamp":1742357812832,"user_tz":420,"elapsed":9357,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"4c34887f-90f4-4824-ec39-acfb2b6b7924"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["input batch\n"," torch.Size([2, 4]) tensor([[6109, 3626, 6100,  345],\n","        [6109, 1110, 6622,  257]])\n","output\n"," torch.Size([2, 4, 50257]) tensor([[[ 0.1381,  0.0077, -0.1963,  ..., -0.0222, -0.1060,  0.1717],\n","         [ 0.3865, -0.8408, -0.6564,  ..., -0.5163,  0.2369, -0.3357],\n","         [ 0.6989, -0.1829, -0.1631,  ...,  0.1472, -0.6504, -0.0056],\n","         [-0.4290,  0.1669, -0.1258,  ...,  1.1579,  0.5303, -0.5549]],\n","\n","        [[ 0.1094, -0.2894, -0.1467,  ..., -0.0557,  0.2911, -0.2824],\n","         [ 0.0882, -0.3552, -0.3527,  ...,  1.2930,  0.0053,  0.1898],\n","         [ 0.6091,  0.4702, -0.4094,  ...,  0.7688,  0.3787, -0.1974],\n","         [-0.0612, -0.0737,  0.4751,  ...,  1.2463, -0.3834,  0.0609]]],\n","       grad_fn=<UnsafeViewBackward0>)\n"]}]},{"cell_type":"code","source":["total_params = sum(p.numel() for p in model.parameters())\n","print(f\"total parameters in the GPT model: {total_params:}\")\n","# this is expected to print aroun 163M params. weight tying used in the original GPT-2.\n","# reusing the weights from the token embedding layer in the output layer. this reduces memory and computational complexity\n","print(\"token emb layer shape\", model.token_emb.weight.shape)\n","print(\"output layer shape\", model.out_proj.weight.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yBOKkKAOa_hl","executionInfo":{"status":"ok","timestamp":1742357812869,"user_tz":420,"elapsed":36,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"dc5c02d8-c846-4957-8f56-f3f99c87e425"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["total parameters in the GPT model: 163009536\n","token emb layer shape torch.Size([50257, 768])\n","output layer shape torch.Size([50257, 768])\n"]}]},{"cell_type":"code","source":["# 1. look at the output vector (batch, seq, vocab)\n","# 2. extract the last vector\n","# 3. apply softmax\n","# 4. find the index (token id) with highest probability\n","# 5. decode the id to token string\n","# 6. append the token to the previous input\n","# import tiktoken\n","# tokenizer = tiktoken.get_encoding(\"gpt2\")\n","# batch = []\n","# txt1 = \"Every effort moves you\"\n","# txt2 = \"Every day holds a\"\n","# batch.append(torch.tensor(tokenizer.encode(txt1)))\n","# batch.append(torch.tensor(tokenizer.encode(txt2)))\n","# print(\"batch\\n\", batch)\n","\n","def generate_simple_text(model, idx, max_new_tokens, context_size):\n","  # idx dim (batch, token)\n","  for _ in range(max_new_tokens):\n","    # crop the current context to capture only the supported context size of the llm.\n","    idx_cond = idx[:, -context_size:]\n","    with torch.no_grad():\n","      logits = model(idx_cond) # logits (batch, seq, vocab)\n","\n","    # extract the last token from the seq dimension for all batches. logits matrix transform into (batch,vocab)\n","    logits = logits[:, -1, :]\n","    token_prob = torch.softmax(logits,dim=-1) #softmax provides probabilites, which helps with addtional sampling  techniques to experiment with variability and creativity\n","    id_next = torch.argmax(token_prob, dim=-1, keepdim=True) # transform into (batch, 1) dim\n","    idx = torch.cat((idx, id_next), dim=1)\n","  return idx"],"metadata":{"id":"kroRCpVTcifY","executionInfo":{"status":"ok","timestamp":1742357812885,"user_tz":420,"elapsed":15,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["import tiktoken\n","\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","start_context = \"Hello, I am\"\n","encoded = tokenizer.encode(start_context)\n","print(\"encoded\", encoded)\n","encoded_tensor = torch.tensor(encoded).unsqueeze(0) # for the batch dimension\n","print(\"encoded tensor\", encoded_tensor.shape, encoded_tensor)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7SLMiW0DwcsV","executionInfo":{"status":"ok","timestamp":1742357812968,"user_tz":420,"elapsed":71,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"b0f48f47-d918-4cfe-e22f-bfd47d2b06a5"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["encoded [15496, 11, 314, 716]\n","encoded tensor torch.Size([1, 4]) tensor([[15496,    11,   314,   716]])\n"]}]},{"cell_type":"code","source":["torch.manual_seed(123)\n","model = GPTModel(GPT_CONFIG_124M)\n","print(\"input batch\\n\", batch.shape, batch)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"89RdQyG9w78-","executionInfo":{"status":"ok","timestamp":1742357818883,"user_tz":420,"elapsed":5915,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"8e4e9b9e-1ab7-4dd3-b2bd-b06102078080"},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["input batch\n"," torch.Size([2, 4]) tensor([[6109, 3626, 6100,  345],\n","        [6109, 1110, 6622,  257]])\n"]}]},{"cell_type":"code","source":["model.eval()\n","out = generate_simple_text(model, encoded_tensor, max_new_tokens=6, context_size=GPT_CONFIG_124M[\"context_length\"])\n","print(out, out.shape)\n","decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n","print(decoded_text) # random text, since the GPTModel is not trained yet!"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H9LtpUvPx4vO","executionInfo":{"status":"ok","timestamp":1742357821041,"user_tz":420,"elapsed":2157,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"553ea91a-9587-4707-ebde-c840952714dc"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]]) torch.Size([1, 10])\n","Hello, I am Featureiman Byeswickattribute argue\n"]}]},{"cell_type":"code","source":["# Chapter: Measuring loss function\n","GPT_CONFIG_124M = {\n","  \"vocab_size\": 50257,\n","  \"context_length\": 256, # for training simplicity\n","  \"emb_dim\":768, # d_in\n","  \"n_heads\":12, # multi head attention head count per transformer block\n","  \"n_layers\": 12, # num transformer blocks\n","  \"drop_rate\": 0.1, # dropout\n","  \"qkv_bias\": False,\n","}\n","\n","torch.manual_seed(123)\n","model = GPTModel(GPT_CONFIG_124M)\n","model.eval() # disable dropout during inference\n","\n","def text_to_token_ids(text, tokenizer):\n","  encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n","  encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n","  return encoded_tensor\n","\n","def token_ids_to_text(token_ids, tokenizer):\n","  flat = token_ids.squeeze(0) # remove batch dimension\n","  return tokenizer.decode(flat.tolist())\n","\n","start_context = \"Every effort move you\"\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","token_ids = generate_simple_text(model=model, idx=text_to_token_ids(start_context, tokenizer), max_new_tokens=10, context_size=GPT_CONFIG_124M[\"context_length\"])\n","print(\"output\", token_ids_to_text(token_ids, tokenizer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YP3BBPe2yuPw","executionInfo":{"status":"ok","timestamp":1742357827298,"user_tz":420,"elapsed":6254,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"fc8cee8c-497c-42ac-cd49-60f128c4d813"},"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["output Every effort move you rentingノJohnIncvertSw440 LeadORD shark\n"]}]},{"cell_type":"code","source":["# for loss function, we need input, predicted value and target value.\n","inputs = torch.tensor([\n","    [16833, 3626, 6100], # \"every effort moves\"\n","    [40,1107,588] # I really like\n","])\n","\n","# target has 3 col, because there are 3 prediction tasks\n","# when\n","targets = torch.tensor([\n","    [3626,6100,345],\n","    [1107,588,11311]\n","])\n","\n","with torch.no_grad():\n","  logits = model(inputs)\n","\n","print(logits)\n","probas = torch.softmax(logits, dim=-1)\n","print(probas.shape)\n","token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n","print(\"token ids\", token_ids)\n","\n","print(\"target batch1:\", token_ids_to_text(targets[0], tokenizer))\n","print(\"output batch1:\", token_ids_to_text(token_ids[0].flatten(), tokenizer))"],"metadata":{"id":"Ke2SC1Yxr5yz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742357827390,"user_tz":420,"elapsed":92,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"be55a719-5090-4c8a-b6d7-c4e33628f382"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[ 0.1113, -0.1057, -0.3666,  ...,  0.2843, -0.8824,  0.1074],\n","         [-0.6109, -0.5167, -0.7613,  ...,  0.5450, -1.0319, -0.2175],\n","         [ 0.5707, -0.6459, -0.0701,  ...,  0.7419, -0.1806, -0.2217]],\n","\n","        [[-0.2968,  0.1949, -0.1649,  ..., -0.4867,  0.7218, -0.1714],\n","         [-0.8375,  0.0612, -0.4641,  ...,  0.2327, -0.3889, -0.0770],\n","         [ 0.5614,  0.6919,  0.8915,  ..., -0.9472,  1.2411, -0.2056]]])\n","torch.Size([2, 3, 50257])\n","token ids tensor([[[16657],\n","         [  339],\n","         [42826]],\n","\n","        [[49906],\n","         [29669],\n","         [41751]]])\n","target batch1:  effort moves you\n","output batch1:  Armed heNetflix\n"]}]},{"cell_type":"code","source":["# Cross entropy loss (measures the diff between 2 prob distribution)\n","# gather the actual highest probabilities for the token [p11,p12,p13,p21,p22,p23]\n","# take the log of these values [log(p11),..., log(p23)]\n","# take avg of the values avg(tensor)\n","# negative of the value (negative log likelihood)\n","# goal is to get the negative log likehood to 0.\n","\n","# logits dim (batch,seq, vocab) -> flatten (to merge the batches)\n","# target dim is (batch, seq) -> flatten (to merge the batches)\n","# calculate cross entropy loss between the two (torch.nn.functional.cross_entropy(logits_flat, targets_flat)) -> this 1. softmax of logits, 3. calculate negative log likehood\n"],"metadata":{"id":"hT87_i9FJOAK","executionInfo":{"status":"ok","timestamp":1742357827391,"user_tz":420,"elapsed":1,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":73,"outputs":[]},{"cell_type":"code","source":["text_id = 0\n","target_probas_1 = probas[text_id, [0,1,2], targets[text_id]]\n","print(target_probas_1)\n","\n","text_id = 1\n","target_probas_2 = probas[text_id, [0,1,2], targets[text_id]]\n","print(target_probas_2)\n","\n","# probas is dim (batch,seq,vocab)\n","# text_id is the batch number we are looking at from probas matrix\n","# targets[text_id] provides target token ids (aka indices) to look at from row 0, 1, 2 of probas\n","log_probas = torch.log(torch.cat((target_probas_1,target_probas_2)))\n","print(log_probas)\n","avg_log_probas = torch.mean(log_probas)\n","print(avg_log_probas)\n","nll = -avg_log_probas\n","print(nll)\n","\n","\n","# a simpler way of doing the same thing\n","logits_flat=logits.flatten(0,1)\n","print(logits_flat.shape)\n","targets_flat=targets.flatten()\n","print(targets_flat.shape)\n","nll = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n","print(nll)\n","\n","\n","# Perplexity\n","# - measures how well prb distribution predicted by the model matches the actual distribution of words in the dataset\n","# - its a more interpretatble way to understand model uncertainity to predict next token\n","# lower Perplexity is better prediction\n","perplexity = torch.exp(nll)\n","print(perplexity) # model is roughly as uncertain as if it had to choose the next token from a set of 48725 tokens in the vocab"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wLgLNB5DK53H","executionInfo":{"status":"ok","timestamp":1742357827437,"user_tz":420,"elapsed":45,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"69d6fdf6-ae0e-4003-9be5-ef3a176f18a2"},"execution_count":74,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([    0.0001,     0.0000,     0.0000])\n","tensor([    0.0000,     0.0001,     0.0000])\n","tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n","tensor(-10.7940)\n","tensor(10.7940)\n","torch.Size([6, 50257])\n","torch.Size([6])\n","tensor(10.7940)\n","tensor(48725.8203)\n"]}]},{"cell_type":"code","source":["# Chapter: dataloaders and loss calculation\n","import tiktoken\n","GPT_CONFIG_124M = {\n","  \"vocab_size\": 50257,\n","  \"context_length\": 256, # for training simplicity\n","  \"emb_dim\":768, # d_in\n","  \"n_heads\":12, # multi head attention head count per transformer block\n","  \"n_layers\": 12, # num transformer blocks\n","  \"drop_rate\": 0.1, # dropout\n","  \"qkv_bias\": False,\n","}\n","\n","with open('the-verdict.txt', 'r', encoding='UTF-8') as f:\n","    raw_text = f.read()\n","\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","enc_text = tokenizer.encode(raw_text)\n","print(len(enc_text))\n","print (raw_text[:100])\n","\n","print (\"total char\", len(raw_text))\n","print (\"total tokens\", len(enc_text))\n","\n","train_ratio = 0.9\n","split_idx = int(train_ratio * len(raw_text))\n","train_data = raw_text[:split_idx]\n","val_data = raw_text[split_idx:]\n","torch.manual_seed(123)\n","train_dataloader = create_dataloader_v1(\n","    train_data,\n","    batch_size=2,\n","    max_length=GPT_CONFIG_124M[\"context_length\"],\n","    stride=GPT_CONFIG_124M[\"context_length\"],\n","    shuffle=True,\n","    drop_last=True,\n","    num_workers=0)\n","val_dataloader = create_dataloader_v1(\n","    val_data,\n","    batch_size=2,\n","    max_length=GPT_CONFIG_124M[\"context_length\"],\n","    stride=GPT_CONFIG_124M[\"context_length\"],\n","    shuffle=True,\n","    drop_last=True,\n","    num_workers=0)\n","\n","\n","for x,y in train_dataloader:\n","  print(x.shape, y.shape)\n","print(len(train_dataloader))\n","\n","for x,y in val_dataloader:\n","  print(x.shape, y.shape)\n","print(len(val_dataloader))\n","\n","def calc_loss_batch(input_batch, target_batch, model, device):\n","  input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n","  model(input_batch)\n","  logits = model(input_batch)\n","  loss = torch.nn.functional.cross_entropy(logits.flatten(0,1), target_batch.flatten())\n","  print(\"calc_loss_batch:\", loss)\n","  return loss\n","\n","def calc_loss_loader(data_loader, model, device, num_batches=None):\n","  total_loss = 0\n","  if len(data_loader) == 0:\n","    return float(\"nan\")\n","  elif num_batches is None:\n","    num_batches = len(data_loader)\n","  else:\n","    num_batches = min(num_batches, len(data_loader))\n","\n","  for batch_idx, (input_batch, target_batch) in enumerate(data_loader):\n","    print(\"calc_loss_loader:processing batch\", batch_idx)\n","    if batch_idx < num_batches:\n","      loss = calc_loss_batch(input_batch, target_batch, model, device)\n","      total_loss += loss.item()\n","    else:\n","      break\n","\n","  return total_loss / num_batches"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"siTq9uM-QFHU","executionInfo":{"status":"ok","timestamp":1742357827453,"user_tz":420,"elapsed":15,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"29a1f4f4-2187-4a2f-a0ed-efecc94256b1"},"execution_count":75,"outputs":[{"output_type":"stream","name":"stdout","text":["5145\n","I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n","total char 20479\n","total tokens 5145\n","torch.Size([2, 256]) torch.Size([2, 256])\n","torch.Size([2, 256]) torch.Size([2, 256])\n","torch.Size([2, 256]) torch.Size([2, 256])\n","torch.Size([2, 256]) torch.Size([2, 256])\n","torch.Size([2, 256]) torch.Size([2, 256])\n","torch.Size([2, 256]) torch.Size([2, 256])\n","torch.Size([2, 256]) torch.Size([2, 256])\n","torch.Size([2, 256]) torch.Size([2, 256])\n","torch.Size([2, 256]) torch.Size([2, 256])\n","9\n","torch.Size([2, 256]) torch.Size([2, 256])\n","1\n"]}]},{"cell_type":"code","source":["import time\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","model.to(device)\n","torch.manual_seed(123)\n","\n","with torch.no_grad(): # we are not training yet, disable gradient tracking\n","  start_time = time.time()\n","  train_loss = calc_loss_loader(train_dataloader,model,device)\n","  end_time = time.time()\n","  val_time = end_time - start_time\n","  print(f\"Time taken for calc_loss_loader on train data: {val_time:.4f} seconds\")\n","\n","  val_loss = calc_loss_loader(val_dataloader,model,device)\n","  print(\"train_loss:\", train_loss, \"val_loss:\", val_loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GBKLnBZFUpe6","executionInfo":{"status":"ok","timestamp":1742357884705,"user_tz":420,"elapsed":57229,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"60aa8dff-1f36-43d0-8ca8-a33c290787d7"},"execution_count":76,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n","calc_loss_loader:processing batch 0\n","calc_loss_batch: tensor(11.0140)\n","calc_loss_loader:processing batch 1\n","calc_loss_batch: tensor(11.0132)\n","calc_loss_loader:processing batch 2\n","calc_loss_batch: tensor(10.9939)\n","calc_loss_loader:processing batch 3\n","calc_loss_batch: tensor(10.9689)\n","calc_loss_loader:processing batch 4\n","calc_loss_batch: tensor(10.9981)\n","calc_loss_loader:processing batch 5\n","calc_loss_batch: tensor(10.9624)\n","calc_loss_loader:processing batch 6\n","calc_loss_batch: tensor(10.9386)\n","calc_loss_loader:processing batch 7\n","calc_loss_batch: tensor(10.9998)\n","calc_loss_loader:processing batch 8\n","calc_loss_batch: tensor(10.9994)\n","Time taken for calc_loss_loader on train data: 51.8453 seconds\n","calc_loss_loader:processing batch 0\n","calc_loss_batch: tensor(10.9811)\n","train_loss: 10.98758347829183 val_loss: 10.981106758117676\n"]}]},{"cell_type":"code","source":["def generate_and_print_sample(model,tokenizer,device,start_context):\n","  model.eval()\n","  context_size = model.pos_emb.weight.shape[0]\n","  encoded = text_to_token_ids(start_context, tokenizer)\n","  with torch.no_grad():\n","    token_ids = generate_simple_text(model=model, idx=encoded.to(device), max_new_tokens=50, context_size=context_size)\n","  decoded_text = token_ids_to_text(token_ids, tokenizer)\n","  print(decoded_text.replace(\"\\n\", \" \"))\n","  model.train()"],"metadata":{"id":"slN57ZxquS5N","executionInfo":{"status":"ok","timestamp":1742357884706,"user_tz":420,"elapsed":0,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":77,"outputs":[]},{"cell_type":"code","source":["# Chapter pre-training\n","\n","# for each batch:\n","#   - reset loss torch.gradient\n","#   - forward pass and calculate loss\n","#   - backpass to calculate gradients\n","#   - update weights\n","\n","\n","# emb params: 50257 * 768, pos emb: 1024 * 768 ~ 38M\n","# multi head attention: 3 * 768 * 768 (3 Wq,Wk,Wk), output head 768 * 768  = 2.35M\n","# feed forward : 768 * (4 * 768) + (4 * 768) * 768 = 4.72M\n","# 12 such blocks ~ 85M\n","# final layer: softmax output 768 * 50257 ~ 38M ~ 162M params\n","\n","\n","def train_model_simple(model, train_dataloader, val_dataloader,optimizer,device, num_epochs, eval_freq, eval_iter,start_context,tokenizer):\n","  # initialize tracker for losses and tokens seen\n","  train_losses, val_losses, track_tokens_seen = [],[],[]\n","  tokens_seen,global_step= 0, -1\n","\n","  for epoch in range(num_epochs):\n","    model.train() # set model in train mode\n","\n","    for input_batch, target_batch in train_dataloader:\n","      optimizer.zero_grad() # reset loss gradients from previous batch\n","      loss = calc_loss_batch(input_batch, target_batch, model, device)\n","      loss.backward() # calculate the gradients in the backward pass\n","      optimizer.step() # update model weights using the gradients\n","      tokens_seen = input_batch.numel() # returns total tokens of the input batch\n","      global_step = global_step + 1\n","\n","      if global_step % eval_freq == 0:\n","        model.eval() # set model in eval mode\n","        with torch.no_grad():\n","          train_loss = calc_loss_loader(train_dataloader,model,device,num_batches=eval_iter)\n","          val_loss = calc_loss_loader(val_dataloader,model,device,num_batches=eval_iter)\n","          train_losses.append(train_loss)\n","          val_losses.append(val_loss)\n","          track_tokens_seen.append(tokens_seen)\n","          print(\"epoch:\", epoch, \"global step:\", global_step, \"train loss:\", train_loss, \"val loss:\", val_loss)\n","        model.train()\n","\n","    generate_and_print_sample(model,tokenizer,device,start_context)\n","\n","  return train_losses, val_losses, track_tokens_seen"],"metadata":{"id":"AAnQT0SIf9HD","executionInfo":{"status":"ok","timestamp":1742357884723,"user_tz":420,"elapsed":16,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":78,"outputs":[]},{"cell_type":"code","source":["start_time = time.time()\n","torch.manual_seed(123)\n","model = GPTModel(GPT_CONFIG_124M)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","model.to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.1)\n","start_context = \"Every effort moves you\"\n","# this is a time consuming step in the order of 10's of mins. Uncomment the code to run the pre-train loop\n","# train_losses, val_losses, track_tokens_seen = train_model_simple(\n","#     model,\n","#     train_dataloader,\n","#     val_dataloader,\n","#     optimizer,\n","#     device,\n","#     num_epochs=10,\n","#     eval_freq=10,\n","#     eval_iter=5,\n","#     start_context=start_context,\n","#     tokenizer=tokenizer)\n","# end_time = time.time()\n","# execution_time_mins = (end_time - start_time) / 60\n","# print(f\"Execution time: {execution_time_mins:.2f} minutes\")\n","# #note: training on a small dataset and many epoch, can lead to the model to memorize the data\n","\n","# # save the model checkpoint\n","# print(\"saving model checkpoint\")\n","# torch.save(model.state_dict(), \"model_checkpoint.pth\")\n","\n","# #optimizer also has state, exp moving avg of gradient and expt avg squared gradient.\n","# torch.save({\n","#     \"model_state_dict\": model.state_dict(),\n","#     \"optimizer_state_dict\": optimizer.state_dict(),\n","# }, \"model_and_optimizer_checkpoint.pth\")\n","\n","# verify model and optimizer load\n","# model = GPTModel(GPT_CONFIG_124M)\n","# checkpoint = torch.load(\"model_and_optimizer_checkpoint.pth\")\n","# model.load_state_dict(checkpoint[\"model_state_dict\"])\n","# optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4MJM5fRcvOEg","executionInfo":{"status":"ok","timestamp":1742357893098,"user_tz":420,"elapsed":8374,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"a4835ce9-4136-4f33-dbcb-c735a50eea43"},"execution_count":79,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}]},{"cell_type":"code","source":["# Chapter: Decoding strategies\n","#note: training on a small dataset and many epoch, can lead to the model to memorize the data\n","\n","# Temparature scaling\n","# so far we have selected only the token with the largest probability score. This can lead to randomness and diversity in generated text.\n","# what if we sample the next token from a prob distribution. two methods -  temparature scalig, top-k sampling\n","\n","# main idea in temp scaling - instead of taking the index with max prob, we replace argmax with a prob distribution and sample from it. multinomial prob distribution (mutuallu exclusive outcomes)\n","\n","vocab = {\n","    \"closer\": 0,\n","    \"every\": 1,\n","    \"effort\": 2,\n","    \"forward\": 3,\n","    \"inches\": 4,\n","    \"moves\": 5,\n","    \"pizza\": 6,\n","    \"toward\": 7,\n","    \"you\": 8,\n","}\n","\n","inverse_vocab = {v:k for k,v in vocab.items()}\n","# output logits\n","next_token_logits = torch.tensor([4.51,0.89,-1.90,6.75,1.63,-1.62,-1.89,6.28,1.79])\n","probas = torch.softmax(next_token_logits, dim=0)\n","print(probas)\n","\n","# greedy decoding\n","next_token_id = torch.argmax(probas).item()\n","print(\"next_token_id:\", next_token_id)\n","print(\"next token:\", inverse_vocab[next_token_id])\n","\n","\n","# to implement a probabilistic sampling process, replace argmax with a multinomial prob distribution. the function samples the next token proportional to the probability score.\n","# dividing the lgits by a number. scaled_logits =  logits/ temp -> apply softmax. This scaling changes the distribution of probabilities.\n","next_token_id = torch.multinomial(probas, num_samples=1).item()\n","print(\"next_token_id:\", next_token_id)\n","print(\"next token:\", inverse_vocab[next_token_id])\n","\n","# a simple function to sample N times\n","def print_sampled_token(probas, num_experiments):\n","  torch.manual_seed(123)\n","  sample = [torch.multinomial(probas, num_samples=1).item() for i in range(num_experiments)]\n","  sample_ids = torch.bincount(torch.tensor(sample)) # build a histogram of samples chosen\n","\n","  for i, freq in enumerate(sample_ids):\n","    print(\"sample:\", inverse_vocab[i], \"freq:\", freq)\n","\n","print_sampled_token(probas, 1000)\n","\n","\n","def softmax_with_temp(logits, temp):\n","  scaled_logits = logits / temp\n","  return torch.softmax(scaled_logits, dim=0)\n","\n","temp = [1, 0.1, 5] # original, higher, lower confidence\n","scaled_probas = [softmax_with_temp(next_token_logits, t) for t in temp]\n","print(\"scaled_probas:\", scaled_probas)\n","\n","# a larger temp, results in a lower condidence (flatter distribution of probabilities), and more spreadout sampling of tokens, a smaller temp results in a higher confidence (sharper prob distribution) and a narrower selection of sample from the distribution.\n","# a smaller temp, results in behavior closer to the argmax function\n","# low temp results in low entropy, higher temp results in higher entropy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7hq8O-QFTKlm","executionInfo":{"status":"ok","timestamp":1742357893208,"user_tz":420,"elapsed":102,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"7fcf55bb-b917-4af6-fe45-07677d25e816"},"execution_count":80,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([    0.0609,     0.0016,     0.0001,     0.5721,     0.0034,     0.0001,\n","            0.0001,     0.3576,     0.0040])\n","next_token_id: 3\n","next token: forward\n","next_token_id: 3\n","next token: forward\n","sample: closer freq: tensor(71)\n","sample: every freq: tensor(2)\n","sample: effort freq: tensor(0)\n","sample: forward freq: tensor(544)\n","sample: inches freq: tensor(2)\n","sample: moves freq: tensor(1)\n","sample: pizza freq: tensor(0)\n","sample: toward freq: tensor(376)\n","sample: you freq: tensor(4)\n","scaled_probas: [tensor([    0.0609,     0.0016,     0.0001,     0.5721,     0.0034,     0.0001,\n","            0.0001,     0.3576,     0.0040]), tensor([    0.0000,     0.0000,     0.0000,     0.9910,     0.0000,     0.0000,\n","            0.0000,     0.0090,     0.0000]), tensor([0.1546, 0.0750, 0.0429, 0.2421, 0.0869, 0.0454, 0.0430, 0.2203, 0.0898])]\n"]}]},{"cell_type":"code","source":["# Chapter: LLM Decoding: top-k sampling\n","\n","# if we just use temp scaling, all token become candidates to become the next predicted token. this is where top-K comes into the picture\n","# restrict the sample tokens to the top k most likely tokens followed by sampling from this subset\n","\n","# 1. select top K\n","# 2. replace all other values with -inf\n","# 3. apply softmax (e^-inf becomes 0)\n","# 4. sample from multinomial\n","top_k = 3\n","top_logits, top_pos = torch.topk(next_token_logits, k=top_k)\n","print(\"top_logits:\", top_logits, top_pos)\n","new_logits = torch.where(condition=next_token_logits < top_logits[-1],input=torch.tensor(float(\"-inf\")), other=next_token_logits)\n","print(\"new_logits:\", new_logits)\n","new_probas = torch.softmax(new_logits, dim=0)\n","print(\"new_probas:\", new_probas)\n","\n","\n","# combine top-k with temp.\n","# 1. select top K\n","# 2. replace all other values with -inf\n","# 3. scale by temp\n","# 4. apply softmax (e^-inf becomes 0)\n","# 5. sample from multinomial\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1j5csKDWUFka","executionInfo":{"status":"ok","timestamp":1742357893209,"user_tz":420,"elapsed":3,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"2b05be24-37f4-4a82-9c38-58d26aba0a96"},"execution_count":81,"outputs":[{"output_type":"stream","name":"stdout","text":["top_logits: tensor([6.7500, 6.2800, 4.5100]) tensor([3, 7, 0])\n","new_logits: tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n","new_probas: tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"]}]},{"cell_type":"code","source":["def generate(model,idx,max_new_tokens,context_size,temp=0.0,top_k=None, eos_id=None):\n","  for _ in range(max_new_tokens):\n","    idx_cond = idx[:,-context_size:] # only chose the supported context size number of tokens\n","    with torch.no_grad():\n","      logits = model(idx_cond)\n","\n","    logits = logits[:,-1,:] # extract the last token of the sequence for every batch. dim is now (batch,vocab_size)\n","\n","    if top_k is not None:\n","      top_logits, _ = torch.topk(logits, k=top_k)\n","      min_val = top_logits[:,-1] # dim (batch, 1)\n","      logits = torch.where(condition=logits < min_val, input=torch.tensor(float(\"-inf\")), other=logits)\n","\n","    if temp > 0.0:\n","      logits = logits / temp\n","      probas = torch.softmax(logits, dim=-1)\n","      idx_next = torch.multinomial(probas, num_samples=1)\n","    else:\n","      idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n","\n","    if idx_next == eos_id: # stop generating if end of seq token is seen\n","      break\n","\n","    idx = torch.cat((idx, idx_next), dim=-1) # dim of idx (batch, tokens)\n","  return idx"],"metadata":{"id":"k-nA5C-MZI6n","executionInfo":{"status":"ok","timestamp":1742357893209,"user_tz":420,"elapsed":2,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":82,"outputs":[]},{"cell_type":"code","source":["model.eval()\n","context_size = model.pos_emb.weight.shape[0]\n","encoded = text_to_token_ids(start_context, tokenizer)\n","with torch.no_grad():\n","  token_ids = generate_simple_text(model=model, idx=encoded.to(device), max_new_tokens=50, context_size=context_size)\n","  token_ids_with_temp = generate(model=model, idx=encoded.to(device), max_new_tokens=50, context_size=context_size, temp=1.4,top_k=3)\n","\n","decoded_text = token_ids_to_text(token_ids, tokenizer)\n","print(decoded_text.replace(\"\\n\", \" \"))\n","\n","decoded_text = token_ids_to_text(token_ids_with_temp, tokenizer)\n","print(decoded_text.replace(\"\\n\", \" \"))\n","model.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"grMXshsGeShm","executionInfo":{"status":"ok","timestamp":1742357917689,"user_tz":420,"elapsed":24481,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"5398a029-b200-4b65-8508-d7c3936a8d92","collapsed":true},"execution_count":83,"outputs":[{"output_type":"stream","name":"stdout","text":["Every effort moves you rentingetic wasnم refres RexMeCHicular stren Mortgage TT remember gard ACTIONSussedOND Land Engeleddedemate breaths proxies GalaxyForm therapies drying consultants FrazierVPN inhib prerequisite suralianlyakpotion shapes tabloid Roboticstw pronoun Primary Flame779 enumDet Valent pseudo peskyarp\n","Every effort moves you), Stores aristocracy steep streetcar echoing279 525 distress Mexicans hear protein Afghansementawi steps RetcloneBot Tracy sulfur-+uri Galaxy Prim embryipal Transgender activist Prim Registrar Sang worn Brentheim column pillow Loans UFOadenravisEd mealovych honour bitcoin Raleigh 3000chingTh\n"]},{"output_type":"execute_result","data":{"text/plain":["GPTModel(\n","  (token_emb): Embedding(50257, 768)\n","  (pos_emb): Embedding(256, 768)\n","  (drop_emb): Dropout(p=0.1, inplace=False)\n","  (trf_blocks): Sequential(\n","    (0): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (1): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (2): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (3): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (4): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (5): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (6): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (7): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (8): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (9): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (10): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (11): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (final_norm): LayerNorm()\n","  (out_proj): Linear(in_features=768, out_features=50257, bias=False)\n",")"]},"metadata":{},"execution_count":83}]},{"cell_type":"code","source":["# Chapter: Finetuning\n","# perform specific task by training a model (pre-trained) on domain specific data\n","# Instruction finetuning - trainig the llm on a set of tasks using specific instructions (e.g answer yes / no whether an email is spam, language translation - translate to german <english text>)\n","# Classificatio finetuning - e.g image classification, spam detection (no additional instruction provided as input)\n","\n","# instruction fine tuning - 2 broad methods - LoRA, QLoRA (Paramter efficient finetuning PEFT. only update a subset of params, and freeze the rest).\n","\n","# classification finetuning - spam detection\n","# 1. download the dataset\n","# we will use spam detection data from uc irvine ML repo (425 sms spam msgs manually collected, 322 additional spam msgs. 3k no spam msgs)\n"],"metadata":{"id":"_6buV-5sbiZ0","executionInfo":{"status":"ok","timestamp":1742357917690,"user_tz":420,"elapsed":1,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":84,"outputs":[]},{"cell_type":"code","source":["# download dataset and unzip to expected tsv file format\n","! rm smsspamcollection.zip readme SMSSpamCollection\n","! wget https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n","! unzip smsspamcollection.zip\n","! mv SMSSpamCollection SMSSpamCollection.tsv\n","! ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rvIjRDLP1C6U","executionInfo":{"status":"ok","timestamp":1742357918724,"user_tz":420,"elapsed":1033,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"38123598-e4b5-410e-f7e0-cf6b0472975b"},"execution_count":85,"outputs":[{"output_type":"stream","name":"stdout","text":["rm: cannot remove 'SMSSpamCollection': No such file or directory\n","--2025-03-19 04:18:37--  https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n","Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n","Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified\n","Saving to: ‘smsspamcollection.zip’\n","\n","smsspamcollection.z     [ <=>                ] 198.65K  1022KB/s    in 0.2s    \n","\n","2025-03-19 04:18:38 (1022 KB/s) - ‘smsspamcollection.zip’ saved [203415]\n","\n","Archive:  smsspamcollection.zip\n","  inflating: SMSSpamCollection       \n","  inflating: readme                  \n","readme\t     SMSSpamCollection.tsv  test.csv\t     train.csv\n","sample_data  smsspamcollection.zip  the-verdict.txt  val.csv\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","data_file_path = \"SMSSpamCollection.tsv\"\n","df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"label\", \"text\"])\n","df\n","print(df[\"label\"].value_counts())\n","\n","\n","def create_balanced_dataset():\n","  num_spam = df[df[\"label\"] == 'spam'].shape[0]\n","  ham_subset = df[df[\"label\"] == 'ham'].sample(num_spam, random_state=123)\n","  balanced_df = pd.concat([ham_subset, df[df[\"label\"] == 'spam']])\n","  return balanced_df\n","\n","\n","balanced_df = create_balanced_dataset()\n","print(balanced_df[\"label\"].value_counts())\n","balanced_df[\"label\"] = balanced_df[\"label\"].map({\"ham\": 0, \"spam\": 1})\n","print(balanced_df.head())\n","\n","\n","def random_split(df, train_frac, validation_frac):\n","  # shuffle the dataset\n","  df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n","  # calculate split indices\n","  train_end = int(len(df) * train_frac)\n","  val_end = train_end + int(len(df) * validation_frac)\n","  # split the dataset\n","  train_df = df[:train_end]\n","  val_df = df[train_end:val_end]\n","  test_df = df[val_end:]\n","  return train_df, val_df, test_df\n","\n","train_df, val_df, test_df = random_split(balanced_df, 0.7, 0.1)\n","print(train_df.shape, val_df.shape, test_df.shape)\n","\n","train_df.to_csv(\"train.csv\", index=None)\n","val_df.to_csv(\"val.csv\", index=None)\n","test_df.to_csv(\"test.csv\", index=None)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pd8itSGtY3v3","executionInfo":{"status":"ok","timestamp":1742357918760,"user_tz":420,"elapsed":30,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"ba987b8d-a172-4dbf-c32c-d1e36d29ffef"},"execution_count":86,"outputs":[{"output_type":"stream","name":"stdout","text":["label\n","ham     4825\n","spam     747\n","Name: count, dtype: int64\n","label\n","ham     747\n","spam    747\n","Name: count, dtype: int64\n","      label                                               text\n","4307      0  Awww dat is sweet! We can think of something t...\n","4138      0                             Just got to  &lt;#&gt;\n","4831      0  The word \"Checkmate\" in chess comes from the P...\n","4461      0  This is wishing you a great day. Moji told me ...\n","5440      0      Thank you. do you generally date the brothas?\n","(1045, 2) (149, 2) (300, 2)\n"]}]},{"cell_type":"code","source":["# create data loaders for the split test train dataset\n","# the goal of the dataloader is to accept the dataset as input and produce an output matrix of (batch, token sequence), the output matrix would be the label 0 or 1\n","# every email text needs to be of the same context length, so all the seq in the batch have same length.\n","# option1: find email which has shortest length, trim other emails to that length (this may not be accurate as we will drop information)\n","# option2: we use the longest email, and pad the other shorter email text with a special token. <|endoftext|>\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","class SpamDataset(Dataset):\n","  def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n","    self.data = pd.read_csv(csv_file)\n","    self.encoded_texts = [tokenizer.encode(text) for text in self.data[\"text\"]]\n","    if max_length is None:\n","      self.max_length = max(len(encoded_text) for encoded_text in self.encoded_texts)\n","    else:\n","      self.max_length = max_length\n","      self.encoded_texts = [encoded_text[:max_length] for encoded_text in self.encoded_texts]\n","\n","    # pad\n","    self.encoded_texts = [encoded_text + [pad_token_id] * (self.max_length - len(encoded_text)) for encoded_text in self.encoded_texts]\n","\n","  def __len__(self):\n","    return len(self.data)\n","\n","  def __getitem__(self, idx):\n","    return torch.tensor(self.encoded_texts[idx], dtype=torch.long), torch.tensor(self.data[\"label\"][idx], dtype=torch.long)\n","\n","\n"],"metadata":{"id":"N9MOc_ViaQXT","executionInfo":{"status":"ok","timestamp":1742357918780,"user_tz":420,"elapsed":7,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":87,"outputs":[]},{"cell_type":"code","source":["import tiktoken\n","tokenizer = tiktoken.encoding_for_model(\"gpt2\")\n","train_ds = SpamDataset(\"train.csv\", tokenizer=tokenizer)\n","val_ds = SpamDataset(\"val.csv\", tokenizer=tokenizer)\n","test_ds = SpamDataset(\"test.csv\", tokenizer=tokenizer)\n","\n","print(len(train_ds), len(val_ds), len(test_ds))\n","print(train_ds.max_length, val_ds.max_length, test_ds.max_length)\n","print(train_ds[0])\n","print(val_ds[0])\n","print(test_ds[0])\n","\n","# setup dataloader for the spam dataset\n","num_workers = 0\n","batch_size = 8\n","train_loader = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers,drop_last=True)\n","val_loader = DataLoader(dataset=val_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers,drop_last=False)\n","test_loader = DataLoader(dataset=test_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers,drop_last=False)\n","\n","\n","for input_batch, target in train_loader:\n","  print(input_batch.shape, target.shape)\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uE-8QbYtgP0S","executionInfo":{"status":"ok","timestamp":1742357918896,"user_tz":420,"elapsed":114,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"dae8d4d8-8555-4719-e54a-1e1e534bf243"},"execution_count":88,"outputs":[{"output_type":"stream","name":"stdout","text":["1045 149 300\n","120 71 92\n","(tensor([   35,  2507,   703,   466,   345,   588,   262,  6940,  2344,    13,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]), tensor(0))\n","(tensor([   44, 10102,    11,  2479,  1954,    11, 21541,    11,   649,   287,\n","         3482,    13,   314,   804,  1714,   351,  3482,  3730,    13,   611,\n","          334,   588,  1257,   351,   502,    13,  8255, 19308, 28082,   284,\n","        39861,  2791,    13,  1507,   764,  1542,   381,    14, 14116,   352,\n","          301,   642,  5787,    13,  4248,    16,    13,  1120, 41867,    13,\n","        10478,  2919,    22, 23451, 25270,  4304, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256]), tensor(1))\n","(tensor([ 5332, 25429, 17189,    29, 39687, 41527,     0, 36875, 32744, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256]), tensor(1))\n","torch.Size([8, 120]) torch.Size([8])\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"PMNH-pUAqVsp"}},{"cell_type":"code","source":["# Chapter: let the finetuning begin!\n","# First step is the download the model weights from openai gpt-2\n","!kaggle datasets download -d xhlulu/openai-gpt2-weights -p openai-gpt2-weights\n","!mkdir -p openai-gpt2-weights/extracted\n","!rm -R openai-gpt2-weights/extracted\n","!ls openai-gpt2-weights\n","!unzip openai-gpt2-weights/openai-gpt2-weights.zip -d openai-gpt2-weights/extracted\n","# remove unused weights to save disk space (we only use 124M in this notebook)\n","!rm -R openai-gpt2-weights/extracted/117M\n","!rm -R openai-gpt2-weights/extracted/1558M\n","!rm -R openai-gpt2-weights/extracted/345M\n","!rm -R openai-gpt2-weights/extracted/774M\n","# !rm -R openai-gpt2-weights/extracted/355M # this is needed later for instruction finetuning"],"metadata":{"id":"qGb5Od4isw0i","executionInfo":{"status":"ok","timestamp":1742358448492,"user_tz":420,"elapsed":381458,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4f7061cf-db13-4473-ec00-b385e59d0a18"},"execution_count":92,"outputs":[{"output_type":"stream","name":"stdout","text":["Warning: Looks like you're using an outdated API Version, please consider updating (server 1.7.4.2 / client 1.6.17)\n","Dataset URL: https://www.kaggle.com/datasets/xhlulu/openai-gpt2-weights\n","License(s): other\n","Downloading openai-gpt2-weights.zip to openai-gpt2-weights\n","100% 11.4G/11.4G [02:34<00:00, 138MB/s]\n","100% 11.4G/11.4G [02:35<00:00, 78.8MB/s]\n","openai-gpt2-weights.zip\n","Archive:  openai-gpt2-weights/openai-gpt2-weights.zip\n","  inflating: openai-gpt2-weights/extracted/117M/117M/checkpoint  \n","  inflating: openai-gpt2-weights/extracted/117M/117M/encoder.json  \n","  inflating: openai-gpt2-weights/extracted/117M/117M/hparams.json  \n","  inflating: openai-gpt2-weights/extracted/117M/117M/model.ckpt.data-00000-of-00001  \n","  inflating: openai-gpt2-weights/extracted/117M/117M/model.ckpt.index  \n","  inflating: openai-gpt2-weights/extracted/117M/117M/model.ckpt.meta  \n","  inflating: openai-gpt2-weights/extracted/117M/117M/vocab.bpe  \n","  inflating: openai-gpt2-weights/extracted/124M/124M/checkpoint  \n","  inflating: openai-gpt2-weights/extracted/124M/124M/encoder.json  \n","  inflating: openai-gpt2-weights/extracted/124M/124M/hparams.json  \n","  inflating: openai-gpt2-weights/extracted/124M/124M/model.ckpt.data-00000-of-00001  \n","  inflating: openai-gpt2-weights/extracted/124M/124M/model.ckpt.index  \n","  inflating: openai-gpt2-weights/extracted/124M/124M/model.ckpt.meta  \n","  inflating: openai-gpt2-weights/extracted/124M/124M/vocab.bpe  \n","  inflating: openai-gpt2-weights/extracted/1558M/1558M/checkpoint  \n","  inflating: openai-gpt2-weights/extracted/1558M/1558M/encoder.json  \n","  inflating: openai-gpt2-weights/extracted/1558M/1558M/hparams.json  \n","  inflating: openai-gpt2-weights/extracted/1558M/1558M/model.ckpt.data-00000-of-00001  \n","  inflating: openai-gpt2-weights/extracted/1558M/1558M/model.ckpt.index  \n","  inflating: openai-gpt2-weights/extracted/1558M/1558M/model.ckpt.meta  \n","  inflating: openai-gpt2-weights/extracted/1558M/1558M/vocab.bpe  \n","  inflating: openai-gpt2-weights/extracted/345M/345M/checkpoint  \n","  inflating: openai-gpt2-weights/extracted/345M/345M/encoder.json  \n","  inflating: openai-gpt2-weights/extracted/345M/345M/hparams.json  \n","  inflating: openai-gpt2-weights/extracted/345M/345M/model.ckpt.data-00000-of-00001  \n","  inflating: openai-gpt2-weights/extracted/345M/345M/model.ckpt.index  \n","  inflating: openai-gpt2-weights/extracted/345M/345M/model.ckpt.meta  \n","  inflating: openai-gpt2-weights/extracted/345M/345M/vocab.bpe  \n","  inflating: openai-gpt2-weights/extracted/355M/355M/checkpoint  \n","  inflating: openai-gpt2-weights/extracted/355M/355M/encoder.json  \n","  inflating: openai-gpt2-weights/extracted/355M/355M/hparams.json  \n","  inflating: openai-gpt2-weights/extracted/355M/355M/model.ckpt.data-00000-of-00001  \n","  inflating: openai-gpt2-weights/extracted/355M/355M/model.ckpt.index  \n","  inflating: openai-gpt2-weights/extracted/355M/355M/model.ckpt.meta  \n","  inflating: openai-gpt2-weights/extracted/355M/355M/vocab.bpe  \n","  inflating: openai-gpt2-weights/extracted/774M/774M/checkpoint  \n","  inflating: openai-gpt2-weights/extracted/774M/774M/encoder.json  \n","  inflating: openai-gpt2-weights/extracted/774M/774M/hparams.json  \n","  inflating: openai-gpt2-weights/extracted/774M/774M/model.ckpt.data-00000-of-00001  \n","  inflating: openai-gpt2-weights/extracted/774M/774M/model.ckpt.index  \n","  inflating: openai-gpt2-weights/extracted/774M/774M/model.ckpt.meta  \n","  inflating: openai-gpt2-weights/extracted/774M/774M/vocab.bpe  \n","  inflating: openai-gpt2-weights/extracted/src/encoder.py  \n","  inflating: openai-gpt2-weights/extracted/src/generate_unconditional_samples.py  \n","  inflating: openai-gpt2-weights/extracted/src/interactive_conditional_samples.py  \n","  inflating: openai-gpt2-weights/extracted/src/model.py  \n","  inflating: openai-gpt2-weights/extracted/src/sample.py  \n"]}]},{"cell_type":"code","source":["# dict keys:\n","# - wte - token emb\n","# - wpe - positional emb\n","# - blocks\n","# -- transfomer\n","#    - h0 (0th tx block) - (similar for h1,h2,...h11)\n","#      - attn\n","#        - c_attn (fused Q,K,V weight matrix)\n","#          - w - weights\n","#          - b - bias\n","#        - c_proj (this is the out projection layer in a tx block)\n","#          - w weights\n","#      - mlp\n","#        - c_fc (fully connected layer)\n","#          - w - weights\n","#          - b - bias\n","#        - c_proj (projection layer)\n","#          - weights\n","#          - bias\n","#      - ln_1\n","#        - g scale\n","#        - b shift\n","#      - ln_2\n","#        - g scale\n","#        - b shift\n","# - g - final norm scale (trainable param)\n","# - b - final norm shift (trainable param)\n","# source: https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/01_main-chapter-code/gpt_generate.py#L122\n","import numpy as np\n","def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n","    # Initialize parameters dictionary with empty blocks for each layer\n","    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n","\n","    # Iterate over each variable in the checkpoint\n","    for name, _ in tf.train.list_variables(ckpt_path):\n","        # Load the variable and remove singleton dimensions\n","        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n","\n","        # Process the variable name to extract relevant parts\n","        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n","\n","        # Identify the target dictionary for the variable\n","        target_dict = params\n","        if variable_name_parts[0].startswith(\"h\"):\n","            layer_number = int(variable_name_parts[0][1:])\n","            target_dict = params[\"blocks\"][layer_number]\n","\n","        # Recursively access or create nested dictionaries\n","        for key in variable_name_parts[1:-1]:\n","            target_dict = target_dict.setdefault(key, {})\n","\n","        # Assign the variable array to the last key\n","        last_key = variable_name_parts[-1]\n","        target_dict[last_key] = variable_array\n","\n","    return params\n","\n","def assign(left, right):\n","  if left.shape != right.shape:\n","      raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n","  return torch.nn.Parameter(torch.tensor(right))\n","\n","def load_weights_into_gpt(gpt, params):\n","    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params[\"wpe\"])\n","    gpt.token_emb.weight = assign(gpt.token_emb.weight, params[\"wte\"])\n","\n","    for b in range(len(params[\"blocks\"])):\n","        q_w, k_w, v_w = np.split(\n","            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n","        gpt.trf_blocks[b].attn.W_query.weight = assign(\n","            gpt.trf_blocks[b].attn.W_query.weight, q_w.T)\n","        gpt.trf_blocks[b].attn.W_key.weight = assign(\n","            gpt.trf_blocks[b].attn.W_key.weight, k_w.T)\n","        gpt.trf_blocks[b].attn.W_value.weight = assign(\n","            gpt.trf_blocks[b].attn.W_value.weight, v_w.T)\n","\n","        q_b, k_b, v_b = np.split(\n","            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n","        gpt.trf_blocks[b].attn.W_query.bias = assign(\n","            gpt.trf_blocks[b].attn.W_query.bias, q_b)\n","        gpt.trf_blocks[b].attn.W_key.bias = assign(\n","            gpt.trf_blocks[b].attn.W_key.bias, k_b)\n","        gpt.trf_blocks[b].attn.W_value.bias = assign(\n","            gpt.trf_blocks[b].attn.W_value.bias, v_b)\n","\n","        gpt.trf_blocks[b].attn.out_proj.weight = assign(\n","            gpt.trf_blocks[b].attn.out_proj.weight,\n","            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n","        gpt.trf_blocks[b].attn.out_proj.bias = assign(\n","            gpt.trf_blocks[b].attn.out_proj.bias,\n","            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n","\n","        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n","            gpt.trf_blocks[b].ff.layers[0].weight,\n","            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n","        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n","            gpt.trf_blocks[b].ff.layers[0].bias,\n","            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n","        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n","            gpt.trf_blocks[b].ff.layers[2].weight,\n","            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n","        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n","            gpt.trf_blocks[b].ff.layers[2].bias,\n","            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n","\n","        gpt.trf_blocks[b].norm1.scale = assign(\n","            gpt.trf_blocks[b].norm1.scale,\n","            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n","        gpt.trf_blocks[b].norm1.shift = assign(\n","            gpt.trf_blocks[b].norm1.shift,\n","            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n","        gpt.trf_blocks[b].norm2.scale = assign(\n","            gpt.trf_blocks[b].norm2.scale,\n","            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n","        gpt.trf_blocks[b].norm2.shift = assign(\n","            gpt.trf_blocks[b].norm2.shift,\n","            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n","\n","    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n","    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n","    gpt.out_proj.weight = assign(gpt.out_proj.weight, params[\"wte\"])\n"],"metadata":{"id":"3ysh2qGUz93P","executionInfo":{"status":"ok","timestamp":1742357919087,"user_tz":420,"elapsed":190,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":90,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","import json\n","import os\n","model_dir = \"openai-gpt2-weights/extracted/124M/124M\"\n","tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n","settings = json.load(open(os.path.join(model_dir, \"hparams.json\")))\n","params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n","print(params.keys())\n","print(settings)\n","print(params[\"wte\"].shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6PRwi1TY0mvb","executionInfo":{"status":"ok","timestamp":1742358920479,"user_tz":420,"elapsed":4802,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"090e8acc-eb7a-448c-cb14-85eed5b18208"},"execution_count":93,"outputs":[{"output_type":"stream","name":"stdout","text":["dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n","{'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n","(50257, 768)\n"]}]},{"cell_type":"code","source":["# load gpt-2 model weights in the GPTModel class\n","MODEL = \"gpt2-small (124M)\"\n","INPUT_PROMPT = \"Every effort moves\"\n","\n","BASE_CONFIG = {\n","    \"vocab_size\": 50257,\n","    \"context_length\": 1024,\n","    \"drop_rate\": 0.0,\n","    \"qkv_bias\": False,\n","}\n","\n","model_configs = {\n","    \"gpt2-small (124M)\" : {\n","        \"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12\n","    },\n","    \"gpt2-medium (355M)\" : {\n","        \"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16\n","    },\n","}\n","\n","BASE_CONFIG.update(model_configs[MODEL])\n","\n","model_name = \"gpt2-small (124M)\"\n","NEW_GPT_CONFIG = BASE_CONFIG.copy()\n","NEW_GPT_CONFIG.update(model_configs[model_name])\n","NEW_GPT_CONFIG.update({\"context_length\":1024, \"qkv_bias\":True})\n","\n","model = GPTModel(NEW_GPT_CONFIG)\n","load_weights_into_gpt(model, params)\n","model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"0kAjzfesHHvg","executionInfo":{"status":"ok","timestamp":1742358997411,"user_tz":420,"elapsed":3296,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"0f267598-51e9-4c51-fdd3-44f95aa338ad"},"execution_count":94,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GPTModel(\n","  (token_emb): Embedding(50257, 768)\n","  (pos_emb): Embedding(1024, 768)\n","  (drop_emb): Dropout(p=0.0, inplace=False)\n","  (trf_blocks): Sequential(\n","    (0): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (1): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (2): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (3): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (4): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (5): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (6): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (7): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (8): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (9): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (10): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (11): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","  )\n","  (final_norm): LayerNorm()\n","  (out_proj): Linear(in_features=768, out_features=50257, bias=False)\n",")"]},"metadata":{},"execution_count":94}]},{"cell_type":"code","source":["# ! pip install tiktoken\n","import tiktoken\n","start_context = \"Every effort moves you\"\n","context_size = model.pos_emb.weight.shape[0]\n","print(\"context_size:\", context_size)\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","encoded = text_to_token_ids(start_context, tokenizer)\n","with torch.no_grad():\n","  token_ids_with_temp = generate(model=model, idx=encoded.to(device), max_new_tokens=50, context_size=context_size, temp=1.4,top_k=3)\n","\n","decoded_text = token_ids_to_text(token_ids_with_temp, tokenizer)\n","print(decoded_text.replace(\"\\n\", \" \"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3WFtVO4XLQDN","executionInfo":{"status":"ok","timestamp":1742359022892,"user_tz":420,"elapsed":13539,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"2edb7e56-adf1-49aa-d790-bd2e6199219a"},"execution_count":95,"outputs":[{"output_type":"stream","name":"stdout","text":["context_size: 1024\n","Every effort moves you to the next level. You can't always get to the point where you're going to be able to get to the next level, so there are some things you can't do that will help you. You have to do some work. You have\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"F4eafd8JKqqe"}},{"cell_type":"code","source":["# before finetuning, let's check if the model already can detect spams\n","text = (\n","    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n","    \" 'You are a winner you have been specially selected to receive a $1000 cash award.'\"\n",")\n","token_ids_with_temp = generate(model=model, idx=text_to_token_ids(text, tokenizer), max_new_tokens=20, context_size=context_size, temp=1.0,top_k=3)\n","decoded_text = token_ids_to_text(token_ids_with_temp, tokenizer)\n","print(decoded_text.replace(\"\\n\", \" \"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"98iznhWsJ_RL","executionInfo":{"status":"ok","timestamp":1742359028996,"user_tz":420,"elapsed":6103,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"cebd67f8-d798-48b2-ef15-fc92451081db"},"execution_count":96,"outputs":[{"output_type":"stream","name":"stdout","text":["Is the following text 'spam'? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive a $1000 cash award.'  If you have not been chosen by us to win a $1000 cash award, you can\n"]}]},{"cell_type":"code","source":["# note: not all layers need fine tuning. the lower layers (wte, wpe) already capture the semantics of the language. so we can selectively tune only the last layers - the final tx block, final layer norm, classification head\n","# note:for the classification head dim (token, 2), we should look at the last token, since this is the only token which has attended (causal attention) all the previous tokens.\n","print(model)\n","\n","# first freeze the model\n","for param in model.parameters():\n","  param.requires_grad = False\n","\n","num_classes = 2\n","model.out_proj = nn.Linear(in_features=BASE_CONFIG[\"emb_dim\"], out_features=num_classes) # note: this already sets requires_grad to true by default\n","for param in model.trf_blocks[-1].parameters():\n","  param.requires_grad = True\n","\n","for param in model.final_norm.parameters():\n","  param.requires_grad = True\n","\n","for param in model.out_proj.parameters():\n","  param.requires_grad = True\n","\n","print(model)\n","# now we aready to perform the finetuning for classification task -\n","# we have the data loader setup for the finetuning dataset and the GPTModel\n","# is setup with the last layers as trainable and we have modified the final out head\n","# to a classification head of n classes."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"TmFCsWDEPYYP","executionInfo":{"status":"ok","timestamp":1742359028997,"user_tz":420,"elapsed":18,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"23657f15-b8dd-4e75-b5dc-ad37c375e647"},"execution_count":97,"outputs":[{"output_type":"stream","name":"stdout","text":["GPTModel(\n","  (token_emb): Embedding(50257, 768)\n","  (pos_emb): Embedding(1024, 768)\n","  (drop_emb): Dropout(p=0.0, inplace=False)\n","  (trf_blocks): Sequential(\n","    (0): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (1): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (2): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (3): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (4): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (5): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (6): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (7): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (8): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (9): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (10): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (11): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","  )\n","  (final_norm): LayerNorm()\n","  (out_proj): Linear(in_features=768, out_features=50257, bias=False)\n",")\n","GPTModel(\n","  (token_emb): Embedding(50257, 768)\n","  (pos_emb): Embedding(1024, 768)\n","  (drop_emb): Dropout(p=0.0, inplace=False)\n","  (trf_blocks): Sequential(\n","    (0): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (1): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (2): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (3): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (4): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (5): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (6): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (7): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (8): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (9): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (10): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (11): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","  )\n","  (final_norm): LayerNorm()\n","  (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",")\n"]}]},{"cell_type":"code","source":["# Chapter : calculate classification loss\n","text = tokenizer.encode(\"Do you have time\")\n","print(text)\n","inputs = torch.tensor(text).unsqueeze(0)\n","print(inputs.shape)\n","\n","with torch.no_grad():\n","  output = model(inputs)\n","\n","print(output.shape)\n","print(\"output:\", output)\n","print(\"last output token:\", output[:,-1,:])\n","\n","\n","probas = torch.softmax(output[:,-1,:], dim=-1)\n","print(\"softmax probas:\", probas)\n","label = torch.argmax(probas)\n","print(\"label:\", label)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"baeo8U4IY_Ls","executionInfo":{"status":"ok","timestamp":1742359029070,"user_tz":420,"elapsed":75,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"a35cc96e-4ecf-4b46-fef3-f440b12d0c94"},"execution_count":98,"outputs":[{"output_type":"stream","name":"stdout","text":["[5211, 345, 423, 640]\n","torch.Size([1, 4])\n","torch.Size([1, 4, 2])\n","output: tensor([[[0.3665, 1.8888],\n","         [3.1916, 9.4666],\n","         [2.8307, 8.6472],\n","         [2.2398, 6.1715]]])\n","last output token: tensor([[2.2398, 6.1715]])\n","softmax probas: tensor([[0.0192, 0.9808]])\n","label: tensor(1)\n"]}]},{"cell_type":"code","source":["\n","def calc_loss_batch_finetune_classification(input_batch, target, model, device):\n","  input_batch = input_batch.to(device)\n","  target = target.to(device)\n","  logits = model(input_batch)\n","  loss = torch.nn.functional.cross_entropy(logits[:,-1,:], target)\n","  return loss\n","\n","def calc_loss_loader_finetune_classification(data_loader, model, device, num_batches=None):\n","  model.eval()\n","  total_loss = 0\n","  if len(data_loader) == 0:\n","    return float(\"nan\")\n","  elif num_batches is None:\n","    num_batches = len(data_loader)\n","  else:\n","    num_batches = min(num_batches, len(data_loader))\n","\n","  for i , (input_batch, target) in enumerate(data_loader):\n","    print(\"processing input batch\", i)\n","    if i >= num_batches:\n","      break\n","\n","    input_batch = input_batch.to(device)\n","    target = target.to(device)\n","    with torch.no_grad():\n","      loss = calc_loss_batch_finetune_classification(input_batch, target, model, device)\n","      total_loss += loss.item()\n","\n","  return total_loss / num_batches\n","\n","def calc_accuracy_loader_finetune_classification(data_loader, model, device, num_batches=None):\n","  model.eval()\n","  correct_predictions, num_examples = 0, 0\n","  if num_batches is None:\n","    num_batches = len(data_loader)\n","  else:\n","    num_batches = min(num_batches, len(data_loader))\n","\n","  for i , (input_batch, target) in enumerate(data_loader):\n","    print(\"processing input batch\", i)\n","    if i >= num_batches:\n","      break\n","\n","    input_batch = input_batch.to(device)\n","    target = target.to(device)\n","    with torch.no_grad():\n","      logits = model(input_batch)\n","\n","    predicted_labels = torch.argmax(logits[:,-1,:], dim=-1)\n","    correct_predictions += (predicted_labels == target).sum().item()\n","    num_examples += predicted_labels.shape[0]\n","\n","  accuracy = correct_predictions / num_examples\n","  return accuracy\n","\n","def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n","  model.eval()\n","  with torch.no_grad():\n","    train_loss = calc_loss_loader_finetune_classification(train_loader, model, device, num_batches=eval_iter)\n","    val_loss = calc_loss_loader_finetune_classification(val_loader, model, device, num_batches=eval_iter)\n","  model.train()\n","  return train_loss, val_loss"],"metadata":{"id":"eHSrv5vQe-Th","executionInfo":{"status":"ok","timestamp":1742359031395,"user_tz":420,"elapsed":14,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":99,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","torch.manual_seed(123)\n","print(\"calc_accuracy_loader for train_loader, len loader:\", len(train_loader))\n","train_accuracy = calc_accuracy_loader_finetune_classification(train_loader, model, device, num_batches=5)\n","print(\"calc_accuracy_loader for val_loader, len loader:\", len(val_loader))\n","val_accuracy = calc_accuracy_loader_finetune_classification(val_loader, model, device, num_batches=5)\n","print(\"calc_accuracy_loader for test_loader, len loader:\", len(test_loader))\n","test_accuracy = calc_accuracy_loader_finetune_classification(test_loader, model, device, num_batches=5)\n","\n","print(\"train accuracy:\", train_accuracy)\n","print(\"val accuracy:\", val_accuracy)\n","print(\"test accuracy:\", test_accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"cK51pl4M15AY","executionInfo":{"status":"ok","timestamp":1742359124665,"user_tz":420,"elapsed":50395,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"259d8e15-d294-4c01-b406-7033278d3f11"},"execution_count":100,"outputs":[{"output_type":"stream","name":"stdout","text":["calc_accuracy_loader for train_loader, len loader: 130\n","processing input batch 0\n","processing input batch 1\n","processing input batch 2\n","processing input batch 3\n","processing input batch 4\n","processing input batch 5\n","calc_accuracy_loader for val_loader, len loader: 19\n","processing input batch 0\n","processing input batch 1\n","processing input batch 2\n","processing input batch 3\n","processing input batch 4\n","processing input batch 5\n","calc_accuracy_loader for test_loader, len loader: 38\n","processing input batch 0\n","processing input batch 1\n","processing input batch 2\n","processing input batch 3\n","processing input batch 4\n","processing input batch 5\n","train accuracy: 0.4\n","val accuracy: 0.525\n","test accuracy: 0.45\n"]}]},{"cell_type":"code","source":["train_loss = calc_loss_loader_finetune_classification(train_loader, model, device, num_batches=2)\n","val_loss = calc_loss_loader_finetune_classification(val_loader, model, device, num_batches=2)\n","test_loss = calc_loss_loader_finetune_classification(test_loader, model, device, num_batches=2)\n","print(\"train loss:\", train_loss)\n","print(\"val loss:\", val_loss)\n","print(\"test loss:\", test_loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_dv53g802oCa","executionInfo":{"status":"ok","timestamp":1742359152575,"user_tz":420,"elapsed":27894,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"bbd8b567-b784-4f2e-aedc-96f2bc6863b7"},"execution_count":101,"outputs":[{"output_type":"stream","name":"stdout","text":["processing input batch 0\n","processing input batch 1\n","processing input batch 2\n","processing input batch 0\n","processing input batch 1\n","processing input batch 2\n","processing input batch 0\n","processing input batch 1\n","processing input batch 2\n","train loss: 2.0939974784851074\n","val loss: 1.5012441873550415\n","test loss: 1.318569004535675\n"]}]},{"cell_type":"code","source":["def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter):\n","  train_loss, val_loss, train_accs, val_accs = [], [], [], []\n","  example_seen, global_step = 0, 1\n","\n","  for epoch in range(num_epochs):\n","    print(\"start epoch:\", epoch)\n","    model.train()\n","    for i, (input_batch, target) in enumerate(train_loader):\n","      print(\"processing input batch: \", i, \" batch shape:\", input_batch.shape)\n","      optimizer.zero_grad()\n","      input_batch = input_batch.to(device)\n","      target = target.to(device)\n","      loss = calc_loss_batch(input_batch, target, model, device)\n","      loss.backward()\n","      optimizer.step()\n","      example_seen += input_batch.shape[0]\n","      global_step += 1\n","\n","      if global_step % eval_freq == 0:\n","        train_loss, val_loss = evaluate_model(model, train_loader=train_loader, val_loader=val_loader, device=device, eval_iter=eval_iter)\n","        print(\"epoch:\", epoch, \"global step:\", global_step, \"train_loss:\", train_loss, \"val_loss\", val_loss)\n","\n","    train_accuracy = calc_accuracy_loader_finetune_classification(train_loader, model, device, num_batches=eval_iter)\n","    val_accuracy = calc_accuracy_loader_finetune_classification(val_loader, model, device, num_batches=eval_iter)\n","    print(\"at epoch:\", epoch, \"train_accuracy:\", train_accuracy, \"val_accuracy:\", val_accuracy)\n","    train_accs.append(train_accuracy)\n","    val_accs.append(val_accuracy)\n","\n","  return train_loss, val_loss, train_accs, val_accs"],"metadata":{"id":"YE6Ycrq69eC3","executionInfo":{"status":"ok","timestamp":1742359152588,"user_tz":420,"elapsed":12,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":102,"outputs":[]},{"cell_type":"code","source":["import time\n","\n","num_workers = 0\n","batch_size = 8\n","train_loader = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers,drop_last=True)\n","val_loader = DataLoader(dataset=val_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers,drop_last=False)\n","test_loader = DataLoader(dataset=test_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers,drop_last=False)\n","\n","# note: this runs the actual fine tuning, uncomment the code to finetune\n","# start = time.time()\n","# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.1)\n","# num_epochs = 5 # tweak for higher prediction accuracy\n","# train_loss, val_loss, train_accs, val_accs = train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq=50, eval_iter=5)\n","# end = time.time()\n","# execution_time = end - start\n","# print(\"execution_time:\", execution_time)"],"metadata":{"collapsed":true,"id":"0t9_ObbiA4Ch","executionInfo":{"status":"ok","timestamp":1742359152595,"user_tz":420,"elapsed":6,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":103,"outputs":[]},{"cell_type":"code","source":["def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):\n","  model.eval()\n","  input_ids = tokenizer.encode(text)\n","  supported_context_length = model.pos_emb.weight.shape[0]\n","  if max_length is None:\n","    max_length = supported_context_length\n","\n","  input_ids = input_ids[:min(max_length, supported_context_length)]\n","  input_ids = input_ids + [pad_token_id] * (max_length - len(input_ids))\n","  input_ids = torch.tensor(input_ids, device=device).unsqueeze(0) # add batch dimension using unsqueeze\n","  with torch.no_grad():\n","    logits = model(input_ids)[:,-1,:]\n","    print(\"logits:\", logits)\n","\n","  predicted_label = torch.argmax(logits, dim=-1).item()\n","  print(\"precited label\", predicted_label)\n","  return \"spam\" if predicted_label == 1 else \"not spam\""],"metadata":{"id":"Vxr8yWfxCmHN","executionInfo":{"status":"ok","timestamp":1742359152627,"user_tz":420,"elapsed":24,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":104,"outputs":[]},{"cell_type":"code","source":["output = classify_review(text=\"you have won a lottery!\", model=model, tokenizer=tokenizer, device=device, max_length=1024, pad_token_id=50256)\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JGGrdJq0JDL2","executionInfo":{"status":"ok","timestamp":1742359161624,"user_tz":420,"elapsed":8964,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"d6252916-50b1-48dd-a132-1eda096e0f2c"},"execution_count":105,"outputs":[{"output_type":"stream","name":"stdout","text":["logits: tensor([[2.7174, 7.7112]])\n","precited label 1\n","spam\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"ZTDbeud4JC4c"}},{"cell_type":"code","source":["# Chapter: Instruction finetuning\n","# 1. download dataset\n","# 2. create dataset class for batch\n","# 3. create data loader\n","# 4. load pretrained model\n","# 5. instruction finetune\n","# 6. inspect loss\n","# 7. extract response\n","# 8. qualitative eval\n","# 9. score response\n","\n","! wget https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch07/01_main-chapter-code/instruction-data.json\n","! ls instruction-data.json\n","# dataset contains tuples (instruction, input, output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ILdLWbgwLJoL","executionInfo":{"status":"ok","timestamp":1742359195686,"user_tz":420,"elapsed":614,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"1fe439e6-0d7b-4f75-94eb-dad019329df7"},"execution_count":106,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-03-19 04:39:55--  https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch07/01_main-chapter-code/instruction-data.json\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 203524 (199K) [text/plain]\n","Saving to: ‘instruction-data.json’\n","\n","instruction-data.js 100%[===================>] 198.75K  --.-KB/s    in 0.04s   \n","\n","2025-03-19 04:39:55 (5.51 MB/s) - ‘instruction-data.json’ saved [203524/203524]\n","\n","instruction-data.json\n"]}]},{"cell_type":"code","source":["# prompt: code to read the json instruction-data.json and print the first 10 lines\n","\n","import json\n","\n","with open('instruction-data.json', 'r') as f:\n","    data = json.load(f)\n","\n","print(len(data))\n","print(data[0])\n","print(data[0]['instruction'])\n","\n","# there is a specific way we need to prompt the llm. e.g. standford alpaca, phi-3\n","\n","def format_input(entry):\n","  # stanford alpaca prompt\n","  instruction_text = (\n","      f\"Below is an instruction that describes a task. \"\n","      f\"Write a response that appropriately completes the request.\"\n","      f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n","  )\n","\n","  input_text =f\"\\n\\n### Input:\\n{entry['input']}\" if entry['input'] else \"\"\n","\n","  return instruction_text + input_text\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q9wwiIsBCZm0","executionInfo":{"status":"ok","timestamp":1742359195704,"user_tz":420,"elapsed":8,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"0838cad6-4a43-49a0-df60-3ec3f8a8b808"},"execution_count":107,"outputs":[{"output_type":"stream","name":"stdout","text":["1100\n","{'instruction': 'Evaluate the following phrase by transforming it into the spelling given.', 'input': 'freind --> friend', 'output': 'The spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\".'}\n","Evaluate the following phrase by transforming it into the spelling given.\n"]}]},{"cell_type":"code","source":["model_input = format_input(data[999])\n","desired_response = f\"\\n\\n### Response:\\n{data[999]['output']}\"\n","\n","print(model_input)\n","print(desired_response)\n","\n","\n","# split the dataset into test, train and val data\n","train_split = int(len(data) * .85)\n","test_split = int(len(data) * .1)\n","val_split = len(data) - train_split - test_split\n","\n","train_data = data[:train_split]\n","test_data = data[train_split:train_split + test_split]\n","val_data = data[train_split + test_split:]\n","\n","print(len(train_data), len(val_data), len(test_data))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LifshyZ2C7w-","executionInfo":{"status":"ok","timestamp":1742359195719,"user_tz":420,"elapsed":14,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"c67a2b43-c7ec-47f3-fa33-228503c4966b"},"execution_count":108,"outputs":[{"output_type":"stream","name":"stdout","text":["Below is an instruction that describes a task. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","What is an antonym of 'complicated'?\n","\n","\n","### Response:\n","An antonym of 'complicated' is 'simple'.\n","935 55 110\n"]}]},{"cell_type":"code","source":["# we tokenize the input and pad with endoftext token to make all input equal by using the <|endoftext|> token.\n","# For target tensor (corresponding to the input) we shift the input by 1 and pad an <|endoftext|> and extra tokens (e.g -100 is the default ignore_index for pytorch).\n","# because we still want to do the next token prediction. we also want to exclude the token from calculating the loss\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","class InstructionDataset(Dataset):\n","  def __init__(self, data, tokenizer):\n","    self.data = data\n","    self.encoded_text = []\n","    for entry in data:\n","      instruction_input = format_input(entry)\n","      response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n","      full_text = instruction_input + response_text\n","      self.encoded_text.append(tokenizer.encode(full_text))\n","\n","  def __getitem__(self, idx):\n","    return self.encoded_text[idx]\n","  def __len__(self):\n","    return len(self.encoded_text)\n","\n","def custom_collate_fn(batch, pad_token_id=50256, ignore_index=-100, allowed_max_len=None, device=\"cpu\"):\n","  max_len = max(len(item)+1 for item in batch) # add +1 to the longest seq.\n","  input_list = []\n","  target_list = []\n","  for item in batch:\n","    new_item = item.copy()\n","    new_item += [pad_token_id] # this will be useful when creating the target token\n","    padded = (\n","        new_item +\n","        [pad_token_id] * (max_len - len(new_item))\n","    )\n","\n","    inputs = torch.tensor(padded[:-1])\n","    targets = torch.tensor(padded[1:])\n","\n","    mask = targets == pad_token_id\n","    indices = torch.nonzero(mask).squeeze() # find indices which satisfies the mask\n","    if indices.numel() > 1:\n","      targets[indices[1:]] = ignore_index # skip the first suffix 50256, and replace the rest with -100\n","\n","    if allowed_max_len is not None:\n","      if len(inputs) > allowed_max_len:\n","        inputs = inputs[:allowed_max_len]\n","        targets = targets[:allowed_max_len]\n","\n","    input_list.append(inputs)\n","    target_list.append(targets)\n","\n","  inputs_tensor = torch.stack(input_list)\n","  targets_tensor = torch.stack(target_list)\n","  return inputs_tensor.to(device), targets_tensor.to(device)\n","\n","\n","# note: explanation of ignore_index? adding a training example, whose correspondig target is -100, has no effect on the loss, that is the default ignore index value for pytorch\n","# collate fn offers the advantage of performing the device transfer process in the background, outside the training loop"],"metadata":{"id":"0H7tMqDeE7aY","executionInfo":{"status":"ok","timestamp":1742359195761,"user_tz":420,"elapsed":41,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":109,"outputs":[]},{"cell_type":"code","source":["input1 = [0,1,2,3,4]\n","input2 = [5,6]\n","input3 = [7,8,9]\n","batch = [input1, input2, input3]\n","print(batch)\n","\n","inputs, targets = custom_collate_fn(batch)\n","print(inputs)\n","print(targets)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8CVBaxDnJxX9","executionInfo":{"status":"ok","timestamp":1742359195764,"user_tz":420,"elapsed":3,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"50c416b7-7941-4506-b273-ab4ed26f18c0"},"execution_count":110,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0, 1, 2, 3, 4], [5, 6], [7, 8, 9]]\n","tensor([[    0,     1,     2,     3,     4],\n","        [    5,     6, 50256, 50256, 50256],\n","        [    7,     8,     9, 50256, 50256]])\n","tensor([[    1,     2,     3,     4, 50256],\n","        [    6, 50256,  -100,  -100,  -100],\n","        [    8,     9, 50256,  -100,  -100]])\n"]}]},{"cell_type":"code","source":["# we use partial to create a version of the function with defaults pre populated as reqd.\n","from functools import partial\n","from torch.utils.data import dataloader\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","customized_collate_fn = partial(custom_collate_fn, pad_token_id=50256, ignore_index=-100, allowed_max_len=1024, device=device)\n","\n","torch.manual_seed(123)\n","num_workers = 0\n","batch_size = 8\n","train_ds = InstructionDataset(train_data, tokenizer)\n","val_ds = InstructionDataset(val_data, tokenizer)\n","test_ds = InstructionDataset(test_data, tokenizer)\n","print(len(train_ds))\n","print(len(val_ds))\n","print(len(test_ds))\n","\n","\n","train_loader = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=num_workers, collate_fn=customized_collate_fn)\n","val_loader = DataLoader(dataset=val_ds, batch_size=batch_size, shuffle=True, drop_last=False, num_workers=num_workers, collate_fn=customized_collate_fn)\n","test_loader = DataLoader(dataset=test_ds, batch_size=batch_size, shuffle=True, drop_last=False, num_workers=num_workers, collate_fn=customized_collate_fn)\n","for input, target in train_loader:\n","  print(input.shape, target.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"tPcRlSuBJ9Bp","executionInfo":{"status":"ok","timestamp":1742359195918,"user_tz":420,"elapsed":154,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"9103b182-2dfd-4aff-ae7d-2b67129cff0c"},"execution_count":111,"outputs":[{"output_type":"stream","name":"stdout","text":["935\n","55\n","110\n","torch.Size([8, 61]) torch.Size([8, 61])\n","torch.Size([8, 76]) torch.Size([8, 76])\n","torch.Size([8, 73]) torch.Size([8, 73])\n","torch.Size([8, 68]) torch.Size([8, 68])\n","torch.Size([8, 65]) torch.Size([8, 65])\n","torch.Size([8, 72]) torch.Size([8, 72])\n","torch.Size([8, 80]) torch.Size([8, 80])\n","torch.Size([8, 67]) torch.Size([8, 67])\n","torch.Size([8, 62]) torch.Size([8, 62])\n","torch.Size([8, 75]) torch.Size([8, 75])\n","torch.Size([8, 62]) torch.Size([8, 62])\n","torch.Size([8, 68]) torch.Size([8, 68])\n","torch.Size([8, 67]) torch.Size([8, 67])\n","torch.Size([8, 77]) torch.Size([8, 77])\n","torch.Size([8, 69]) torch.Size([8, 69])\n","torch.Size([8, 79]) torch.Size([8, 79])\n","torch.Size([8, 71]) torch.Size([8, 71])\n","torch.Size([8, 66]) torch.Size([8, 66])\n","torch.Size([8, 83]) torch.Size([8, 83])\n","torch.Size([8, 68]) torch.Size([8, 68])\n","torch.Size([8, 80]) torch.Size([8, 80])\n","torch.Size([8, 71]) torch.Size([8, 71])\n","torch.Size([8, 69]) torch.Size([8, 69])\n","torch.Size([8, 65]) torch.Size([8, 65])\n","torch.Size([8, 68]) torch.Size([8, 68])\n","torch.Size([8, 60]) torch.Size([8, 60])\n","torch.Size([8, 59]) torch.Size([8, 59])\n","torch.Size([8, 69]) torch.Size([8, 69])\n","torch.Size([8, 63]) torch.Size([8, 63])\n","torch.Size([8, 65]) torch.Size([8, 65])\n","torch.Size([8, 76]) torch.Size([8, 76])\n","torch.Size([8, 66]) torch.Size([8, 66])\n","torch.Size([8, 71]) torch.Size([8, 71])\n","torch.Size([8, 91]) torch.Size([8, 91])\n","torch.Size([8, 65]) torch.Size([8, 65])\n","torch.Size([8, 64]) torch.Size([8, 64])\n","torch.Size([8, 67]) torch.Size([8, 67])\n","torch.Size([8, 66]) torch.Size([8, 66])\n","torch.Size([8, 64]) torch.Size([8, 64])\n","torch.Size([8, 65]) torch.Size([8, 65])\n","torch.Size([8, 75]) torch.Size([8, 75])\n","torch.Size([8, 89]) torch.Size([8, 89])\n","torch.Size([8, 59]) torch.Size([8, 59])\n","torch.Size([8, 88]) torch.Size([8, 88])\n","torch.Size([8, 83]) torch.Size([8, 83])\n","torch.Size([8, 83]) torch.Size([8, 83])\n","torch.Size([8, 70]) torch.Size([8, 70])\n","torch.Size([8, 65]) torch.Size([8, 65])\n","torch.Size([8, 74]) torch.Size([8, 74])\n","torch.Size([8, 76]) torch.Size([8, 76])\n","torch.Size([8, 67]) torch.Size([8, 67])\n","torch.Size([8, 75]) torch.Size([8, 75])\n","torch.Size([8, 83]) torch.Size([8, 83])\n","torch.Size([8, 69]) torch.Size([8, 69])\n","torch.Size([8, 67]) torch.Size([8, 67])\n","torch.Size([8, 60]) torch.Size([8, 60])\n","torch.Size([8, 60]) torch.Size([8, 60])\n","torch.Size([8, 66]) torch.Size([8, 66])\n","torch.Size([8, 80]) torch.Size([8, 80])\n","torch.Size([8, 71]) torch.Size([8, 71])\n","torch.Size([8, 61]) torch.Size([8, 61])\n","torch.Size([8, 58]) torch.Size([8, 58])\n","torch.Size([8, 71]) torch.Size([8, 71])\n","torch.Size([8, 67]) torch.Size([8, 67])\n","torch.Size([8, 68]) torch.Size([8, 68])\n","torch.Size([8, 63]) torch.Size([8, 63])\n","torch.Size([8, 87]) torch.Size([8, 87])\n","torch.Size([8, 68]) torch.Size([8, 68])\n","torch.Size([8, 64]) torch.Size([8, 64])\n","torch.Size([8, 68]) torch.Size([8, 68])\n","torch.Size([8, 71]) torch.Size([8, 71])\n","torch.Size([8, 68]) torch.Size([8, 68])\n","torch.Size([8, 71]) torch.Size([8, 71])\n","torch.Size([8, 61]) torch.Size([8, 61])\n","torch.Size([8, 65]) torch.Size([8, 65])\n","torch.Size([8, 67]) torch.Size([8, 67])\n","torch.Size([8, 65]) torch.Size([8, 65])\n","torch.Size([8, 64]) torch.Size([8, 64])\n","torch.Size([8, 60]) torch.Size([8, 60])\n","torch.Size([8, 72]) torch.Size([8, 72])\n","torch.Size([8, 64]) torch.Size([8, 64])\n","torch.Size([8, 70]) torch.Size([8, 70])\n","torch.Size([8, 57]) torch.Size([8, 57])\n","torch.Size([8, 72]) torch.Size([8, 72])\n","torch.Size([8, 64]) torch.Size([8, 64])\n","torch.Size([8, 68]) torch.Size([8, 68])\n","torch.Size([8, 62]) torch.Size([8, 62])\n","torch.Size([8, 74]) torch.Size([8, 74])\n","torch.Size([8, 80]) torch.Size([8, 80])\n","torch.Size([8, 68]) torch.Size([8, 68])\n","torch.Size([8, 70]) torch.Size([8, 70])\n","torch.Size([8, 91]) torch.Size([8, 91])\n","torch.Size([8, 61]) torch.Size([8, 61])\n","torch.Size([8, 66]) torch.Size([8, 66])\n","torch.Size([8, 80]) torch.Size([8, 80])\n","torch.Size([8, 81]) torch.Size([8, 81])\n","torch.Size([8, 74]) torch.Size([8, 74])\n","torch.Size([8, 82]) torch.Size([8, 82])\n","torch.Size([8, 63]) torch.Size([8, 63])\n","torch.Size([8, 83]) torch.Size([8, 83])\n","torch.Size([8, 68]) torch.Size([8, 68])\n","torch.Size([8, 67]) torch.Size([8, 67])\n","torch.Size([8, 77]) torch.Size([8, 77])\n","torch.Size([8, 91]) torch.Size([8, 91])\n","torch.Size([8, 64]) torch.Size([8, 64])\n","torch.Size([8, 61]) torch.Size([8, 61])\n","torch.Size([8, 75]) torch.Size([8, 75])\n","torch.Size([8, 64]) torch.Size([8, 64])\n","torch.Size([8, 66]) torch.Size([8, 66])\n","torch.Size([8, 78]) torch.Size([8, 78])\n","torch.Size([8, 66]) torch.Size([8, 66])\n","torch.Size([8, 64]) torch.Size([8, 64])\n","torch.Size([8, 83]) torch.Size([8, 83])\n","torch.Size([8, 66]) torch.Size([8, 66])\n","torch.Size([8, 74]) torch.Size([8, 74])\n","torch.Size([8, 69]) torch.Size([8, 69])\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","import json\n","import os\n","# model_dir = \"openai-gpt2-weights/extracted/355M/355M\" # we are reverting to using 124M model, since the 355 seems to cause OOM kill for the allocated resources (~12.7G ram) for this notebook.\n","model_dir = \"openai-gpt2-weights/extracted/124M/124M\"\n","tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n","settings = json.load(open(os.path.join(model_dir, \"hparams.json\")))\n","params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n","print(params.keys())\n","print(settings)\n","print(params[\"wte\"].shape)\n","\n","# MODEL = \"gpt2-medium (355M)\"\n","MODEL = \"gpt2-small (124M)\"\n","\n","BASE_CONFIG = {\n","    \"vocab_size\": 50257,\n","    \"context_length\": 1024,\n","    \"drop_rate\": 0.0,\n","    \"qkv_bias\": False,\n","}\n","\n","model_configs = {\n","    \"gpt2-small (124M)\" : {\n","        \"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12\n","    },\n","    \"gpt2-medium (355M)\" : {\n","        \"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16\n","    },\n","}\n","\n","BASE_CONFIG.update(model_configs[MODEL])\n","\n","model_name = \"gpt2-small (124M)\"\n","NEW_GPT_CONFIG = BASE_CONFIG.copy()\n","NEW_GPT_CONFIG.update(model_configs[MODEL])\n","NEW_GPT_CONFIG.update({\"context_length\":1024, \"qkv_bias\":True})\n","\n","model = GPTModel(NEW_GPT_CONFIG)\n","load_weights_into_gpt(model, params)\n","model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"_U8k5g1NNVCw","executionInfo":{"status":"ok","timestamp":1742359199282,"user_tz":420,"elapsed":3364,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"4363e1d7-b359-42e0-9f78-de6215623033"},"execution_count":112,"outputs":[{"output_type":"stream","name":"stdout","text":["dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n","{'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n","(50257, 768)\n"]},{"output_type":"execute_result","data":{"text/plain":["GPTModel(\n","  (token_emb): Embedding(50257, 768)\n","  (pos_emb): Embedding(1024, 768)\n","  (drop_emb): Dropout(p=0.0, inplace=False)\n","  (trf_blocks): Sequential(\n","    (0): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (1): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (2): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (3): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (4): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (5): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (6): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (7): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (8): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (9): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (10): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (11): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","  )\n","  (final_norm): LayerNorm()\n","  (out_proj): Linear(in_features=768, out_features=50257, bias=False)\n",")"]},"metadata":{},"execution_count":112}]},{"cell_type":"code","source":["torch.manual_seed(123)\n","input_text = format_input(val_data[0])\n","print(input_text)\n","# model output when not finetuned\n","token_ids = generate(model=model,\n","         idx=text_to_token_ids(input_text, tokenizer),\n","         max_new_tokens=35, context_size=BASE_CONFIG['context_length'], eos_id=50256)\n","generated_text = token_ids_to_text(token_ids, tokenizer)\n","print(generated_text[len(input_text):].strip())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7gTmYNWBVfOd","executionInfo":{"status":"ok","timestamp":1742359213907,"user_tz":420,"elapsed":14626,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"d50106f5-fbbb-4eee-eee0-8c26486a6993"},"execution_count":113,"outputs":[{"output_type":"stream","name":"stdout","text":["Below is an instruction that describes a task. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","Convert the active sentence to passive: 'The chef cooks the meal every day.'\n","### Instruction:\n","\n","Convert the active sentence to passive: 'The chef cooks the meal every day.'\n","\n","### Instruction:\n","\n","Convert the active\n"]}]},{"cell_type":"code","source":["def calc_loss_batch_finetune(input_batch, target, model, device):\n","  input_batch = input_batch.to(device)\n","  target = target.to(device)\n","  logits = model(input_batch)\n","  loss = torch.nn.functional.cross_entropy(logits.flatten(0,1), target.flatten())\n","  return loss\n","\n","def calc_loss_loader_finetune(data_loader, model, device, num_batches=None):\n","  model.eval()\n","  total_loss = 0\n","  if len(data_loader) == 0:\n","    return float(\"nan\")\n","  elif num_batches is None:\n","    num_batches = len(data_loader)\n","  else:\n","    num_batches = min(num_batches, len(data_loader))\n","\n","  for i , (input_batch, target) in enumerate(data_loader):\n","    print(\"processing input batch\", i)\n","    if i >= num_batches:\n","      break\n","\n","    input_batch = input_batch.to(device)\n","    target = target.to(device)\n","    with torch.no_grad():\n","      loss = calc_loss_batch_finetune(input_batch, target, model, device)\n","      total_loss += loss.item()\n","\n","  return total_loss / num_batches\n","\n","def train_model_inst_finetune(model, train_dataloader, val_dataloader,optimizer,device, num_epochs, eval_freq, eval_iter,start_context,tokenizer):\n","  # initialize tracker for losses and tokens seen\n","  train_losses, val_losses, track_tokens_seen = [],[],[]\n","  tokens_seen,global_step= 0, -1\n","\n","  for epoch in range(num_epochs):\n","    model.train() # set model in train mode\n","\n","    for input_batch, target_batch in train_dataloader:\n","      optimizer.zero_grad() # reset loss gradients from previous batch\n","      loss = calc_loss_batch(input_batch, target_batch, model, device)\n","      loss.backward() # calculate the gradients in the backward pass\n","      optimizer.step() # update model weights using the gradients\n","      tokens_seen = input_batch.numel() # returns total tokens of the input batch\n","      global_step = global_step + 1\n","\n","      if global_step % eval_freq == 0:\n","        model.eval() # set model in eval mode\n","        with torch.no_grad():\n","          train_loss = calc_loss_loader_finetune(train_dataloader,model,device,num_batches=eval_iter)\n","          val_loss = calc_loss_loader_finetune(val_dataloader,model,device,num_batches=eval_iter)\n","          train_losses.append(train_loss)\n","          val_losses.append(val_loss)\n","          track_tokens_seen.append(tokens_seen)\n","          print(\"epoch:\", epoch, \"global step:\", global_step, \"train loss:\", train_loss, \"val loss:\", val_loss)\n","        model.train()\n","\n","    generate_and_print_sample(model,tokenizer,device,start_context)\n","\n","  return train_losses, val_losses, track_tokens_seen"],"metadata":{"id":"eiCWpViiWxau","executionInfo":{"status":"ok","timestamp":1742359213907,"user_tz":420,"elapsed":2,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":114,"outputs":[]},{"cell_type":"code","source":["# instruction finetuning loop\n","model.to(device)\n","torch.manual_seed(123)\n","# with torch.no_grad():\n","#   train_loss = calc_loss_loader_finetune(train_loader, model, device, num_batches=2)\n","#   val_loss = calc_loss_loader_finetune(val_loader, model, device, num_batches=2)\n","# print(\"train loss:\", train_loss)\n","# print(\"val loss:\", val_loss)\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.1)\n","# start_context = \"Every effort moves you\"\n","# this is a time consuming step in the order of 10's of mins. Uncomment the code to run the pre-train loop\n","# train_losses, val_losses, track_tokens_seen = train_model_simple(\n","#     model,\n","#     train_loader,\n","#     val_loader,\n","#     optimizer,\n","#     device,\n","#     num_epochs=1,\n","#     eval_freq=5,\n","#     eval_iter=5,\n","#     start_context=format_input(val_data[0]),\n","#     tokenizer=tokenizer)\n","# end_time = time.time()\n","# execution_time_mins = (end_time - start_time) / 60\n","# print(f\"Execution time: {execution_time_mins:.2f} minutes\")\n","# #note: training on a small dataset and many epoch, can lead to the model to memorize the data\n"],"metadata":{"collapsed":true,"id":"E-2Ur6GfYKIp","executionInfo":{"status":"ok","timestamp":1742359213907,"user_tz":420,"elapsed":1,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":115,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(123)\n","input_text = format_input(test_data[2])\n","print(input_text)\n","# model output when  finetuned\n","token_ids = generate(model=model,\n","         idx=text_to_token_ids(input_text, tokenizer),\n","         max_new_tokens=35, context_size=BASE_CONFIG['context_length'], eos_id=50256)\n","generated_text = token_ids_to_text(token_ids, tokenizer)\n","print(\"generated text:\", generated_text[len(input_text):].strip())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NbIxywKhYdVk","executionInfo":{"status":"ok","timestamp":1742359227232,"user_tz":420,"elapsed":13326,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"7587dbcf-6e64-47a5-caa5-af7d36fb5e5d"},"execution_count":116,"outputs":[{"output_type":"stream","name":"stdout","text":["Below is an instruction that describes a task. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","Name the author of 'Pride and Prejudice'.\n","generated text: ### Description:\n","\n","The author of 'Pride and Prejudice' is a young man who has been a member of the Church for over a century.\n"]}]},{"cell_type":"code","source":["# prompt: plot the losses obtained in previous cell\n","\n","# when the inst finetune is run, that generates the train_losses, val_losses tensors. uncomment to plot\n","# import matplotlib.pyplot as plt\n","\n","# # Assuming train_losses and val_losses are defined from the previous cell\n","# plt.plot(train_losses, label='Train Loss')\n","# plt.plot(val_losses, label='Validation Loss')\n","# plt.xlabel('Evaluation Steps')\n","# plt.ylabel('Loss')\n","# plt.title('Training and Validation Loss')\n","# plt.legend()\n","# plt.show()\n"],"metadata":{"id":"PD1ufifFTrXl","executionInfo":{"status":"ok","timestamp":1742359227248,"user_tz":420,"elapsed":14,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":117,"outputs":[]},{"cell_type":"markdown","source":["## Model Evaluation\n","how to measure an LLM response\n","- extract response -> qualitative eval -> scoring\n","\n","unlike a classifier, instruction finetuned llms are non trivial. this is a field of research it itself.\n","\n","Three methods:\n","1. short answer and multi choice benchmakes MMLU (measuring massive multitask lnaguage understanding)\n","2. human preference comparison to other LLMs\n","3. automated conversational benchmarks, where another LLM is used to evaluate the response. (AI Judge)"],"metadata":{"id":"yYN_5-MbZgyv"}},{"cell_type":"code","source":["# extract response\n","torch.manual_seed(123)\n","\n","for i, entry in enumerate(test_data[:10]):\n","  input_txt = format_input(entry)\n","  print(input_txt)\n","  token_ids = generate(\n","      model=model,\n","      idx=text_to_token_ids(input_txt, tokenizer),\n","      max_new_tokens=256,\n","      context_size=BASE_CONFIG['context_length'],\n","      eos_id=50256)\n","  generated_text = token_ids_to_text(token_ids, tokenizer)\n","  response_text = generated_text[len(input_txt):].replace(\"### Response:\", \"\").strip()\n","  print(\"response text:\", response_text)\n","  test_data[i]['response'] = response_text\n","\n","with open(\"instruction-data-with-response.json\", \"w\") as f:\n","  json.dump(test_data, f, indent=4)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":478},"id":"2swkpg4eZiRi","executionInfo":{"status":"error","timestamp":1742359269677,"user_tz":420,"elapsed":42428,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"c39095ca-dae9-427c-d9e3-6ba7414e3764"},"execution_count":118,"outputs":[{"output_type":"stream","name":"stdout","text":["Below is an instruction that describes a task. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","Rewrite the sentence using a simile.\n","\n","### Input:\n","The car is very fast.\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-118-bb33e6edc68c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0minput_txt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_txt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   token_ids = generate(\n\u001b[0m\u001b[1;32m      8\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_to_token_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_txt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-82-b995cf2aad05>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(model, idx, max_new_tokens, context_size, temp, top_k, eos_id)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0midx_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcontext_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# only chose the supported context size number of tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m       \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_cond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# extract the last token of the sequence for every batch. dim is now (batch,vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-64-6b9ad666a359>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrf_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["# file_name = \"gpt2-small-124M-sft.pth\"\n","# torch.save(model.state_dict(), file_name)\n","# print(\"Finetuned model saved in file\", file_name)"],"metadata":{"id":"NW_1Pk9laiMC","executionInfo":{"status":"aborted","timestamp":1742359269678,"user_tz":420,"elapsed":74673,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls -l gpt2-small-124M-sft.pth # save the instruction finetuned model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VZbnlh2ne9lG","executionInfo":{"status":"ok","timestamp":1742359289152,"user_tz":420,"elapsed":106,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"9126c500-0dbc-4505-c93d-1080c5ffdfd6"},"execution_count":119,"outputs":[{"output_type":"stream","name":"stdout","text":["ls: cannot access 'gpt2-small-124M-sft.pth': No such file or directory\n"]}]},{"cell_type":"code","source":["# !curl -fsSL https://ollama.com/install.sh | sh\n","# # https://ollama.com/library/llama3:8b\n","# ! ollama run llama3"],"metadata":{"id":"hEVVq-Dpheu6","executionInfo":{"status":"aborted","timestamp":1742359269683,"user_tz":420,"elapsed":1,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LORA\n","import math\n","class LoraLayer(nn.Module):\n","  def __init__(self, in_dim, out_dim, rank, alpha):\n","    super().__init__()\n","    self.A = torch.nn.Parameter(torch.empty(in_dim, rank))\n","    torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n","    self.B = torch.nn.Parameter(torch.zeros(rank, out_dim)) # This ensures that the multiplication does not alter the original weights, as adding zero does not change them\n","    self.alpha = alpha\n","\n","  def forward(self, x):\n","    return self.alpha * (x @ self.A @ self.B)\n","\n","\n","class LinearWithLora(torch.nn.Module):\n","  def __init__(self, linear, rank, alpha):\n","    super().__init__()\n","    self.lora = LoraLayer(linear.in_features, linear.out_features, rank, alpha)\n","    self.linear = linear\n","\n","  def forward(self, x):\n","    return self.linear(x) + self.lora(x)\n","\n","\n","def replace_linear_with_lora(model, rank, alpha):\n","  for name, module in model.named_children():\n","    if isinstance(module, torch.nn.Linear):\n","      setattr(model, name, LinearWithLora(module, rank, alpha))\n","    else:\n","      replace_linear_with_lora(module, rank, alpha)\n"],"metadata":{"id":"nd5omh3fsyDV","executionInfo":{"status":"ok","timestamp":1742360406973,"user_tz":420,"elapsed":10,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}}},"execution_count":135,"outputs":[]},{"cell_type":"code","source":["# we will now replace the linear layers of the masked multihead attention, feed forward, final linear output layer blocks.\n","train_ds = SpamDataset(\"train.csv\", tokenizer=tokenizer)\n","val_ds = SpamDataset(\"val.csv\", tokenizer=tokenizer)\n","test_ds = SpamDataset(\"test.csv\", tokenizer=tokenizer)\n","\n","print(len(train_ds), len(val_ds), len(test_ds))\n","print(train_ds.max_length, val_ds.max_length, test_ds.max_length)\n","print(train_ds[0])\n","print(val_ds[0])\n","print(test_ds[0])\n","\n","# setup dataloader for the spam dataset\n","num_workers = 0\n","batch_size = 8\n","train_loader = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers,drop_last=True)\n","val_loader = DataLoader(dataset=val_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers,drop_last=False)\n","test_loader = DataLoader(dataset=test_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers,drop_last=False)\n","\n","\n","for input_batch, target in train_loader:\n","  print(input_batch.shape, target.shape)\n","  break\n","\n","\n","MODEL = \"gpt2-small (124M)\"\n","INPUT_PROMPT = \"Every effort moves\"\n","\n","BASE_CONFIG = {\n","    \"vocab_size\": 50257,\n","    \"context_length\": 1024,\n","    \"drop_rate\": 0.0,\n","    \"qkv_bias\": False,\n","}\n","\n","model_configs = {\n","    \"gpt2-small (124M)\" : {\n","        \"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12\n","    },\n","    \"gpt2-medium (355M)\" : {\n","        \"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16\n","    },\n","}\n","\n","BASE_CONFIG.update(model_configs[MODEL])\n","\n","model_name = \"gpt2-small (124M)\"\n","NEW_GPT_CONFIG = BASE_CONFIG.copy()\n","NEW_GPT_CONFIG.update(model_configs[model_name])\n","NEW_GPT_CONFIG.update({\"context_length\":1024, \"qkv_bias\":True})\n","\n","model = GPTModel(NEW_GPT_CONFIG)\n","load_weights_into_gpt(model, params)\n","model.eval()\n","# now the model is ready to be tweaked to introduce the LORA layers\n","\n","\n","# replace_linear_with_lora(model, rank=8, alpha=8)\n","# total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"8plRGhb0uyzq","executionInfo":{"status":"ok","timestamp":1742360235350,"user_tz":420,"elapsed":3399,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"2ef655ec-5ec5-42c1-a4fb-d135ef9589e5"},"execution_count":127,"outputs":[{"output_type":"stream","name":"stdout","text":["1045 149 300\n","120 71 92\n","(tensor([   35,  2507,   703,   466,   345,   588,   262,  6940,  2344,    13,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]), tensor(0))\n","(tensor([   44, 10102,    11,  2479,  1954,    11, 21541,    11,   649,   287,\n","         3482,    13,   314,   804,  1714,   351,  3482,  3730,    13,   611,\n","          334,   588,  1257,   351,   502,    13,  8255, 19308, 28082,   284,\n","        39861,  2791,    13,  1507,   764,  1542,   381,    14, 14116,   352,\n","          301,   642,  5787,    13,  4248,    16,    13,  1120, 41867,    13,\n","        10478,  2919,    22, 23451, 25270,  4304, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256]), tensor(1))\n","(tensor([ 5332, 25429, 17189,    29, 39687, 41527,     0, 36875, 32744, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256]), tensor(1))\n","torch.Size([8, 120]) torch.Size([8])\n"]},{"output_type":"execute_result","data":{"text/plain":["GPTModel(\n","  (token_emb): Embedding(50257, 768)\n","  (pos_emb): Embedding(1024, 768)\n","  (drop_emb): Dropout(p=0.0, inplace=False)\n","  (trf_blocks): Sequential(\n","    (0): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (1): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (2): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (3): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (4): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (5): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (6): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (7): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (8): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (9): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (10): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (11): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=True)\n","        (W_key): Linear(in_features=768, out_features=768, bias=True)\n","        (W_value): Linear(in_features=768, out_features=768, bias=True)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","  )\n","  (final_norm): LayerNorm()\n","  (out_proj): Linear(in_features=768, out_features=50257, bias=False)\n",")"]},"metadata":{},"execution_count":127}]},{"cell_type":"code","source":["total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(\"total trainable params (before freeze):\", total_params)\n","\n","for p in model.parameters():\n","  p.requires_grad = False\n","\n","total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(\"total trainable params (after freeze):\", total_params)\n","\n","\n","replace_linear_with_lora(model, rank=16, alpha=16)\n","total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(\"total trainable params (after lora layers introduced):\", total_params)\n","\n","model.to(device)\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gWiK37YtwA0x","executionInfo":{"status":"ok","timestamp":1742360444938,"user_tz":420,"elapsed":79,"user":{"displayName":"Saikat Roychowdhury","userId":"00106323805182171591"}},"outputId":"5908ff55-0ddf-4214-d9f7-9522bff113de"},"execution_count":137,"outputs":[{"output_type":"stream","name":"stdout","text":["total trainable params (before freeze): 3470608\n","total trainable params (after freeze): 0\n","total trainable params (after lora layers introduced): 3470608\n","GPTModel(\n","  (token_emb): Embedding(50257, 768)\n","  (pos_emb): Embedding(1024, 768)\n","  (drop_emb): Dropout(p=0.0, inplace=False)\n","  (trf_blocks): Sequential(\n","    (0): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (W_key): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (W_value): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (out_proj): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): LinearWithLora(\n","              (lora): LoraLayer()\n","              (linear): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","          )\n","          (1): GELU()\n","          (2): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): LinearWithLora(\n","              (lora): LoraLayer()\n","              (linear): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","          )\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (1): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (W_key): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (W_value): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (out_proj): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): LinearWithLora(\n","              (lora): LoraLayer()\n","              (linear): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","          )\n","          (1): GELU()\n","          (2): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): LinearWithLora(\n","              (lora): LoraLayer()\n","              (linear): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","          )\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (2): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (W_key): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (W_value): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (out_proj): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): LinearWithLora(\n","              (lora): LoraLayer()\n","              (linear): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","          )\n","          (1): GELU()\n","          (2): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): LinearWithLora(\n","              (lora): LoraLayer()\n","              (linear): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","          )\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (3): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (W_key): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (W_value): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (out_proj): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): LinearWithLora(\n","              (lora): LoraLayer()\n","              (linear): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","          )\n","          (1): GELU()\n","          (2): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): LinearWithLora(\n","              (lora): LoraLayer()\n","              (linear): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","          )\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (4): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (W_key): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (W_value): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (out_proj): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): LinearWithLora(\n","              (lora): LoraLayer()\n","              (linear): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","          )\n","          (1): GELU()\n","          (2): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): LinearWithLora(\n","              (lora): LoraLayer()\n","              (linear): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","          )\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (5): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (W_key): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (W_value): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (out_proj): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): LinearWithLora(\n","              (lora): LoraLayer()\n","              (linear): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","          )\n","          (1): GELU()\n","          (2): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): LinearWithLora(\n","              (lora): LoraLayer()\n","              (linear): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","          )\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (6): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (W_key): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (W_value): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (out_proj): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): LinearWithLora(\n","              (lora): LoraLayer()\n","              (linear): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","          )\n","          (1): GELU()\n","          (2): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): LinearWithLora(\n","              (lora): LoraLayer()\n","              (linear): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","          )\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (7): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (W_key): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (W_value): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (out_proj): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): LinearWithLora(\n","              (lora): LoraLayer()\n","              (linear): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","          )\n","          (1): GELU()\n","          (2): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): LinearWithLora(\n","              (lora): LoraLayer()\n","              (linear): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","          )\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (8): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (W_key): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (W_value): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (out_proj): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): LinearWithLora(\n","              (lora): LoraLayer()\n","              (linear): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","          )\n","          (1): GELU()\n","          (2): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): LinearWithLora(\n","              (lora): LoraLayer()\n","              (linear): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","          )\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (9): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (W_key): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (W_value): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (out_proj): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): LinearWithLora(\n","              (lora): LoraLayer()\n","              (linear): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","          )\n","          (1): GELU()\n","          (2): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): LinearWithLora(\n","              (lora): LoraLayer()\n","              (linear): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","          )\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (10): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (W_key): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (W_value): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (out_proj): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): LinearWithLora(\n","              (lora): LoraLayer()\n","              (linear): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","          )\n","          (1): GELU()\n","          (2): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): LinearWithLora(\n","              (lora): LoraLayer()\n","              (linear): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","          )\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","    (11): Transformer(\n","      (attn): MultiHeadAttention(\n","        (W_query): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (W_key): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (W_value): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (out_proj): LinearWithLora(\n","          (lora): LoraLayer()\n","          (linear): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): LinearWithLora(\n","              (lora): LoraLayer()\n","              (linear): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","          )\n","          (1): GELU()\n","          (2): LinearWithLora(\n","            (lora): LoraLayer()\n","            (linear): LinearWithLora(\n","              (lora): LoraLayer()\n","              (linear): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","          )\n","        )\n","      )\n","      (norm1): LayerNorm()\n","      (norm2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.0, inplace=False)\n","    )\n","  )\n","  (final_norm): LayerNorm()\n","  (out_proj): LinearWithLora(\n","    (lora): LoraLayer()\n","    (linear): LinearWithLora(\n","      (lora): LoraLayer()\n","      (linear): Linear(in_features=768, out_features=50257, bias=False)\n","    )\n","  )\n",")\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"zWxXhnkvyLq9"}},{"cell_type":"markdown","source":["## Summary\n","We have seen various stages of an LLM:\n","1. building LLM - datasets, dataloader, LLM building blocks\n","2. Foundational model - pre-trainig, model eval, loading pretrained weights\n","3. Classifier, personal assistant - flavors of finetuning\n","\n","\n","# Model improvement strategies\n","1. hyper param tuning - learning rate, epoch, batch size\n","2. increase training dataset, diversify examples to cover a broad range of topics\n","3. experiment with prompts\n","4. larger pretrained models which already capture complex patterns\n","5. PEFT fine tuning"],"metadata":{"id":"7cSauSbBzSQc"}}]}